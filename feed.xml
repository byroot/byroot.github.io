<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://byroot.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://byroot.github.io/" rel="alternate" type="text/html" /><updated>2025-03-30T08:27:14+00:00</updated><id>https://byroot.github.io/feed.xml</id><title type="html">byroot’s blog</title><subtitle>Various ramblings.</subtitle><entry><title type="html">Database Protocols Are Underwhelming</title><link href="https://byroot.github.io/performance/2025/03/21/database-protocols.html" rel="alternate" type="text/html" title="Database Protocols Are Underwhelming" /><published>2025-03-21T08:03:51+00:00</published><updated>2025-03-21T08:03:51+00:00</updated><id>https://byroot.github.io/performance/2025/03/21/database-protocols</id><content type="html" xml:base="https://byroot.github.io/performance/2025/03/21/database-protocols.html"><![CDATA[<p>If you’ve been in this trade for a while, you have probably seen dozens of debates on the merits and problems of SQL
as a relational database query language.
As an ORM maintainer, I have a few gripes with SQL, but overall it is workable, and anyway, it has so much inertia
that there’s no point fantasizing about a replacement.</p>

<p>However one database-adjacent topic I don’t think I’ve ever seen any discussions about, and that I think could be improved,
is the protocols exposed by these databases to execute queries.
Relational databases are very impressive pieces of technology, but their client protocol makes me wonder if they ever
considered being used by anything other than a human typing commands in a CLI interface.</p>

<p>I also happen to maintain the Redis client for Ruby, and while the Redis protocol is far from perfect, I think
there are some things it does better than PostgreSQL and MySQL protocols, which are the two I am somewhat familiar with.</p>

<h2 id="mutable-state-lot-of-mutable-state">Mutable State, Lot Of Mutable State</h2>

<p>You’ve probably never seen them, because they’re not logged by default, but when Active Record connects to your database
it starts by executing several database-specific queries, which I generally call the “prelude”.</p>

<p>Which queries are sent exactly depends on how you configured Active Record, but for most people, it will be the default.</p>

<p>In the case of MySQL it will look like this:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SET</span>  <span class="o">@@</span><span class="k">SESSION</span><span class="p">.</span><span class="n">sql_mode</span> <span class="o">=</span> <span class="n">CONCAT</span><span class="p">(</span><span class="o">@@</span><span class="n">sql_mode</span><span class="p">,</span> <span class="s1">',STRICT_ALL_TABLES,NO_AUTO_VALUE_ON_ZERO'</span><span class="p">),</span>
     <span class="o">@@</span><span class="k">SESSION</span><span class="p">.</span><span class="n">wait_timeout</span> <span class="o">=</span> <span class="mi">2147483</span>
</code></pre></div></div>

<p>For PostgreSQL, there’s a bit more:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SET</span> <span class="n">client_min_messages</span> <span class="k">TO</span> <span class="s1">'warning'</span><span class="p">;</span>
<span class="k">SET</span> <span class="n">standard_conforming_strings</span> <span class="o">=</span> <span class="k">on</span><span class="p">;</span>
<span class="k">SET</span> <span class="n">intervalstyle</span> <span class="o">=</span> <span class="n">iso_8601</span><span class="p">;</span>
<span class="k">SET</span> <span class="k">SESSION</span> <span class="n">timezone</span> <span class="k">TO</span> <span class="s1">'UTC'</span>
</code></pre></div></div>

<p>In both cases the idea is the same, we’re configuring the connection, making it behave differently.
And there’s nothing wrong with the general idea of that, as a database gets older, new modes and features get introduced
so for backward compatibility reasons you have to opt-in to them.</p>

<p>My issue with this however is that you can set these at any point.
They’re not restricted to an initial authentication and configuration step, so when as a framework or library you hand
over a connection to user code and later get it back, you can’t know for sure they haven’t changed any of these settings.
Similarly, it means you have both configured and unconfigured connections and must be careful to never use an unconfigured one.
It’s not the end of the world but noticeably complexifies the connection management code.</p>

<p>This statefulness also makes it hard if not impossible to recover from errors. If for some reason a query fails, it’s hard
to tell which state the connection is in, and the only reasonable thing to do is to close it and start from scratch with a new connection.</p>

<p>If these protocols had an explicit initial configuration phase, it would make it easier to have some sort of “reset state”
message you could send after an error (or after letting user code run unknown queries) to get the connection back to a known clean state.</p>

<p>From a Ruby client perspective, it would look like this:</p>
<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">connection</span> <span class="o">=</span> <span class="no">MyDB</span><span class="p">.</span><span class="nf">new_connection</span>
<span class="n">connection</span><span class="p">.</span><span class="nf">authenticate</span><span class="p">(</span><span class="n">user</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>
<span class="n">connection</span><span class="p">.</span><span class="nf">configure</span><span class="p">(</span><span class="s2">"SET ..."</span><span class="p">)</span>
<span class="n">connection</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="s2">"INSERT INTO ..."</span><span class="p">)</span>
<span class="n">connection</span><span class="p">.</span><span class="nf">reset</span>
</code></pre></div></div>

<p>You could even cheaply reset the state whenever a connection is checked back into a connection pool.</p>

<p>I’m not particularly knowledgeable about all the constraints database servers face, but I can’t think of a reason why such
protocol feature would be particularly tricky to implement.</p>

<h2 id="safe-retries">Safe Retries</h2>

<p>One of the most important jobs of a database client, or network clients in general, is to deal with network errors.</p>

<p>Under the hood, most if not all clients will look like this:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">query</span><span class="p">(</span><span class="n">command</span><span class="p">)</span>
  <span class="n">packet</span> <span class="o">=</span> <span class="n">serialize</span><span class="p">(</span><span class="n">command</span><span class="p">)</span>
  <span class="vi">@socket</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">packet</span><span class="p">)</span>
  <span class="n">response</span> <span class="o">=</span> <span class="vi">@socket</span><span class="p">.</span><span class="nf">read</span>
  <span class="n">deserialize</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div></div>

<p>It’s fairly trivial, you send the query to the server and read the server response.
The difficulty however is that both the <code class="language-plaintext highlighter-rouge">write</code> and the <code class="language-plaintext highlighter-rouge">read</code> operations can fail in dozens of different ways.</p>

<p>Perhaps the server is temporarily unreachable and will work again in a second or two.
Or perhaps it’s reachable but was temporarily overloaded and didn’t answer fast enough so the client timeout was reached.</p>

<p>These errors should hopefully be rare, but can’t be fully avoided.
Whenever you are sending something through the network, there is a chance it might not work, it’s a fact of life.
Hence a client should try to gracefully handle such errors as much as possible, and there aren’t many ways to do so.</p>

<p>The most obvious way to handle such an error is to retry the query, the problem is that most of the time, from the point
of view of the database client, it isn’t clear whether it is safe to retry or not.</p>

<p>In my view, the best feature of <code class="language-plaintext highlighter-rouge">HTTP</code> by far is its explicit verb specification.
The HTTP spec clearly states that clients, and even proxies, are allowed to retry some specific verbs such as <code class="language-plaintext highlighter-rouge">GET</code> or <code class="language-plaintext highlighter-rouge">DELETE</code>
because they are <a href="https://en.wikipedia.org/wiki/Idempotence#Computer_science_meaning">idempotent</a>.</p>

<p>The reason this is important is that whenever the <code class="language-plaintext highlighter-rouge">write</code> or the <code class="language-plaintext highlighter-rouge">read</code> fails, in the overwhelming majority of cases,
you don’t know whether the query was executed on the server or not.
That is why idempotency is such a valuable property, by definition an idempotent operation can safely be executed twice,
hence when you are in doubt whether it was executed, you can retry.</p>

<p>But knowing whether a query is idempotent or not with SQL isn’t easy.
For instance, a simple <code class="language-plaintext highlighter-rouge">DELETE</code> query is idempotent:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">DELETE</span>
<span class="k">FROM</span> <span class="n">articles</span>
<span class="k">WHERE</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">42</span><span class="p">;</span>
</code></pre></div></div>

<p>But one can perfectly write a <code class="language-plaintext highlighter-rouge">DELETE</code> query that isn’t:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">DELETE</span>
<span class="k">FROM</span> <span class="n">articles</span>
<span class="k">WHERE</span> <span class="n">id</span> <span class="k">IN</span> <span class="p">(</span>
  <span class="k">SELECT</span> <span class="n">id</span>
  <span class="k">FROM</span> <span class="n">articles</span>
  <span class="k">LIMIT</span> <span class="mi">10</span>
<span class="p">);</span>
</code></pre></div></div>

<p>So in practice, database clients can’t safely retry on errors, unless the caller instructs them that it is safe to do so.
You could attempt to write a client that parses the queries to figure out whether they are idempotent, but it is fraught with peril,
hence it’s generally preferable to rely on the caller to tell us.</p>

<p>That’s one of the reasons why I’ve been slowly refactoring Active Record lately, to progressively make it easier to retry
more queries in case of network errors.
But even once I’ll be done with that refactoring, numerous non-idempotent queries will remain, and whenever they fail,
there is still nothing Active Record will be able to do about it.</p>

<h2 id="idempotency-keys">Idempotency Keys</h2>

<p>However, there are solutions to turn non-idempotent operations into idempotent ones, using what is sometimes called “Idempotency Keys”.
If you’ve used <a href="https://docs.stripe.com/api/idempotent_requests">the Stripe API</a>, perhaps you are already familiar with them.
I suspect they’re not the first ones to come up with such a solution, but that’s where I was first exposed to it.</p>

<p>Conceptually it’s rather simple, when performing a non-idempotent operation, say creating a new customer record, you can
add an <code class="language-plaintext highlighter-rouge">Idempotency-Key</code> <code class="language-plaintext highlighter-rouge">HTTP</code> header containing a randomly generated string.
If for some reason you need to retry that request, you do it with the same idempotency key, allowing the Stripe API to
check if the initial request succeeded or not, and either perform or discard the retry.</p>

<p>They even go a bit further, when a request with an idempotency key succeeds, they record the response so that in case of
a retry, they return you exactly the original response. Thanks to this feature, it is safe to retry all API calls to their
API, regardless of whether they are idempotent or not.</p>

<p>This is such a great feature that last year, at Rails World 2024, when I saw there was a ValKey booth, hosted by
<a href="https://fosstodon.org/@linux_mclinuxface">Kyle Davis</a>, I decided to go have a chat with him, to see if perhaps ValKey
was interested in tackling this fairly common problem.</p>

<p>Because everything I said about SQL and idempotency also applies to Redis (hence to ValKey).
It is also hard for a Redis client to know if a query can safely be retried, and for decades, long before I became the
maintainer, the Redis client would <a href="https://github.com/avgerin0s/redis-rb/blob/f17a33f05146d29256622e7736abe00870aed6ef/lib/redis.rb#L139-L152">retry all queries by default</a>.</p>

<p>At first, it would only do so in case of <code class="language-plaintext highlighter-rouge">ECONNRESET</code> errors, but over time more errors were added to the retry list.
I must admit I’m not the most knowledgeable person about <code class="language-plaintext highlighter-rouge">TCP</code>, so perhaps it is indeed safe to assume the server never
received the query when such an error is returned, but over time more and more errors were added to the list, and I highly
doubt all of them are safe to retry.</p>

<p>That’s why when I later wrote <code class="language-plaintext highlighter-rouge">redis-client</code>, a much simpler and lower-level client for Redis, I made sure not to retry by
default, as well as a way to distinguish idempotent queries by having both a <code class="language-plaintext highlighter-rouge">call</code> and a <code class="language-plaintext highlighter-rouge">call_once</code> method.</p>

<p>But from the feedback I got when Mike Perham replaced the <code class="language-plaintext highlighter-rouge">redis</code> gem with <code class="language-plaintext highlighter-rouge">redis-client</code> in Sidekiq, lots of users
started noticing reports of errors they wouldn’t experience before, showing how unreliable remote data stores can be in practice,
especially in cloud environments.</p>

<p>So even though these retries were potentially unsafe, and may have occasionally caused data loss, they were desired by users.</p>

<p>That’s why I tried to pitch an idempotency key kind of feature to Kyle, and he encouraged me to open <a href="https://github.com/valkey-io/valkey/issues/1087">a feature request
in the ValKey repo</a>. After a few rounds of discussion, the ValKey core
team accepted the feature, and while as far as I know it hasn’t been implemented yet, the next version of ValKey will likely
have it.</p>

<p>It is again pretty simple conceptually:</p>

<pre><code class="language-SQL">MULTISTORE 699accd1-c7fa-4c40-bc85-5cfcd4d3d344 EX 10
INC counter
LPOP queue
EXEC
</code></pre>

<p>Just like with Stripe’s API, you start a transaction with a randomly generated key, in this case, a UUID, as well as an expiry.</p>

<p>In the example above we ask ValKey to remember this transaction for the next 10 seconds, that’s for how long we can safely
retry, after that ValKey can discard the response.</p>

<p>Assuming the next version of ValKey ships with the feature, that should finally offer a solution to safely retry all possible queries.</p>

<p>I fully understand that relational databases are much bigger beasts than an in-memory key-value store, hence it likely is harder
to implement, but if I was ever asked what feature MySQL or PostgreSQL could add to make them nicer to work with, it certainly would be this one.</p>

<p>In the case of ValKey, given it’s a text protocol that meant introducing a new command, but MySQL and PostgreSQL both have
binary protocols, with distinct packet types, so I think it would be possible to introduce at the protocol level with
no change to their respective SQL syntax, and no backward compatibility concerns.</p>

<h2 id="prepared-statements">Prepared Statements</h2>

<p>Another essential part of database protocols that I think isn’t pleasant to work with is prepared statements.</p>

<p>Prepared statements mostly serve two functions, the most important one is to provide a query and its parameters separately,
as to eliminate the risk of SQL injections.
In addition to that, it can in some cases help with performance, because it saves on having to parse the query every time,
as well as to send it down the wire. Some databases will also cache the associated query plan.</p>

<p>Here’s how you use prepared statements using the MySQL protocol:</p>

<ul>
  <li>First send a <code class="language-plaintext highlighter-rouge">COM_STMT_PREPARE</code> packet with the parametized query (<code class="language-plaintext highlighter-rouge">SELECT * FROM users WHERE id = ?</code>).</li>
  <li>Read the returned <code class="language-plaintext highlighter-rouge">COM_STMT_PREPARE_OK</code> packet and extract the <code class="language-plaintext highlighter-rouge">statement_id</code>.</li>
  <li>Then send a <code class="language-plaintext highlighter-rouge">COM_STMT_EXECUTE</code> with the <code class="language-plaintext highlighter-rouge">statement_id</code> and the parameters.</li>
  <li>Read the <code class="language-plaintext highlighter-rouge">OK_Packet</code> response.</li>
  <li>Whenever you no longer need that prepared statement, send a <code class="language-plaintext highlighter-rouge">COM_STMT_CLOSE</code> packet with the <code class="language-plaintext highlighter-rouge">statement_id</code>.</li>
</ul>

<p>Now ideally, you execute the same statements relatively often, so you keep track of them, and in the happy path you
can perform a parameterized query in a single roundtrip by directly sending a <code class="language-plaintext highlighter-rouge">COM_STMT_EXECUTE</code> with the known <code class="language-plaintext highlighter-rouge">statement_id</code>.</p>

<p>But one major annoyance is that these <code class="language-plaintext highlighter-rouge">statement_id</code> are session-scoped, meaning they’re only valid with the connection
that was used to create them.
In a modern web application, you don’t just have one connection, but a pool of them, and that’s per process, so you need
to keep track of the same thing many times.</p>

<p>Worse, as explained previously, since closing and reopening the connection is often the only safe way to recover from errors,
whenever that happens, all prepared statements are lost.</p>

<p>These statements also have a cost on the server side. Each statement requires some amount of memory in the database server.
So you have to be careful not to create an unbounded amount of them, which for an ORM isn’t easy to enforce.</p>

<p>It’s not rare for applications to dynamically generate queries based on user input, typically some advanced search or filtering form.</p>

<p>In addition, Active Record allows you to provide SQL fragments, and it can’t know whether they are static strings or dynamically
generated ones. For example, it’s not good practice, but users can perfectly do something like this:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="no">Article</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="s2">"published_at &gt; '</span><span class="si">#{</span><span class="no">Time</span><span class="p">.</span><span class="nf">now</span><span class="p">.</span><span class="nf">to_s</span><span class="p">(</span><span class="n">db</span><span class="p">)</span><span class="si">}</span><span class="s2">'"</span><span class="p">)</span>
</code></pre></div></div>

<p>Also, if you have <a href="https://api.rubyonrails.org/classes/ActiveRecord/QueryLogs.html">Active Record query logs</a>, then most
queries will be unique.</p>

<p>All this means that a library like Active Record has to have lots of logic to keep track of prepared statements and their
lifetime. You might even need some form of Least Recently Used logic to prune unused statements and free resources on the server.</p>

<p>In many cases, when you have no reason to believe a particular query will be executed again soon, it is actually advantageous
not to use prepared statements.
Ideally, you’d still use a parameterized query, but then it means doing 2-3 rountrips<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> to the database instead of just one.</p>

<p>So for MySQL at least, when you use Active Record with a SQL fragment provided as a string, Active Record fallback to
not use prepared statements, and instead interpolate the parameters inside the query.</p>

<p>Ideally, we’d still use a parameterised query, just not a prepared one, but the MySQL protocol doesn’t offer such functionality.
If you want to use parameterized queries, you have to use prepared statements and in many cases, that will mean an extra roundtrip.</p>

<p>I’m much less familiar with the PostgreSQL protocol, but from glancing at its specification I believe it works largely in the same way.</p>

<p>So how could it be improved?</p>

<p>First I think it should be possible to perform parameterized queries without a prepared statement, I can’t think of a reason
why this isn’t a possibility yet.</p>

<p>Then I think that here again, some inspiration could be taken from Redis.</p>

<h2 id="evalsha">EVALSHA</h2>

<p>Redis doesn’t have prepared statements, that wouldn’t make much sense, but it does have something rather similar in
the form of <a href="https://valkey.io/topics/eval-intro/">Lua scripts</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; EVAL "return ARGV[1] .. ARGV[2]" 0 "hello" "world!"
"helloworld!"
</code></pre></div></div>

<p>But just like SQL queries, Lua code needs to be parsed and can be relatively large, so caching that operation is preferable for
performance.
But rather than a <code class="language-plaintext highlighter-rouge">PREPARE</code> command that returns you a connection-specific identifier for your given script, Redis
instead use SHA1 digests.</p>

<p>You can first load a script with the <code class="language-plaintext highlighter-rouge">SCRIPT LOAD</code> command:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; SCRIPT LOAD "return ARGV[1] .. ARGV[2]"
"702b19e4aa19aaa9858b9343630276d13af5822e"
</code></pre></div></div>

<p>Then you can execute the script as many times as desired by only referring its digest:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; EVALSHA "702b19e4aa19aaa9858b9343630276d13af5822e" 0 "hello" "world!"
"helloworld!"
</code></pre></div></div>

<p>And that script registry is global, so even if you have 5000 connections, they can all share the same script, and you can
even assume scripts have been loaded already, and load them on a retry if they weren’t:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">require</span> <span class="s2">"redis-client"</span>
<span class="nb">require</span> <span class="s2">"digest/sha1"</span>

<span class="k">class</span> <span class="nc">RedisScript</span>
  <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
    <span class="vi">@src</span> <span class="o">=</span> <span class="n">src</span>
    <span class="vi">@digest</span> <span class="o">=</span> <span class="no">Digest</span><span class="o">::</span><span class="no">SHA1</span><span class="p">.</span><span class="nf">hexdigest</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
  <span class="k">end</span>

  <span class="k">def</span> <span class="nf">execute</span><span class="p">(</span><span class="n">connection</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="n">connection</span><span class="p">.</span><span class="nf">call</span><span class="p">(</span><span class="s2">"EVALSHA"</span><span class="p">,</span> <span class="vi">@digest</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
  <span class="k">rescue</span> <span class="no">RedisClient</span><span class="o">::</span><span class="no">CommandError</span>
    <span class="n">connection</span><span class="p">.</span><span class="nf">call</span><span class="p">(</span><span class="s2">"SCRIPT"</span><span class="p">,</span> <span class="s2">"LOAD"</span><span class="p">,</span> <span class="vi">@src</span><span class="p">)</span>
    <span class="n">connection</span><span class="p">.</span><span class="nf">call</span><span class="p">(</span><span class="s2">"EVALSHA"</span><span class="p">,</span> <span class="vi">@digest</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="no">CONCAT_SCRIPT</span> <span class="o">=</span> <span class="no">RedisScript</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span><span class="o">&lt;&lt;~</span><span class="no">LUA</span><span class="p">)</span><span class="sh">
  return ARGV[1] .. " " .. ARGV[2]
</span><span class="no">LUA</span>

<span class="n">redis</span> <span class="o">=</span> <span class="no">RedisClient</span><span class="p">.</span><span class="nf">new</span>
<span class="nb">p</span> <span class="no">CONCAT_SCRIPT</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="n">redis</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">"Hello"</span><span class="p">,</span> <span class="s2">"World!"</span><span class="p">)</span>
</code></pre></div></div>

<p>I’m not a database engineer, so perhaps there’s some big constraint I’m missing, but I think it would make a lot of sense
for prepared statement identifiers to be some sort of predictable digests, so that they are much more easily shared
across connection, and let the server deal with garbage-collecting prepared statements that haven’t been seen in a long
time, or use some sort of reference counting strategy.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I could probably find a few more examples of things that are impractical in MySQL and PostgreSQL protocols, but I think
I’ve shown enough to share my feelings about them.</p>

<p>Relational databases are extremely impressive projects, clearly built by very smart people, but It feels like the developer
experience isn’t very high on their priority list, if it’s even considered.
And that perhaps explains part of the NoSQL appeal in the early 2010’s.
However, I think it would be possible to significantly improve their usability without changing the query language, just by improving the query
protocol.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>3 roundtrips in total, but you theoretically can do the <code class="language-plaintext highlighter-rouge">COM_STMT_CLOSE</code> asynchronously. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="performance" /><summary type="html"><![CDATA[If you’ve been in this trade for a while, you have probably seen dozens of debates on the merits and problems of SQL as a relational database query language. As an ORM maintainer, I have a few gripes with SQL, but overall it is workable, and anyway, it has so much inertia that there’s no point fantasizing about a replacement.]]></summary></entry><entry><title type="html">The Pitchfork Story</title><link href="https://byroot.github.io/ruby/performance/2025/03/04/the-pitchfork-story.html" rel="alternate" type="text/html" title="The Pitchfork Story" /><published>2025-03-04T10:03:51+00:00</published><updated>2025-03-04T10:03:51+00:00</updated><id>https://byroot.github.io/ruby/performance/2025/03/04/the-pitchfork-story</id><content type="html" xml:base="https://byroot.github.io/ruby/performance/2025/03/04/the-pitchfork-story.html"><![CDATA[<p>A bit more than two years ago, as part of my work in Shopify’s Ruby and Rails Infrastructure team,
I released a new Ruby HTTP server called <a href="https://github.com/Shopify/pitchfork">Pitchfork</a>.</p>

<p>It has a bit of an unusual design and makes hard tradeoffs, so I’d like to explain the thought process behind
these decisions and how I see the future of that project.</p>

<h2 id="unicorns-design-is-fine">Unicorn’s Design Is Fine</h2>

<p>Ever since I joined Shopify over 11 years ago, the main monolith application has been using <a href="https://yhbt.net/unicorn/">Unicorn</a>
as its application server in production.
I know that Unicorn is seen as legacy software by many if not most Rubyists, <a href="https://yhbt.net/unicorn-public/20200908084429.GA16521@dcvr/T/#u">including Unicorn’s own maintainer</a>,
but I very strongly disagree with this opinion.</p>

<p>A major argument against Unicorn is that Rails apps are mostly IO-bound, so besides the existence of the GVL,
you can use a threaded server to increase throughput. <a href="/ruby/performance/2025/01/23/the-mythical-io-bound-rails-app.html">I explained in a previous post why I don’t believe most
Rails applications are IO-bound</a>,
but regardless of how true it is in general, it certainly isn’t the case of Shopify’s monolith, hence using a threaded
server wasn’t a viable option.</p>

<p>In addition, back in 2014, before the existence of the Ruby and Rails Infrastructure team at Shopify,
I worked on the Resiliency team, where we were in charge of reducing the likeliness of outages, as well as reducing the
blast radius of any outage we failed to prevent. That’s the team where we developed tools such as
<a href="https://github.com/Shopify/toxiproxy">Toxiproxy</a> and <a href="https://github.com/Shopify/semian">Semian</a>.</p>

<p>During my stint on the Resiliency team, I’ve witnessed some pretty catastrophic failures.
Some <a href="https://github.com/protocolbuffers/protobuf/issues/11968">C extensions segfaulting</a>, or worse,
<a href="https://github.com/grpc/grpc/pull/16332">deadlocking the Ruby VM</a>, some datastores becoming unresponsive, and more.</p>

<p>What I learned from that experience, is that while you should certainly strive to catch as many bugs as possible out front on CI,
you have to accept that you can’t possibly catch them all.
So ultimately, it becomes a number game. If an application is developed by half a dozen people, this kind of event
may only happen once in a blue moon. But when dealing with a monolith on which hundreds if not thousands of developers are
actively making changes every day, bugs are a fact of life.</p>

<p>As such, it’s important to adopt a defense-in-depth strategy, if you cannot possibly abolish all bugs, you can at least
limit their blast radius with various techniques.
And <a href="/ruby/performance/2025/02/09/guard-rails-are-not-code-smells.html">Unicorn’s process based execution model largely participated in the resiliency of the system</a>.</p>

<h2 id="its-not-all-rainbows-and-unicorns">It’s Not All Rainbows And Unicorns</h2>

<p>But while I’ll never cease to defend Unicorn’s design, I’m also perfectly able to recognize that it also has its downsides.</p>

<p>One is that Unicorn doesn’t attempt to protect against common attacks such as <a href="https://en.wikipedia.org/wiki/Slowloris_(cyber_attack)">slowloris</a>,
so it’s mandatory to put it behind a buffering reverse proxy such as NGINX.
You may consider this to be extra complexity, but to me, it’s the opposite.
Yes, it’s one more “moving piece”, but from my point of view, it’s less complex to defer many classic concerns to a battle-tested software used
across the world, with lots of documentation, rather than to trust my application server can safely be exposed directly
to the internet. I’d much rather trust the NGINX community to keep up with whatever novel attack was engineered last week
than rely on the part of the Ruby community that uses my app server of choice. Not that I distrust the Ruby community,
but my assumption is that the larger community is more likely to quickly get the security fixes in.</p>

<p>And if a reverse proxy will be involved anyway, you can let it take care of many standard concerns such as terminating SSL,
allowing newer versions of HTTP, serving static assets, etc. I don’t think that an extra moving piece brings extra complexity
when it is such a standard part of so many stacks and removes a ton of complexity from the next piece in the chain.
But that’s just me, I suppose, especially after reading some of the reactions to my previous posts, that not everybody
agree on what is complex and what is simple.</p>

<p>Another shortcoming of the multi-process design that’s often mentioned, is its inability to do efficient connection pooling.
Since connections aren’t easily shared across processes, each unicorn worker will maintain a separate pool of connections,
that will be idle most of the time.</p>

<p>But here too, there aren’t many alternatives. Even if you accept the tradeoff of using a threaded server, you will still need to
run at least one process per core, hence you won’t be able to cut the number of idle connections significantly compared to Unicorn.
You may be able to buy a bit of time that way, but sooner or later it won’t be enough.</p>

<p>Ultimately, once you scale past a certain size you kinda have to accept that external connection pooling is a necessity.
The only alternative I can think of would be to implement cross-process connection pooling by passing file descriptors via IPC.
It’s technically doable, but I can’t imagine myself arguing that it’s less complex than setting up <a href="https://proxysql.com/">ProxySQL</a>,
<a href="https://github.com/facebook/mcrouter">mcrouter</a> / <a href="https://github.com/twitter/twemproxy">twemproxy</a> etc.</p>

<p>Yet another complaint I heard, was that the multi-process design made it impossible to cache data in memory.
But here too I’m going to sound like a broken record, as long as Ruby doesn’t have a viable way to do in-process parallelism,
you will have to run at least one process per core, so trying to cache data in-process is never going to work well.</p>

<p>But even without that limitation, I’d still argue you’d be better not to use the heap as a cache because by doing so you
are creating extra work for the garbage collector, and anyway, all the caches would be wiped on every deploy, which may
be quite frequent, so I’d much rather run a small local Memcached instance on every web node, or use something like SQLite
or whatever. It’s a bit slower than in-memory caching, in part because it requires serialization, but it persists across
deploys and is shared across all the processes on the server, so have a much better hit ratio.</p>

<p>And finally, by far the most common complaint against the Unicorn model is the extra memory usage induced by processes,
and that’s exactly what Pitchfork was designed to solve.</p>

<h2 id="the-heap-janitor">The Heap Janitor</h2>

<p>Whenever I’m asked what my day-to-day job is like, I have a very hard time explaining it, because I kind of do an
amalgamation of lots of small things that aren’t necessarily all logically related. So it’s almost impossible for me to
come up with an answer that makes sense, and I don’t think I ever gave the same answer twice.
I also probably made a fool of myself more than once.</p>

<p>But among the many hats I occasionally wear, there’s one I call the “Heap Janitor”.
When you task hundreds if not thousands of developers to add features to a monolith, its memory usage will keep growing.
Some of that growth will be legitimate because every line of code has to reside somewhere in memory as VM bytecode, but some of it can
be reduced or eliminated by using better data structures, deduplicating some data, etc.</p>

<p>Most of the time when the Shopify monolith would experience a memory leak, or simply would have increased its memory
usage enough to be problematic, I’d get involved in the investigation.
Over time I developed some expertise on how to analyse a Ruby application’s heap, find leaks or opportunities for memory
usage reduction.</p>

<p>I even <a href="https://github.com/Shopify/heap-profiler">developed some dedicated tools</a> to help with that task, and integrated
them into CI so every morning I’d get a nightly report of what Shopify’s monolith heap is made of, to better see historical
trends and proactively fix newly introduced problems.</p>

<p>Once, <a href="https://github.com/rails/rails/pull/35860#issuecomment-480218928">by deduplicating the schema information Active Record keeps</a>,
I managed to reduce each process memory usage by 114MB, and by now I probably sent over a hundred patches to many gems
to reduce their memory usage, most <a href="https://github.com/dry-rb/dry-schema/pull/399">patches revolve around interning some strings</a>.</p>

<p>But while you can often find more compact ways to represent some data in memory, that can’t possibly compensate for the
new features being added constantly.</p>

<h2 id="the-miracle-cow">The Miracle CoW</h2>

<p>So by far, the most effective way to reduce an application’s memory usage is to allow
more memory to be shared between processes via Copy-on-Write, which in the case of Puma or Unicorn, means ensuring it’s loaded
during boot, and is never mutated after that.</p>

<p>Since the Shopify monolith runs in pretty large containers with 36 workers, if you load 1GiB of extra data in memory,
as long as you do it during boot and it is never mutated, thanks to Copy-on-Write that will only account for an extra
28MiB (<code class="language-plaintext highlighter-rouge">1024 / 36</code>) of actual memory usage per worker, which is perfectly reasonable.</p>

<p>Unfortunately, the lazy loading pattern is extremely common in Ruby code, I’m sure you’ve seen plenty of code like this:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">module</span> <span class="nn">SomeNamespace</span>
  <span class="k">class</span> <span class="o">&lt;&lt;</span> <span class="nb">self</span>
    <span class="k">def</span> <span class="nf">config</span>
      <span class="vi">@config</span> <span class="o">||=</span> <span class="no">YAML</span><span class="p">.</span><span class="nf">load_file</span><span class="p">(</span><span class="s2">"path/to/config.yml"</span><span class="p">)</span>
    <span class="k">end</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Here I used a YAML config file as an example, but sometimes it’s fetching or computing data from somewhere else,
they key point is <code class="language-plaintext highlighter-rouge">@ivar ||=</code> being done in a class or module method.</p>

<p>This pattern is good in development because it means that if you don’t need that data, you won’t waste time computing
it, but in production, it’s bad, because not only that memory won’t be in shared pages, it will also cause the first
request that needs this data to do some extra work, causing latency to spike around deploys.</p>

<p>A very simple way to improve this code is to just use a constant:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">module</span> <span class="nn">SomeNamespace</span>
  <span class="no">CONFIG</span> <span class="o">=</span> <span class="no">YAML</span><span class="p">.</span><span class="nf">load_file</span><span class="p">(</span><span class="s2">"path/to/config.yml"</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div></div>

<p>But if for some reason you really want this to be lazily loaded in development,
<a href="https://guides.rubyonrails.org/configuring.html#config-eager-load-namespaces">Rails offers a not-so-well-known API</a> to
help with that:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">module</span> <span class="nn">SomeNamespace</span>
  <span class="k">class</span> <span class="o">&lt;&lt;</span> <span class="nb">self</span>
    <span class="k">def</span> <span class="nf">eager_load!</span>
      <span class="n">config</span>
    <span class="k">end</span>

    <span class="k">def</span> <span class="nf">config</span>
      <span class="vi">@config</span> <span class="o">||=</span> <span class="no">YAML</span><span class="p">.</span><span class="nf">load_file</span><span class="p">(</span><span class="s2">"path/to/config.yml"</span><span class="p">)</span>
    <span class="k">end</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="c1"># in: config/application.rb</span>
<span class="n">config</span><span class="p">.</span><span class="nf">eager_load_namespaces</span> <span class="o">&lt;&lt;</span> <span class="no">SomeNamespace</span>
</code></pre></div></div>

<p>In the above example, Rails takes care of calling <code class="language-plaintext highlighter-rouge">eager_load!</code> on all objects you add to <code class="language-plaintext highlighter-rouge">config.eager_load_namespaces</code>
when it’s booted in production mode. This way you keep lazy loading in development environments, but get eager loading
in production.</p>

<p>I spent a lot of time improving Shopify’s monolith and its open-source dependencies to make it eager-load more.
To help me track down the offending call sites, I configured <a href="https://github.com/Shopify/app_profiler">our profiling middleware</a>
so that it would automatically trigger profiling of the very first request processed by a worker.
And similarly, I configured our Unicorn so that a few workers would dump their heap with <a href="https://docs.ruby-lang.org/en/3.4/ObjectSpace.html#method-i-dump_all"><code class="language-plaintext highlighter-rouge">ObjectSpace.dump_all</code></a>
before and after their very first request.</p>

<p>On paper, every object allocated as part of a Rails request is supposed to no longer be referenced once the request has been completed.
So by taking a heap snapshot before and after a request, and making a diff of them, you can locate any object that should
have been eager loaded during boot.</p>

<p>Over time this data helped me increase the amount of shared memory, from something around <code class="language-plaintext highlighter-rouge">45%</code> up to about <code class="language-plaintext highlighter-rouge">60%</code> of the total,
hence significantly reduced the memory usage of individual workers, but I was hitting diminishing returns.</p>

<p><code class="language-plaintext highlighter-rouge">60%</code> is good, but I was hoping for more. In theory, only the memory allocated as part of the request cycle can’t be shared,
the overwhelming majority of the rest of the objects should be shareable, so I was expecting the ratio of shared memory
to be more akin to <code class="language-plaintext highlighter-rouge">80%</code>, which begged the question, which memory still wasn’t shared?</p>

<h2 id="inline-caches">Inline Caches</h2>

<p>For a while I tried to answer this question using eBPF probes, but after reading man pages for multiple days,
I had to accept that these sorts of things fly over my head<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, so I gave up.</p>

<p>But one day I had a revelation: It must be the inline caches!</p>

<p>A very large portion of the Shopify monolith heap is comprised of VM bytecode, as mentioned previously, all the code
written by all these developers has to end up somewhere. That bytecode is largely immutable but very close to it there
are inline caches<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> and they are mutable, at least early.</p>

<p>And if they are close together in the heap, mutating an inline cache would invalidate the entire 4kiB page, including lots
of immutable objects on the same page.</p>

<p>To validate my assumption, I wrote a test application:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">module</span> <span class="nn">App</span>
  <span class="no">CONST_NUM</span> <span class="o">=</span> <span class="no">Integer</span><span class="p">(</span><span class="no">ENV</span><span class="p">.</span><span class="nf">fetch</span><span class="p">(</span><span class="s2">"NUM"</span><span class="p">,</span> <span class="mi">100_000</span><span class="p">))</span>

  <span class="no">CONST_NUM</span><span class="p">.</span><span class="nf">times</span> <span class="k">do</span> <span class="o">|</span><span class="n">i</span><span class="o">|</span>
    <span class="nb">class_eval</span><span class="p">(</span><span class="o">&lt;&lt;~</span><span class="no">RUBY</span><span class="p">,</span> <span class="kp">__FILE__</span><span class="p">,</span> <span class="kp">__LINE__</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="sh">
      Const</span><span class="si">#{</span><span class="n">i</span><span class="si">}</span><span class="sh"> = Module.new

      def self.lookup_</span><span class="si">#{</span><span class="n">i</span><span class="si">}</span><span class="sh">
        Const</span><span class="si">#{</span><span class="n">i</span><span class="si">}</span><span class="sh">
      end
</span><span class="no">    RUBY</span>
  <span class="k">end</span>

  <span class="nb">class_eval</span><span class="p">(</span><span class="o">&lt;&lt;~</span><span class="no">RUBY</span><span class="p">,</span> <span class="kp">__FILE__</span><span class="p">,</span> <span class="kp">__LINE__</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="sh">
    def self.warmup
      </span><span class="si">#{</span><span class="no">CONST_NUM</span><span class="p">.</span><span class="nf">times</span><span class="p">.</span><span class="nf">map</span> <span class="p">{</span> <span class="o">|</span><span class="n">i</span><span class="o">|</span> <span class="s2">"lookup_</span><span class="si">#{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span><span class="si">}</span><span class="sh">.join("</span><span class="se">\n</span><span class="sh">")}
    end
</span><span class="no">  RUBY</span>
<span class="k">end</span>
</code></pre></div></div>

<p>It uses meta-programming, but is rather simple, it defines 100k methods, each referencing a unique constant.
If I removed the meta-programing it would look like this:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">module</span> <span class="nn">App</span>
  <span class="no">Const0</span> <span class="o">=</span> <span class="no">Module</span><span class="p">.</span><span class="nf">new</span>
  <span class="k">def</span> <span class="nc">self</span><span class="o">.</span><span class="nf">lookup_0</span>
    <span class="no">Const0</span>
  <span class="k">end</span>

  <span class="no">Const1</span> <span class="o">=</span> <span class="no">Module</span><span class="p">.</span><span class="nf">new</span>
  <span class="k">def</span> <span class="nc">self</span><span class="o">.</span><span class="nf">lookup_1</span>
    <span class="no">Const1</span>
  <span class="k">end</span>

  <span class="k">def</span> <span class="nc">self</span><span class="o">.</span><span class="nf">warmup</span>
    <span class="n">lookup_0</span>
    <span class="n">lookup_1</span>
    <span class="c1"># snip...</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Why this pattern? Because it’s a good way to generate a lot of inline caches, constant caches in this case, and to
trigger their warmup.</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;</span> <span class="nb">puts</span> <span class="no">RubyVM</span><span class="o">::</span><span class="no">InstructionSequence</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="s1">'Const0'</span><span class="p">).</span><span class="nf">disasm</span>
<span class="o">==</span> <span class="ss">disasm: </span><span class="c1">#&lt;ISeq:&lt;compiled&gt;@&lt;compiled&gt;:1 (1,0)-(1,6)&gt;</span>
<span class="mo">0000</span> <span class="n">opt_getconstant_path</span>                   <span class="o">&lt;</span><span class="n">ic</span><span class="p">:</span><span class="mi">0</span> <span class="no">Const0</span><span class="o">&gt;</span>             <span class="p">(</span>   <span class="mi">1</span><span class="p">)[</span><span class="no">Li</span><span class="p">]</span>
<span class="mo">0002</span> <span class="n">leave</span>
</code></pre></div></div>

<p>Here the <code class="language-plaintext highlighter-rouge">&lt;ic:0&gt;</code> tells us this instructions has an associated inline cache.
These constant caches start uninitialized, and the first time this codepath is executed, the Ruby VM goes through
the slow process of finding the object that’s pointed by that constant, and stores it in the cache.
On further execution, it just needs to check the cache wasn’t invalidated, which for constants is extremely rare unless
you are doing some really nasty meta programming during runtime.</p>

<p>Now, using this app, we can demonstrate the effect of inline caches on Copy-on-Write effectiveness:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">show_pss</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
  <span class="c1"># Easy way to get PSS on Linux</span>
  <span class="nb">print</span> <span class="n">title</span><span class="p">.</span><span class="nf">ljust</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="s2">" "</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="no">File</span><span class="p">.</span><span class="nf">read</span><span class="p">(</span><span class="s2">"/proc/self/smaps_rollup"</span><span class="p">).</span><span class="nf">scan</span><span class="p">(</span><span class="sr">/^Pss: (.*)$/</span><span class="p">)</span>
<span class="k">end</span>

<span class="n">show_pss</span><span class="p">(</span><span class="s2">"initial"</span><span class="p">)</span>

<span class="n">pid</span> <span class="o">=</span> <span class="nb">fork</span> <span class="k">do</span>
  <span class="n">show_pss</span><span class="p">(</span><span class="s2">"after fork"</span><span class="p">)</span>

  <span class="no">App</span><span class="p">.</span><span class="nf">warmup</span>
  <span class="n">show_pss</span><span class="p">(</span><span class="s2">"after fork after warmup"</span><span class="p">)</span>
<span class="k">end</span>
<span class="no">Process</span><span class="p">.</span><span class="nf">wait</span><span class="p">(</span><span class="n">pid</span><span class="p">)</span>
</code></pre></div></div>

<p>If you run the above script on Linux, you should get something like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>initial                                    246380 kB
after fork                                 121590 kB
after fork after warmup                    205688 kB
</code></pre></div></div>

<p>So our synthetic <code class="language-plaintext highlighter-rouge">App</code> made our initial Ruby process grow to <code class="language-plaintext highlighter-rouge">246MB</code>, and once we forked a child, its
<a href="https://en.wikipedia.org/wiki/Proportional_set_size">proportionate memory usage</a> was immediately cut in half as expected.
However once <code class="language-plaintext highlighter-rouge">App.warmup</code> is called in the child, all these inline caches end up initialized, and most of the Copy-on-Write
pages get invalidated, making the proportionate memory usage grow back to <code class="language-plaintext highlighter-rouge">205MB</code>.</p>

<p>So you probably guessed the next step, if you can call <code class="language-plaintext highlighter-rouge">App.warmup</code> before forking, you stand to save a ton of memory:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">show_pss</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
  <span class="c1"># Easy way to get PSS on Linux</span>
  <span class="nb">print</span> <span class="n">title</span><span class="p">.</span><span class="nf">ljust</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="s2">" "</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="no">File</span><span class="p">.</span><span class="nf">read</span><span class="p">(</span><span class="s2">"/proc/self/smaps_rollup"</span><span class="p">).</span><span class="nf">scan</span><span class="p">(</span><span class="sr">/^Pss: (.*)$/</span><span class="p">)</span>
<span class="k">end</span>

<span class="n">show_pss</span><span class="p">(</span><span class="s2">"initial"</span><span class="p">)</span>
<span class="no">App</span><span class="p">.</span><span class="nf">warmup</span>
<span class="n">show_pss</span><span class="p">(</span><span class="s2">"after warmup"</span><span class="p">)</span>

<span class="n">pid</span> <span class="o">=</span> <span class="nb">fork</span> <span class="k">do</span>
  <span class="n">show_pss</span><span class="p">(</span><span class="s2">"after fork"</span><span class="p">)</span>

  <span class="no">App</span><span class="p">.</span><span class="nf">warmup</span>
  <span class="n">show_pss</span><span class="p">(</span><span class="s2">"after fork after warmup"</span><span class="p">)</span>
<span class="k">end</span>
<span class="no">Process</span><span class="p">.</span><span class="nf">wait</span><span class="p">(</span><span class="n">pid</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>initial                                    246404 kB
after warmup                               251140 kB
after fork                                 123944 kB
after fork after warmup                    124240 kB
</code></pre></div></div>

<p>My theory was somewhat validated.
If I found a way to fill inline caches before fork, I’d stand to achieve massive memory savings.
Some would for sure continue to flip-flop like inline method caches in polymorphic code paths,
but the vast majority of them would essentially be static memory.</p>

<p>However, that was easier said than done.</p>

<p>Generally, when I mentioned that problem, the suggestion was to exercise these code paths as part of boot, but
it already isn’t easy to get good coverage in the test environment, it would be even harder during boot in the production
environment.
Even worse, many of these code paths have side effects, you can’t just run them like that out of context. Anyway, with something like this in place, the application would take ages to boot, and it would be painful to maintain.</p>

<p>Another idea was to attempt to precompute these caches statically, which for constant caches is relatively easy.
But it’s only part of the picture, method caches, and instance variable caches are much harder, if not impossible to predict
statically, so perhaps it would help a bit, but it wouldn’t solve the issue once and for all.</p>

<p>Given all these types of caches are stored right next to each other, as soon as a single one changes, the entire <code class="language-plaintext highlighter-rouge">4kiB</code> memory page is invalidated.</p>

<p>Yet another suggestion was to serve traffic for a while from the Unicorn master process, but I didn’t like this
idea because that process is in charge of overseeing and coordinating all the workers, it can’t afford to render
requests, as it can’t be timed out.</p>

<h2 id="pumas-fork-worker">Puma’s Fork Worker</h2>

<p>That idea lived in my head for quite some time, not too sure how long but certainly months, until one day I noticed
an experimental feature in Puma: <a href="https://github.com/puma/puma/pull/2099"><code class="language-plaintext highlighter-rouge">fork_worker</code></a>.
Someone had identified the same issue, or at least a very similar one, and came up with an interesting idea.</p>

<p>It would initially start Puma in a normal way, with the cluster process overseeing its workers, but after a while you
could trigger a mechanism that would cause all workers except the first one to shut down, and be replaced not by
forking from the cluster process, but from the remaining worker.</p>

<p>So in terms of process hierarchy, you’d go from:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>10000   \_ puma 4.3.3 (tcp://0.0.0.0:9292) [puma]
10001       \_ puma: cluster worker 0: 10000 [puma]
10002       \_ puma: cluster worker 1: 10000 [puma]
10003       \_ puma: cluster worker 2: 10000 [puma]
10004       \_ puma: cluster worker 3: 10000 [puma]
</code></pre></div></div>

<p>To:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>10000   \_ puma 4.3.3 (tcp://0.0.0.0:9292) [puma]
10001       \_ puma: cluster worker 0: 10000 [puma]
10005           \_ puma: cluster worker 1: 10000 [puma]
10006           \_ puma: cluster worker 2: 10000 [puma]
10007           \_ puma: cluster worker 3: 10000 [puma]
</code></pre></div></div>

<p>I found the solution quite brilliant, rather than trying to exercise code paths in some automated way, just let live traffic
do it and then share that state with other workers. Simple.</p>

<p>But I had a major reservation with that feature, it’s that if you use it you end up with 3 levels of processes,
and as I explained in <a href="/ruby/performance/2025/02/09/guard-rails-are-not-code-smells.html">my post about how guardrails are important</a>,
if anything goes wrong, I want to be able to terminate any worker safely.</p>

<p>In this case, what happens if <code class="language-plaintext highlighter-rouge">worker 0</code> is terminated or crashes by itself? Other workers end up orphaned, which in POSIX
means that they’ll be adopted by the PID 1, AKA the init process, not the Puma cluster process and that’s a major resiliency issue,
as Puma needs the workers to be its direct children for various things.
For this to be resilient, you’d need to fork these workers as siblings, not children, and that’s just not possible.</p>

<p>I really couldn’t reasonably consider deploying Shopify’s monolith this way, it would for sure bite us hard soon enough.
Yet, I was really curious about how effective it could be, so I set an experiment to have a single container in the canary
environment to use Puma with this feature enabled for a while, and it performed both fantastically and horribly.</p>

<p>Fantastically because the memory gains were absolutely massive, and horribly because the newly spawned workers started
raising errors from the <code class="language-plaintext highlighter-rouge">grpc</code> gem.
Errors that I knew relatively well because they came from <a href="https://github.com/grpc/grpc/pull/16332">a safety check added a few years prior in the <code class="language-plaintext highlighter-rouge">grpc</code> gem by one of my coworkers</a>
to prevent <code class="language-plaintext highlighter-rouge">grpc</code> from deadlocking in the presence of <code class="language-plaintext highlighter-rouge">fork</code>.</p>

<p>In addition to my reservations about process parenting, it was also clear that making the <code class="language-plaintext highlighter-rouge">grpc</code> gem fork-safe would
be almost impossible.
So I shoved that idea in the drawer with all the other good ideas that will never be and moved on.</p>

<h2 id="child-subreaper">Child Subreaper</h2>

<p>Until one day, I’m not too sure how long after, I was searching for a solution to a different problem, in <a href="https://man7.org/linux/man-pages/man2/prctl.2.html">the
<code class="language-plaintext highlighter-rouge">prctl(2)</code> manpage</a>, and I stumbled upon <a href="https://man7.org/linux/man-pages/man2/PR_SET_CHILD_SUBREAPER.2const.html">the <code class="language-plaintext highlighter-rouge">PR_SET_CHILD_SUBREAPER</code>
constant</a>.</p>

<blockquote>
  <p>If set is nonzero, set the “child subreaper” attribute of the
calling process; if set is zero, unset the attribute.</p>

  <p>A subreaper fulfills the role of init(1) for its descendant
processes.  When a process becomes orphaned (i.e., its immediate
parent terminates), then that process will be reparented to the
nearest still living ancestor subreaper.</p>
</blockquote>

<p>This was exactly the feature I didn’t know existed and didn’t know I wanted, to make Puma’s experimental feature more robust.</p>

<p>If you’d enable <code class="language-plaintext highlighter-rouge">PR_SET_CHILD_SUBREAPER</code> on the Puma cluster process, the <code class="language-plaintext highlighter-rouge">worker 0</code> would be able to spawn siblings
by doing the classic daemonization procedure: forking a grandchild, and orphaning it.
This would cause the new worker to be reparented to the Puma cluster process, effectively allowing you to fork a sibling.</p>

<p>Additionally, at that point, we were running YJIT in production, which made our memory usage situation noticeably worse, so we had to use tricks to enable it only on a subset of workers.</p>

<p>By definition, JIT compilers generate code at runtime, that is a lot of memory that can’t be in shared pages.
If I could make this idea work in production, that would allow JITed code to be shared, making the potential savings
even bigger.</p>

<p>So I then proceeded to spend the next couple weeks prototyping.</p>

<h2 id="the-very-first-prototype">The Very First Prototype</h2>

<p>I both tried to improve Puma’s feature and also to add the feature to Unicorn to see which would be the simplest.</p>

<p>It is probably in big part due to my higher familiarity with Unicorn, but I found it easier to do in Unicorn,
and proceeded to <a href="https://yhbt.net/unicorn-public/aecd9142-94cf-b195-34f3-bea4870ed9c8@shopify.com/T/">send a patch to the mailing list</a>.</p>

<p>The first version of the patch actually didn’t use <code class="language-plaintext highlighter-rouge">PR_SET_CHILD_SUBREAPER</code> because it is a Linux-only feature, and Unicorn
support all POSIX systems.
Instead, I built on Unicorn’s zero-downtime restart functionality, I’d fork a new master process and proceed to shutdown
the old one, and replace the pidfile.</p>

<p>To help you picture it better, starting from a classic Unicorn process tree:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PID     Proctitle

1000   \_ unicorn master
1001       \_ unicorn worker 0
1002       \_ unicorn worker 1
1003       \_ unicorn worker 2
1004       \_ unicorn worker 3
</code></pre></div></div>

<p>Once you trigger reforking, the worker starts to behave like a new master:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PID     Proctitle

1000   \_ unicorn master
1001       \_ unicorn master, generation 2
1002       \_ unicorn worker 1
1003       \_ unicorn worker 2
1004       \_ unicorn worker 3
</code></pre></div></div>

<p>Then the old and new master processes would progressively shut down and spawn their workers respectively:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PID     Proctitle

1000   \_ unicorn master
1001       \_ unicorn master, generation 2
1005         \_ unicorn worker 0, generation 2
1006         \_ unicorn worker 1, generation 2
1003       \_ unicorn worker 2
1004       \_ unicorn worker 3
</code></pre></div></div>

<p>Until the old master has no workers left, at which point it exits.</p>

<p>This approach had the benefit of working on all POSIX systems, however, it was very brittle and required launching Unicorn
in daemonized mode, which isn’t what you want in containers and most modern deployment systems.</p>

<p>I was also relying on creating named pipes in the file system to allow the master process and workers to have a communication pipe,
which really wasn’t elegant at all.</p>

<p>But that was enough to send a patch and get some feedback on whether such a feature was desired upstream, as well as feedback on the implementation.</p>

<h2 id="inter-process-communication">Inter-Process Communication</h2>

<p>In Unicorn, the master process has to be able to communicate with its workers, for instance, to ask them to shut down,
this sort of thing.</p>

<p>The easiest way to do inter-process communication is to send a signal, but it limits you to just a few predefined
signals, many of which already have a meaning.
In addition, signals are handled asynchronously, so they tend to interrupt system calls and can generally conflict with
the running application.</p>

<p>So what Unicorn does is that it implements “soft signals”. Instead of sending real signals, before spawning each
workers, it creates a pipe, and the children look for messages from the master process in between processing two requests.</p>

<p>Here’s a simplified example of how it works.</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">spawn_worker</span>
  <span class="n">read_pipe</span><span class="p">,</span> <span class="n">write_pipe</span> <span class="o">=</span> <span class="no">IO</span><span class="p">.</span><span class="nf">pipe</span>
  <span class="n">child_pip</span> <span class="o">=</span> <span class="nb">fork</span> <span class="k">do</span>
    <span class="n">write_pipe</span><span class="p">.</span><span class="nf">close</span>
    <span class="kp">loop</span> <span class="k">do</span>
      <span class="n">ready_ios</span> <span class="o">=</span> <span class="no">IO</span><span class="p">.</span><span class="nf">select</span><span class="p">([</span><span class="n">read_pipe</span><span class="p">,</span> <span class="vi">@server_socket</span><span class="p">])</span>
      <span class="n">ready_ios</span><span class="p">.</span><span class="nf">each</span> <span class="k">do</span> <span class="o">|</span><span class="n">io</span><span class="o">|</span>
        <span class="k">if</span> <span class="n">io</span> <span class="o">==</span> <span class="n">read_pipe</span>
          <span class="c1"># handle commands sent by the parent process in the pipe</span>
        <span class="k">else</span>
          <span class="c1"># handle HTTP request</span>
        <span class="k">end</span>
      <span class="k">end</span>
    <span class="k">end</span>
  <span class="k">end</span>
  <span class="n">read_pipe</span><span class="p">.</span><span class="nf">close</span>
  <span class="p">[</span><span class="n">child_pid</span><span class="p">,</span> <span class="n">write_pipe</span><span class="p">]</span>
<span class="k">end</span>
</code></pre></div></div>

<p>The master process keeps the writing end of the pipe, and the worker the reading end.
Whenever it is idle, a worker waits for either the command pipe or the HTTP socket to have something to read using
either <code class="language-plaintext highlighter-rouge">epoll</code>, <code class="language-plaintext highlighter-rouge">kqueue</code> or <code class="language-plaintext highlighter-rouge">select</code>. In this example, I just use Ruby’s provided <code class="language-plaintext highlighter-rouge">IO.select</code>, which is functionally equivalent.</p>

<p>With this in place, the Unicorn master always has both the PID and a communication pipe to all its workers.</p>

<p>But in my case, I wanted the master to be able to know about workers it didn’t spawn itself.
For the PID, it wasn’t that hard, I could just create a second pipe, but in the opposite direction, so that workers
would be able to send a message to the master to let it know about the new worker PID.
But how to establish the communication pipe with the grandparent?</p>

<p>That’s why my first prototype used named pipes, also known as FIFO, which are exactly like regular pipes, except they are
exposed as files on the file system tree. This way the master to look for a named pipe at an agreed-upon location, and
have a way to send messages to its grandchildren. It worked but as Unicorn’s maintainer, pointed out in his feedback, there
was a much cleaner solution, <a href="https://man7.org/linux/man-pages/man2/socketpair.2.html"><code class="language-plaintext highlighter-rouge">socketpair(2)</code></a> and
<a href="https://docs.ruby-lang.org/en/3.4/UNIXSocket.html#method-i-send_io"><code class="language-plaintext highlighter-rouge">UNIXSocket#send_io</code></a>.</p>

<p>First, <code class="language-plaintext highlighter-rouge">socketpair(2)</code> as its name implies creates two sockets that are connected to each other, so it’s very similar
to pipes but is bidirectional. Since I needed two-way communication between processes, that was simpler and cleaner than
creating two pipes each time.</p>

<p>But then, a little-known capability of UNIX domain sockets (at least I didn’t know about it), is that they allow you to
pass file descriptors to another process. Here’s a quick demo in Ruby:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">require</span> <span class="s1">'socket'</span>
<span class="nb">require</span> <span class="s1">'tempfile'</span>

<span class="n">parent_socket</span><span class="p">,</span> <span class="n">child_socket</span> <span class="o">=</span> <span class="no">UNIXSocket</span><span class="p">.</span><span class="nf">socketpair</span>

<span class="n">child_pid</span> <span class="o">=</span> <span class="nb">fork</span> <span class="k">do</span>
  <span class="n">parent_socket</span><span class="p">.</span><span class="nf">close</span>

  <span class="c1"># Create a file that doesn't exist on the file system</span>
  <span class="n">file</span> <span class="o">=</span> <span class="no">Tempfile</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span><span class="ss">anonymous: </span><span class="kp">true</span><span class="p">)</span>
  <span class="n">file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="s2">"Hello"</span><span class="p">)</span>
  <span class="n">file</span><span class="p">.</span><span class="nf">rewind</span>

  <span class="n">child_socket</span><span class="p">.</span><span class="nf">send_io</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
  <span class="n">file</span><span class="p">.</span><span class="nf">close</span>
<span class="k">end</span>
<span class="n">child_socket</span><span class="p">.</span><span class="nf">close</span>

<span class="n">child_io</span> <span class="o">=</span> <span class="n">parent_socket</span><span class="p">.</span><span class="nf">recv_io</span>
<span class="nb">puts</span> <span class="n">child_io</span><span class="p">.</span><span class="nf">read</span>
<span class="no">Process</span><span class="p">.</span><span class="nf">wait</span><span class="p">(</span><span class="n">child_pid</span><span class="p">)</span>
</code></pre></div></div>

<p>In the above example, we have the child process create an anonymous file and share it with its parent through a UNIX
domain socket.</p>

<p>With this new capability, I could make the design much less brittle. Now when a new worker was spawned, it could send
a message to the master process with all the necessary metadata as well as an attached socket for direct communication
with the new worker.</p>

<h2 id="the-decision-to-fork">The Decision To Fork</h2>

<p>Thanks to Eric Wong’s suggestions, I started to have a much neater design based around <code class="language-plaintext highlighter-rouge">PR_SET_CHILD_SUBREAPER</code> but at that
point rather than continue to attempt to upstream that new feature in Unicorn, I chose to instead fork the project under
a different name for multiple reasons.</p>

<p>First, it became clear that several Unicorn features were hard to make work in conjunction with reforking.
Not impossible, but it would have required quite a lot of effort, and ultimately it would induce a risk that I’d break Unicorn
for some of its users.</p>

<p>Unicorn also isn’t the easiest project to contribute to.
It has a policy of supporting very old versions of Ruby, many of them lacking features I wanted to use,
and hard to install on modern systems, making debugging extra hard.
It also doesn’t use bundler nor most of the modern Ruby tooling, which makes it hard to contribute to for many people,
has its own bash-based unit test framework,
and accept patches over a mailing list rather than some forge.</p>

<p>I wouldn’t go as far as to say Unicorn is hostile to outside contributions, as it’s not the intent,
but in practice it kinda is.</p>

<p>So if I had to make large changes to support that new feature, it was preferable to do it as a different project,
one that wouldn’t impact the existing user base in case of mistakes, and one I’d be in control of, allowing me to
iterate and release quickly based on production experience.</p>

<p>That’s why I decided to fork. I started by removing many of Unicorn’s features that I believe aren’t useful in a modern
container-based world, removing the dependency on <code class="language-plaintext highlighter-rouge">kgio</code> in favor of using the non-locking IO APIs introduced in newer
versions of Ruby.</p>

<p>From that simplified Unicorn base I could more easily do a clean and robust implementation of the feature I wanted
without having the constraint of not breaking features I didn’t need.</p>

<p>The nice thing when you start a new project is that you get to choose a name for it.
Initially, I wanted to continue the trend of naming Ruby web servers after animals and possibly marking the lineage with
Unicorn by naming it after another mythical animal.
So for a while, I considered naming the new project <a href="https://en.wikipedia.org/wiki/Dahu">Dahu</a>,
but ultimately I figured something with <code class="language-plaintext highlighter-rouge">fork</code> in the name would be more catchy.
Unfortunately, it’s very hard to find names on Rubygems that haven’t been taken yet, but I decided to send a mail to
the person who owned the <code class="language-plaintext highlighter-rouge">pitchfork</code> gem, which was long abandoned, and they very gracefully transferred the gem to me.
That’s how <code class="language-plaintext highlighter-rouge">pitchfork</code> was born.</p>

<h2 id="the-mold-process">The Mold Process</h2>

<p>Now that I could more significantly change the server, I decided to move the responsibility of spawning new workers
out of the master process, which I renamed “monitor process” for the occasion.</p>

<p>In Unicorn, assuming you use the <code class="language-plaintext highlighter-rouge">preload_app</code> option to better benefit from Copy-on-Write, new workers are forked from
the master process, but that master process never serves any request, so all the application code it loaded is never called.
In addition, if you are running in a container, you can’t reasonably replace the initial process.</p>

<p>What I did instead is that Pitchfork’s monitor process never loads the application code, instead it gives that responsibility
to the first child it spawns: the “mold”. That mold process is responsible for loading the application, and spawning
new workers when ordered to do so by the “monitor” process. The process tree initially looks like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PID     Proctitle

1000   \_ pitchfork monitor
1001       \_ pitchfork mold
</code></pre></div></div>

<p>Then, once the mold is fully booted, the monitor sends requests to spawn workers, which the mold does using the classic double fork:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PID     Proctitle

1000   \_ pitchfork monitor
1001       \_ pitchfork mold
1002          \_ pitchfork init-worker
1003             \_ pitchfork worker 0
</code></pre></div></div>

<p>Once the <code class="language-plaintext highlighter-rouge">init-worker</code> process exits, <code class="language-plaintext highlighter-rouge">worker 0</code> becomes an orphan and is automatically reparented to the monitor:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PID     Proctitle

1000   \_ pitchfork monitor
1001       \_ pitchfork mold
1003       \_ pitchfork worker 0
</code></pre></div></div>

<p>Since all workers and the mold are at the same level, whenever we decide to do so, we can declare that a worker is now the new
mold, and respawn all other workers from it:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PID     Proctitle

1000   \_ pitchfork monitor
1001       \_ pitchfork mold &lt;exiting&gt;
1003       \_ pitchfork mold, generation 2
1005       \_ pitchfork worker 0, generation 2
1007       \_ pitchfork worker 1, generation 2
</code></pre></div></div>

<p>All of this of course being done progressively, one worker at a time, to avoid significantly reducing the capacity
of the server.</p>

<h2 id="benchmarking">Benchmarking</h2>

<p>After that, I turned my constant cache demo into <a href="https://github.com/Shopify/pitchfork/tree/b70ee3c8700a997ee9513c81709b91062cc79ca1/benchmark">a memory usage benchmark for Rack servers</a>,
and that early version of Pitchfork performed as well as I hoped.</p>

<p>Compared to Puma with 2 workers and 2 threads, Pitchfork configured with 4 processes would use half the memory:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ PORT=9292 bundle exec benchmark/cow_benchmark.rb puma -w 2 -t 2 --preload
Booting server...
Warming the app with ab...
Memory Usage:
Single Worker Memory Usage: 207.5 MiB
Total Cluster Memory Usage: 601.6 MiB
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ PORT=8080 bundle exec benchmark/cow_benchmark.rb pitchfork -c examples/pitchfork.conf.minimal.rb 
Booting server...
Warming the app with ab...
Memory Usage:
Single Worker Memory Usage: 62.6 MiB
Total Cluster Memory Usage: 320.3 MiB
</code></pre></div></div>

<p>Of course, this is an extreme micro-benchmark for demonstration purposes, and not indicative of the effect on any
given real application in production, but it was very encouraging.</p>

<h2 id="the-bumpy-road-to-production">The Bumpy Road To Production</h2>

<p>Writing a new server, and benchmarking it, is the fun and easy part, and you can probably spend months ironing it out
if you so wish.</p>

<p>But it’s only once you attempt to put it in production that you’ll learn of all the mistakes you made and all the
problems you didn’t think of.</p>

<p>In this particular case though, there was one major blocker I did know of, and that I did know I had to solve
before even attempting to put Pitchfork in production: my old nemesis, the <code class="language-plaintext highlighter-rouge">grpc</code> gem.</p>

<p>I have a very long history of banging my head against my desk trying to fix compilation issues in that gem,
or figuring out leaks and other issues, so I knew making it fork-safe wouldn’t be an easy task.</p>

<p>To give you an idea of how much of a juggernaut it is, here’s a <code class="language-plaintext highlighter-rouge">sloccount</code> report from the
source package, hence excluding tests, etc:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cloc --include-lang='C,C++,C/C++ Header' .
-----------------------------------------------------------------
Language       files          blank        comment           code
-----------------------------------------------------------------
C/C++ Header    1797          43802          96161         309150
C++              983          35199          53621         261047
C                463           9020           8835          81831
-----------------------------------------------------------------
SUM:            3243          88021         158617         652028
-----------------------------------------------------------------
</code></pre></div></div>

<p>Depending on whether you consider that headers are code or not, that is either
significantly bigger than Ruby’s own source code, or about as big.</p>

<p>Here’s the same <code class="language-plaintext highlighter-rouge">sloccount</code> in <code class="language-plaintext highlighter-rouge">ruby/ruby</code> excluding tests and default gems for comparison:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cloc --include-lang='C,C++,C/C++ Header' --exclude-dir=test,spec,-test-,gems,trans,build .
------------------------------------------------------------------
Language        files          blank        comment           code
------------------------------------------------------------------
C                 304          51562          83404         315614
C/C++ Header      406           8588          32604          84751
------------------------------------------------------------------
SUM:              710          60150         116008         400365
------------------------------------------------------------------
</code></pre></div></div>

<p>And to that, you’d also need to add the <code class="language-plaintext highlighter-rouge">google-protobuf</code> gem that works in hand with <code class="language-plaintext highlighter-rouge">grpc</code> and is also quite
sizeable.</p>

<p>Because of that, rather than try to make <code class="language-plaintext highlighter-rouge">grpc</code> fork-safe, I first tried to see if I could instead eliminate that
problematic dependency, given that after all, it was barely used in the monolith. It was only used to call a single service.
Unfortunately, I wasn’t capable of convincing the team using that gem to move to something else.</p>

<p>I later attempted to find a way to make the library fork-safe, but I was forced to admit I wasn’t capable of it.
All I managed to do was figure out that <a href="https://github.com/grpc/grpc/blob/master/doc/fork_support.md#current-status">the Python bindings had optional support for fork safety behind an environment
variable</a>.
That confirmed it was theoretically possible, but still beyond my capacities.</p>

<p>So I wasn’t happy about it, but I had to abandon the Pitchfork project. It just wasn’t viable as long as <code class="language-plaintext highlighter-rouge">grpc</code> remained
a dependency.</p>

<p>A few months later, a colleague who probably heard me cursing across the Atlantic Ocean asked if he could help.
Given that fork-safety was supported by the Python version of <code class="language-plaintext highlighter-rouge">grpc</code>, and that Shopify is a big Google Cloud customer
with a very high tier of support, he thought he could pull a few strings and get Google to implement it.
And he was right, it took a long time, probably something like six months, but
<a href="https://github.com/grpc/grpc/pull/33430">the <code class="language-plaintext highlighter-rouge">grpc</code> gem did end up gaining fork support</a>.</p>

<p>And just like that, after being derailed for half a year, the Pitchfork project was back on track, so a big thanks to
Alexander Polcyn for improving <code class="language-plaintext highlighter-rouge">grpc</code>.</p>

<h2 id="fixing-other-fork-safety-issues">Fixing Other Fork Safety Issues</h2>

<p>At that point, it was clear there were other issues than <code class="language-plaintext highlighter-rouge">grpc</code>, but I had some confidence I’d be able to
tackle them. Even without enabling reforking, it was advantageous to replace Unicorn with Pitchfork in production,
as to confirm no bugs were introduced in the HTTP and IO layers, but also because it allowed us to remove
our dependency on <code class="language-plaintext highlighter-rouge">kgio</code>, unlocked compatibility with <code class="language-plaintext highlighter-rouge">rack 3</code>, and a few other small things.
So that was the first step.</p>

<p>Then, fixing the fork safety issues other than <code class="language-plaintext highlighter-rouge">grpc</code> took approximately another month.</p>

<p>The first thing I did was to <a href="https://github.com/minitest/minitest/pull/961#issuecomment-1654393109">simulate reforking on CI</a>.
Every 100 tests or so, CI workers would refork the same way Pitchfork does. This uncovered fork-safety issues
in other gems, notably <code class="language-plaintext highlighter-rouge">ruby-vips</code>.
Luckily this gem wasn’t used much by web workers, so I devised a new strategy to deal with it.</p>

<p>Pitchfork doesn’t actually need all workers to be fork-safe, only the ones that will be promoted into the next mold.
So if some libraries cause workers to become fork unsafe once they’ve been used, like <code class="language-plaintext highlighter-rouge">ruby-vips</code>, but are very rarely called,
what we can do is <a href="https://github.com/Shopify/pitchfork/pull/55">mark the worker as no longer being allowed to be promoted</a>.</p>

<p>If you are abusing this feature, you may end up with all workers marked as fork-unsafe, and no longer able
to refork ever. But once I shipped Pitchfork in production, I did put some instrumentation in place to keep an eye on
how often workers would be marked unsafe and it was very rare, so we were fine.</p>

<p>Once I managed to get a green CI with reforking on, I still was a bit worried about the application being fork-safe.
Because simulating reforking on CI was good for catching issues with dead threads, but didn’t do much for catching
issues with inherited file descriptors.</p>

<p>In production, the problem with inheriting file descriptors mostly comes from multiple processes using the same
file descriptor concurrently. But on CI, even with that reforking simulation, we’re always running a single process.</p>

<p>So I had to think of another strategy to ensure no file descriptors were leaking.</p>

<p>This led me to develop another Pitchfork helper: <a href="https://github.com/Shopify/pitchfork/pull/56"><code class="language-plaintext highlighter-rouge">close_all_ios!</code></a>.
The idea is relatively simple, after a reforking happens, you can use <a href="https://docs.ruby-lang.org/en/3.4/ObjectSpace.html#method-c-each_object"><code class="language-plaintext highlighter-rouge">ObjectSpace.each_object</code></a>
to find all instances of <code class="language-plaintext highlighter-rouge">IO</code> and close them unless they’ve been explicitly marked as fork-safe with <code class="language-plaintext highlighter-rouge">Pitchfork::Info.keep_io</code>.</p>

<p>This isn’t fully reliable, as it can only catch Ruby-level IOs, and can’t catch file descriptors held in C extensions,
but it still helped find numerous issues in gems and private code.</p>

<p>Here’s <a href="https://github.com/discourse/mini_mime/pull/50">one example in the <code class="language-plaintext highlighter-rouge">mini_mime</code> gem</a>.</p>

<p>The gem is a small wrapper that allows querying flat files that contain information about mime types,
and to do that it would keep a read-only file, and <code class="language-plaintext highlighter-rouge">seek</code> into it:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">resolve</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
  <span class="vi">@file</span><span class="p">.</span><span class="nf">seek</span><span class="p">(</span><span class="n">row</span> <span class="o">*</span> <span class="vi">@row_length</span><span class="p">)</span>
  <span class="no">Info</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span><span class="vi">@file</span><span class="p">.</span><span class="nf">readline</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Since <code class="language-plaintext highlighter-rouge">seek</code> and <code class="language-plaintext highlighter-rouge">readline</code> aren’t thread-safe, the gem would wrap all that in a global mutex.</p>

<p>The problem here is that on fork file descriptors are inherited, and file descriptors aren’t just a pointer to a file
or socket. File descriptors also include a cursor that is incremented when you call <code class="language-plaintext highlighter-rouge">seek</code> or <code class="language-plaintext highlighter-rouge">read</code>.</p>

<p>To make this fork safe you could detect that a fork happened, and reopen the file, but there’s actually a much better solution.</p>

<p>Rather than to rely on <code class="language-plaintext highlighter-rouge">seek + read</code>, you can instead rely on <a href="https://man7.org/linux/man-pages/man2/pread.2.html"><code class="language-plaintext highlighter-rouge">pread(2)</code></a>,
which Ruby conveniently exposes in the <code class="language-plaintext highlighter-rouge">IO</code> class.
Instead of advancing the cursor like <code class="language-plaintext highlighter-rouge">read</code>, <code class="language-plaintext highlighter-rouge">pread</code> takes absolute offsets from the start of the file, which makes it
ideal to use in multi-threaded and multi-process scenarios:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">resolve</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
  <span class="no">Info</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span><span class="vi">@file</span><span class="p">.</span><span class="nf">pread</span><span class="p">(</span><span class="vi">@row_length</span><span class="p">,</span> <span class="n">row</span> <span class="o">*</span> <span class="vi">@row_length</span><span class="p">))</span>
<span class="k">end</span>
</code></pre></div></div>

<p>In addition to fixing the fork-safety in that gem, using <code class="language-plaintext highlighter-rouge">pread</code> also allowed to remove the global mutex, making the gem faster.
Win-win.</p>

<h2 id="the-first-production-reforking">The First Production Reforking</h2>

<p>After a few more rounds of grepping the codebase and its dependencies for patterns that may be problematic, I started being
confident enough to start manually triggering reforking in a single canary container.</p>

<p>To be clear, I was expecting some issues to be left, but I was out of ideas on how to catch any more of them
and confident the most critical problems such as data corruption were out of the picture.</p>

<p>These manual reforks didn’t reveal any issues, except that <a href="https://github.com/Shopify/pitchfork/issues/60">I forgot to also prevent manual reforking once a worker
had been maked as fork-unsafe</a>, 🤦.</p>

<p>Since other than that it went well, I progressively enabled automatic reforking on more and more servers over the span of
a few days, first 1%, then 10%, etc, with seemingly no problems.
While doing that I was also trying multiple different reforking frequencies, to try to identify a good tradeoff
between memory usage reduction and latency impact.</p>

<p>But one of the characteristics of the Shopify monolith, with so many engineers shipping changes every day, is that
it’s deployed extremely frequently, as often as every 30 minutes, and with teams across the world, this never really
stops except for a couple of hours at night, and a couple of days during weekends.</p>

<p>For the same reason that rebooting your computer will generally make whatever issue you had go away, redeploying a web
application will generally hide various bugs that take time to manifest themselves.
So over the years, doing this sort of infrastructure changes, I learned that even when you think you succeeded,
you might discover problems over the next weekend.</p>

<p>And in this case, it is what happened. On the night of Friday to Saturday, Site Reliability Engineers got paged because
some application servers became unresponsive, with very high CPU usage.</p>

<p>Luckily I had a ton of instrumentation in place to help me tune reforking, so I was able to investigate this immediately
on Saturday morning, and quickly identified some smoking guns.</p>

<p>The first thing I noticed is that on these nodes, the <code class="language-plaintext highlighter-rouge">after_fork</code> callbacks were taking close to a minute on average,
while they’d normally take less than a second. In that callback, we were mostly doing two things,
calling <code class="language-plaintext highlighter-rouge">Pitchfork::Info.close_all_ios!</code>, and eagerly reconnecting to datastores. So a good explanation for these spikes
would be an IO “leak”.</p>

<p>Hence I immediately jumped on a canary container to confirm my suspicion. The worker processes were fine, but
the mold processes were indeed “leaking” file descriptors, I still have the logs from that investigation:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>appuser@web-59bccbbd79-sgfph:~$ date; ls  /proc/135229/fd | wc -l
Sat Sep 23 07:52:46 UTC 2023
155
appuser@web-59bccbbd79-sgfph:~$ date; ls  /proc/135229/fd | wc -l
Sat Sep 23 07:52:47 UTC 2023
156
appuser@web-59bccbbd79-sgfph:~$ date; ls  /proc/135229/fd | wc -l
Sat Sep 23 07:52:47 UTC 2023
157
appuser@web-59bccbbd79-sgfph:~$ date; ls  /proc/135229/fd | wc -l
Sat Sep 23 07:52:48 UTC 2023
157
appuser@web-59bccbbd79-sgfph:~$ date; ls  /proc/135229/fd | wc -l
Sat Sep 23 07:52:49 UTC 2023
158
appuser@web-59bccbbd79-sgfph:~$ date; ls  /proc/135229/fd | wc -l
Sat Sep 23 07:52:49 UTC 2023
158
appuser@web-59bccbbd79-sgfph:~$ date; ls  /proc/135229/fd | wc -l
Sat Sep 23 07:52:50 UTC 2023
159
appuser@web-59bccbbd79-sgfph:~$ date; ls  /proc/135229/fd | wc -l
Sat Sep 23 07:52:51 UTC 2023
160
appuser@web-59bccbbd79-sgfph:~$ date; ls  /proc/135229/fd | wc -l
Sat Sep 23 07:52:51 UTC 2023
160
</code></pre></div></div>

<p>I could see that the mold process was creating file descritors at the rate of roughly one per second.</p>

<p>So I snapshotted the result of <code class="language-plaintext highlighter-rouge">ls -lh /proc/&lt;pid&gt;/fd</code> twice a few seconds apart, and used <code class="language-plaintext highlighter-rouge">diff</code> to see
which ones were new:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ diff tmp/fds-1.txt tmp/fds-2.txt 
130a131,135
&gt; lrwx------ 1 64 Sep 23 07:54 215 -&gt; 'socket:[10443548]'
&gt; lrwx------ 1 64 Sep 23 07:54 216 -&gt; 'socket:[10443561]'
&gt; lrwx------ 1 64 Sep 23 07:54 217 -&gt; 'socket:[10443568]'
&gt; lrwx------ 1 64 Sep 23 07:54 218 -&gt; 'socket:[10443577]'
&gt; lrwx------ 1 64 Sep 23 07:54 219 -&gt; 'socket:[10443605]'
&gt; lrwx------ 1 64 Sep 23 07:54 220 -&gt; 'socket:[10465514]'
&gt; lrwx------ 1 64 Sep 23 07:54 221 -&gt; 'socket:[10443625]'
&gt; lrwx------ 1 64 Sep 23 07:54 222 -&gt; 'socket:[10443637]'
&gt; lrwx------ 1 64 Sep 23 07:54 223 -&gt; 'socket:[10477738]'
&gt; lrwx------ 1 64 Sep 23 07:54 224 -&gt; 'socket:[10477759]'
&gt; lrwx------ 1 64 Sep 23 07:54 225 -&gt; 'socket:[10477764]'
&gt; lrwx------ 1 64 Sep 23 07:54 226 -&gt; 'socket:[10445634]'
...
</code></pre></div></div>

<p>These file descriptors were sockets. I went on and took a heap dump using <code class="language-plaintext highlighter-rouge">rbtrace</code>,
to see what the leak looked like from Ruby’s point of view:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>...
5130070:{"address":"0x7f5d11bfff48", "type":"FILE", "class":"0x7f5d8bc9eec0", "fd":11, "memsize":248}
7857847:{"address":"0x7f5cd9950668", "type":"FILE", "class":"0x7f5d8bc9eec0", "fd":-1, "memsize":8440}
7857868:{"address":"0x7f5cd99511d0", "type":"FILE", "class":"0x7f5d81597280", "fd":4855, "memsize":248}
7857933:{"address":"0x7f5cd9951fb8", "type":"FILE", "class":"0x7f5d8bc9eec0", "fd":-1, "memsize":8440}
7857953:{"address":"0x7f5cd99523c8", "type":"FILE", "class":"0x7f5d81597280", "fd":4854, "memsize":248}
7858016:{"address":"0x7f5cd9952fd0", "type":"FILE", "class":"0x7f5d8bc9eec0", "fd":-1, "memsize":8440}
7858036:{"address":"0x7f5cd9953390", "type":"FILE", "class":"0x7f5d81597280", "fd":4853, "memsize":248}
...
</code></pre></div></div>

<p>Here <code class="language-plaintext highlighter-rouge">"type":"FILE"</code> corresponds to Ruby’s <code class="language-plaintext highlighter-rouge">T_FILE</code> base type, which encompasses all <code class="language-plaintext highlighter-rouge">IO</code> objects.
I then used <a href="https://github.com/csfrancis/harb"><code class="language-plaintext highlighter-rouge">harb</code></a><sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, to get some more context on these IO objects
and quickly got my answer:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>harb&gt; print 0x7f5cd9950668
    0x7f5cd9950668: "FILE"
           memsize: 8,440
  retained memsize: 8,440
     references to: [
                      0x7f5cc9c59158 (FILE: (null))
                      0x7f5cd71d8540 (STRING: "/tmp/raindrop_monitor_84")
                      0x7f5cc9c590e0 (DATA: mutex)
                    ]
   referenced from: [
                      0x7f5cc9c59158 (FILE: (null))
                    ]
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">/tmp/raindrop_monitor</code> path hinted at one of our utility threads, which used to run in the Unicorn master process
and that I had moved into the Pitchfork mold process.</p>

<p>It uses <code class="language-plaintext highlighter-rouge">raindrops</code> gem to connect to the server port and extract TCP statistics to estimate how many requests
are queued, hence producing a utilization metric of the application server.</p>

<p>Basically, it executes the following code in a loop, and makes the result accessible to all workers:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="no">Raindrops</span><span class="o">::</span><span class="no">Linux</span><span class="p">.</span><span class="nf">tcp_listener_stats</span><span class="p">(</span><span class="s2">"localhost:$PORT"</span><span class="p">)</span>
</code></pre></div></div>

<p>The problem here is that <code class="language-plaintext highlighter-rouge">tcp_listener_stats</code> opens a socket to get the TCP stats, but doesn’t close the socket, nor even return it to you. It leaves to the Ruby GC the responsibility of closing the file descriptor.</p>

<p>Normally, this isn’t a big deal, because GC should trigger somewhat frequently, but the Pitchfork mold process, or
even the Unicorn master process, doesn’t do all that much work, hence allocates rarely, as a result, GC may only very
rarely trigger, if at all, letting these objects, hence file descriptors, accumulate over time.</p>

<p>Then once a new worker had to be spawned, it would inherit all these file descriptors, and have to close them all,
causing a lot of work for the kernel. That perfectly explained the observed issue and also explained why it would get
worse over time. The reforking frequency wasn’t fixed, it was configured to be relatively frequent at first,
and then less and less so. Leaving increasingly more time for file descriptors to accumulate.</p>

<p>To fix that problem, <a href="https://yhbt.net/raindrops-public/6E0E349D-A7CE-4B88-8F89-66438BB775A1@gmail.com/T/#u">I submitted a patch to Raindrops</a>,
to make it eagerly close these sockets, and applied the patch immediately on our systems, and the problem was gone.</p>

<p>What I find interesting here, is that in a way this bug was predating the Pitchfork migration.
Sockets were already accumulating in Unicorn’s master process, it just had not enough of an impact there for us to notice.</p>

<p>This wasn’t the only issue found in production, but it was the most impactful and is a good illustration of how
reforking can go wrong.</p>

<h2 id="tuning-reforking-frequency">Tuning Reforking Frequency</h2>

<p>Concurrently to ironing out reforking bugs, I spent a lot of time deploying various reforking settings, as it’s
a bit of a balancing act.</p>

<p>Reforking and Copy-on-Write aren’t free. It sounds a bit magical when described, but this is a lot of work for the
kernel.</p>

<p>Forking a process with which you share memory isn’t terribly costly, but after that, whenever a shared page has to be
invalidated because either the child or the parent has mutated it, the kernel has to pause the process and copy the
page over. So after you trigger a refork, you can expect some negative impact on the process latency, at least for
a little while.</p>

<p>That’s why it can be hard to find the sweet spot. If you refork too often you’ll degrade the service latency,
if you refork too infrequently, you’re not going to save as much memory.</p>

<p>For this sort of configuration, with lots of variables, I just tend to deploy multiple configurations concurrently,
and graph the results to try to locate the sweet spot, which is exactly what I did here.</p>

<p>Ultimately I settled on a setting with fairly linear growth:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">PITCHFORK_REFORK_AFTER</span><span class="o">=</span><span class="s2">"500,750,1000,1200,1400,1800,2000,2200,2400,2600,2800,...
</span></code></pre></div></div>

<p>The idea is that young containers are likely triggering various lazy initializations at a relatively fast rate,
but that over time, as more an more of these have been warmed, invalidations become less frequent.</p>

<p>Back <a href="https://railsatscale.com/2023-10-23-pitchfork-impact-on-shopify-monolith/">in 2023 I wrote a post that shared quite a few details on the results of reforking on Shopify’s monolith</a>,
you can read it if you want more details, but in short, memory usage was reduced by <code class="language-plaintext highlighter-rouge">30%</code>, and latency by <code class="language-plaintext highlighter-rouge">9%</code>.</p>

<p>The memory usage reduction was largely expected, but the latency reduction was a bit of a nice surprise at first,
if anything I was hoping latency wouldn’t be degraded too much.</p>

<p>I had to investigate to understand how it was even possible.</p>

<h2 id="the-unicorn-bias">The Unicorn Bias</h2>

<p>One thing to know about how Unicorn and Pitchfork works is that, on Linux, they wait for incoming requests using the <code class="language-plaintext highlighter-rouge">epoll</code> system call.
Once a request comes in, the worker is woken up by the kernel and immediately calls <code class="language-plaintext highlighter-rouge">accept</code> to, well, accept the request.
This is a very classic pattern, that many servers use, but historically it suffered from a problem called the
<a href="https://en.wikipedia.org/wiki/Thundering_herd_problem">“thundering herd problem”</a>.</p>

<p>Assuming a fully idle server with 32 workers, all waiting on <code class="language-plaintext highlighter-rouge">epoll</code>, whenever a request would come in,
all 32 workers would be woken up, and all try to call <code class="language-plaintext highlighter-rouge">accept</code>, but only one of them would succeed.
This was a pretty big waste of resources, so in 2016, with the release of Linux 4.5, <code class="language-plaintext highlighter-rouge">epoll</code> gained a new flag: <code class="language-plaintext highlighter-rouge">EPOLLEXCLUSIVE</code>.</p>

<p>If this flag is set, the Linux kernel will only wake up a single worker when a request comes in.
However the feature doesn’t try to be fair or anything, it just wakes up the first it finds, and because of how the
feature is implemented, it behaves a bit like a Last In First Out queue, in other words, a stack.</p>

<p>As a result, unless most workers are busy most of the time, what you’ll observe is that some workers will serve
disproportionately more requests than others. In some cases, I witnessed that <code class="language-plaintext highlighter-rouge">worker 0</code> had processed over a thousand
requests while <code class="language-plaintext highlighter-rouge">worker 47</code> had only seen a dozen requests.</p>

<p>Unicorn isn’t the only server impacted by that, <a href="https://blog.cloudflare.com/the-sad-state-of-linux-socket-balancing/">Cloudflare engineers wrote a much more detailed post on how NGINX behaves
the way</a>.</p>

<p>In Ruby’s case, this imbalance means that all these inline caches in the VM, all the lazy initialized code in the application,
as well as YJIT, are much more warmed up in some workers than in others.</p>

<h2 id="how-reforking-can-reduce-latency">How Reforking Can Reduce Latency</h2>

<p>Because of all these caches, JIT, etc, a “cold” worker is measurably slower than a warmed-up one,
and because of the balancing bias, workers are very unevenly warmed up.</p>

<p>However since the criteria for promoting a worker into the new mold is the number of requests it has handled,
it’s almost always the most warmed-up worker that ends up being used as a template for the next generation of workers.</p>

<p>As a result, with reforking enabled, workers are much more warmed up on average, hence running faster.
In my initial post about Pitchfork, I illustrated this by showing how much more JITed code workers had in containers
where reforking was enabled compared to the ones without:</p>

<p><img src="/assets/articles/pitchfork/yjit-code-region-size.png" alt="" /></p>

<p>And more JITed code translates into faster execution and less time spent compiling hot methods.</p>

<h2 id="the-actual-killer-feature">The Actual Killer Feature</h2>

<p>As explained previously, the motivator for working on Pitchfork was reducing memory usage.
Especially with the advent of YJIT, we were hitting some limits, and I wanted to solve that once and for all.
But in reality, it would have been much less effort to just ask for more RAM on servers.
RAM is quite cheap these days, and most hosting services will give you about 4GiB of RAM per core, which even for
Ruby is plenty.</p>

<p>It’s only when working with very large monoliths that this becomes a bit tight. But even then, we could have
relatively easily used servers with more RAM per core, and while it would have incurred extra cost, it probably wouldn’t
have been too bad in the grand scheme of things.</p>

<p>It’s only after reforking fully shipped to production, that I started to understand its real benefits.
Beyond the memory savings, the way the warmest worker is essentially “checkpointed” and used as a template means
that whenever a small spike of traffic comes in, and workers that are normally mostly idle respond to that traffic,
they do it noticeably faster than they used to.</p>

<p>In addition, when we were running Unicorn, we were keeping a close eye on worker terminations caused by request
timeouts or OOM, because killing a Unicorn worker meant replacing a warm worker with a cold worker, hence it had a
noticeable performance impact.</p>

<p>But since reforking was enabled, not only does this happen less often because OOM events are less common,
but also the killed worker is now replaced with a fairly well-warmed-up one, with already a lot of JITed code and such.</p>

<p>And I now believe this is the true killer feature of Pitchfork, before the memory usage reduction.</p>

<p>This realization of how powerful checkpointing is, later led me to further optimize the monolith.</p>

<h2 id="pushing-it-further">Pushing It Further</h2>

<p>YJIT has this nice characteristic that it warms up quite fast and for relatively cheap.
By that, I mean that it reaches its peak performance quickly, and doesn’t slow down normal Ruby execution too much
while doing so.</p>

<p>However last summer, when I started testing Ruby 3.4.0-preview1 in production, I discovered a pretty major regression
in YJIT compile time. The compiled code was still as fast if not faster, but YJIT was suddenly requiring 4 times as much
CPU to do its compilation, which was causing large spikes of CPU utilization on our servers, negatively impacting the
overall latency.</p>

<p>What happened is that the YJIT team had recently rewritten the register allocator to be smarter, but it also ended up
being noticeably slower. This is a common tradeoff in JIT design, if you complexify the compiler, it may generate faster
code, but degrade performance more while it is compiling.</p>

<p>I of course reported the issue to the YJIT team, but it was clear that this performance would not be reclaimed quickly, so
it was complicated to keep the Ruby preview in production with such regression in it.</p>

<p>Until it hit me: why are we even bothering to compile this much?</p>

<p>If you think about it, we were deploying Pitchfork with 36 workers, and all 36 of them have YJIT enabled, so all of
them compile new code when they discover new hot methods. So most methods, especially the hottest ones, are compiled 36 times.</p>

<p>But once one worker has served the 500 requests required to be promoted, all the code compiled by other workers is just
thrown out of the window, it’s a huge waste.</p>

<p>Which gave me the idea, what if we only enabled YJIT in the <code class="language-plaintext highlighter-rouge">worker 0</code>? Thanks to the balancing bias induced by
<code class="language-plaintext highlighter-rouge">EPOLLEXCLUSIVE</code>, we already know it will most likely be the one to be promoted, and for the others, we can just
mark them as not fork-safe.</p>

<p>This is quite trivially done from the Pitchfork config:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">after_worker_fork</span> <span class="k">do</span> <span class="o">|</span><span class="n">server</span><span class="p">,</span> <span class="n">worker</span><span class="o">|</span>
  <span class="k">if</span> <span class="n">worker</span><span class="p">.</span><span class="nf">nr</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="no">RubyVM</span><span class="o">::</span><span class="no">YJIT</span><span class="p">.</span><span class="nf">enable</span>
  <span class="k">else</span>
    <span class="o">::</span><span class="no">Pitchfork</span><span class="o">::</span><span class="no">Info</span><span class="p">.</span><span class="nf">no_longer_fork_safe!</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Of course, once the first generation is promoted, YJIT is then enabled in all workers, but this helped tremendously
to reduce the YJIT overhead soon after a deploy.</p>

<p>Here’s a graph that shows the distribution of system time around deploys. YJIT tends to make the system time spike
when warming up, because it calls <code class="language-plaintext highlighter-rouge">mprotect</code> frequently to mark pages as either executable or writable.
This causes quite a lot of load on the kernel.</p>

<p>The first spike is a deploy before I enabled this configuration, on the second spike the yellow line has the
configuration enabled, while the green one still doesn’t have it.</p>

<p><img src="/assets/articles/pitchfork/yjit_delayed_enable.png" alt="" /></p>

<p>While there is currently no way to turn YJIT back off once it has been enabled, we did experiment with such a feature
for other reasons a few years ago. So there may be a case for bringing that feature back, as it would allow to
keep YJIT compilation disabled in all workers but one, further reducing the overhead caused by YJIT’s warmup.</p>

<p>There are also a few other advanced optimizations that aren’t exclusive to Pitchfork but are facilitated by it, such as
<a href="https://railsatscale.com/2024-10-23-next-generation-oob-gc/">Out of Band Garbage Collection</a>, but I can’t mention everything.</p>

<h2 id="beyond-shopify">Beyond Shopify</h2>

<p>I never really intended Pitchfork to be more than a very opinionated fork of Unicorn, for very specific needs.
I even wrote <a href="https://github.com/Shopify/pitchfork/blob/b70ee3c8700a997ee9513c81709b91062cc79ca1/docs/WHY_MIGRATE.md">a long document essentially explaining why you probably don’t want to migrate to Pitchfork</a>.</p>

<p>But based on issues open on the repo, some conference chatter, and a few DMs I got, it seems that a handful of companies
either migrated to it or are currently working on doing so.</p>

<p>Unsurprisingly, these are mostly companies that used to run Unicorn and have relatively large monoliths.</p>

<p>However, <a href="https://blog.studysapuri.jp/entry/2024-pitchfork-into-the-largest-rails-application-in-studysapuri">the only public article about such migration I know of is in Japanese</a>.</p>

<p>But it’s probably for the better, because while reforking is very powerful, as I tried to demonstrate in this post,
fork-safety issues can lead to pretty catastrophic bugs that can be very hard to debug, hence it’s probably better left
to teams with the resources and expertise needed to handle that sort of thing.</p>

<p>So I prefer to avoid any sort of Pitchfork hype.</p>

<p>That being said, I’ve also noticed some people simply interested in a modernized Unicorn, not intending to ever enable
reforking, which I guess is a good enough reason to migrate.</p>

<h2 id="the-future-of-pitchfork">The Future Of Pitchfork</h2>

<p>At this point, after seeing all the performance improvements I mentioned, you may be thinking that Shopify must be pretty
happy with its brand-new application server.</p>

<p>Well.</p>

<p>While Pitchfork was well received by my immediate team, my manager, my director, and many of my peers, the feedback I got
from upper management wasn’t exactly as positive:</p>

<blockquote>
  <p>reforking is a hack that I think is borderline abdication of engineering responsibilities, so this won’t do</p>
</blockquote>

<p>Brushing aside the offensiveness of the phrasing, it may surprise you to hear that I do happen to, at least partially,
agree with this statement.</p>

<p>This is why before writing this post, I wrote a whole series on <a href="/ruby/performance/2025/01/23/the-mythical-io-bound-rails-app.html">how IO-bound Rails applications really are</a>,
<a href="/ruby/performance/2025/02/27/whats-the-deal-with-ractors.html">the current state of parallelism in Ruby</a> and a few other adjacent subjects.
To better explain the tradeoffs currently at play when designing a Ruby web server.</p>

<p>I truly believe that <strong>today</strong>, Pitchfork’s design is what best answers the needs of a large Rails monolith,
I wouldn’t have developed it otherwise.
It offers true parallelism and faster JIT warmup, absurdly little time spent in GC, while keeping memory usage low and
does so with a decent level of resiliency.</p>

<p>That being said, I also truly hope that <strong>tomorrow</strong>, Pitchfork’s design will be obsolete.</p>

<p>I do hope that in the future Ruby will be capable of true parallelism in a single process, be it via improved Ractors,
or by progressively <a href="/ruby/performance/2025/01/29/so-you-want-to-remove-the-gvl.html">removing the GVL</a>, I’m not picky.</p>

<p>But this is a hypothetical future. The very second it happens, I’ll happily work on Pitchfork’s successor, and slap a deprecation
notice on Pitchfork.</p>

<p>That being said, I know I’m rarely the most optimistic person in the room, it’s in my nature, but I honestly can’t
see this future happening in the short term. Maybe in 2 or 3 years, certainly not before.</p>

<p>Because it’s not just about Ruby itself, it’s also about the ecosystem. Even if Ractors were perfectly usable tomorrow
morning, tons of gems would need to be adapted to work in a Ractor world. This would be the mother of all yak-shaves.</p>

<p>Trust me, I’ve done my fair share of yak-shaves in the past. When Ruby 2.7 started throwing keyword deprecation warnings
I took it upon myself to fix all these issues in Shopify’s monolith and all its dependencies, which led me to open over a hundred pull requests on open-source gems, trying to reach maintainers, etc.
And again recently with frozen string literal, I submitted tons of PRs to fix lots of gems ahead of Ruby 3.4’s release.</p>

<p>All this to say, I’m not scared of yak-shaves, but making an application like Shopify’s monolith, including its dependencies,
Ractor compatible requires an amount of work that is largely beyond what you may imagine.
And more than work, an ecosystem like Ruby’s need time to adapt to new features like Ractors,
It’s not just a matter of throwing more engineers at the problem.</p>

<p>In the meantime, reforking may or may not be a hack, I don’t really care.
What is important to me is that it solves some real problems, and it does so today.</p>

<p>Of course, it’s not perfect, there are several common complaints it doesn’t solve, such as still requiring more
database connections than what would be possible with in-process parallelism.
But I don’t believe it’s a problem that can be reasonably solved today with a different server design that doesn’t mostly
rely on <code class="language-plaintext highlighter-rouge">fork</code>, and trying to do so now would be putting the cart before the horse.</p>

<p>An engineer’s responsibility is to solve problems while considering the limitations imposed by practicality.</p>

<p>As such, I believe Pitchfork will continue to do fine for at least a few more years.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>Years later, <a href="https://github.com/ruby/ruby/pull/10899">John Hawthorn figured how to to it with <code class="language-plaintext highlighter-rouge">perf</code> to great effect</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Since I explained what inline caches are multiple times in the past, I’ll just refer you to <a href="/ruby/json/2024/12/18/optimizing-ruby-json-part-2.html#inline-caches">Optimizing JSON, Part 2</a>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Today I’d probably recommend <a href="https://github.com/jhawthorn/sheap"><code class="language-plaintext highlighter-rouge">sheap</code></a> for the same use case. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="ruby" /><category term="performance" /><summary type="html"><![CDATA[A bit more than two years ago, as part of my work in Shopify’s Ruby and Rails Infrastructure team, I released a new Ruby HTTP server called Pitchfork.]]></summary></entry><entry><title type="html">What’s The Deal With Ractors?</title><link href="https://byroot.github.io/ruby/performance/2025/02/27/whats-the-deal-with-ractors.html" rel="alternate" type="text/html" title="What’s The Deal With Ractors?" /><published>2025-02-27T08:03:51+00:00</published><updated>2025-02-27T08:03:51+00:00</updated><id>https://byroot.github.io/ruby/performance/2025/02/27/whats-the-deal-with-ractors</id><content type="html" xml:base="https://byroot.github.io/ruby/performance/2025/02/27/whats-the-deal-with-ractors.html"><![CDATA[<p>I want to write a post about <a href="https://rubygems.org/gems/pitchfork">Pitchfork</a>, explaining where it comes from, why it
is like it is, and how I see its future.
But before I can get to that, I think I need to share my mental model on a few things, in this case, Ractors.</p>

<p>When Ractors were announced 4 or 5 years ago, many people expected we’d quickly see a Ractor-based web server,
some sort of Puma but with Ractors instead of threads.
Yet this still hasn’t happened, except for a few toy projects and experiments.</p>

<p>Since this post series is about giving context to Ruby HTTP servers design constraints, I think it makes sense to share
my view on Ractors viability.</p>

<h2 id="what-are-they-supposed-to-be">What Are They Supposed to Be?</h2>

<p>The core idea of Ractors is relatively simple, the goal is to provide a primitive that allows true in-process parallelism,
while still not fully <a href="/ruby/performance/2025/01/29/so-you-want-to-remove-the-gvl.html">removing the GVL</a>.</p>

<p>As I mentioned in depth in a previous post, operating without a GVL would require synchronization (mutexes) on every
mutable object that is shared between threads.
Ractors’ solution to that problem is not to allow sharing of mutable objects between Ractors.
Instead, they can send each other copies of objects, or in some cases “move” an object to another Ractor, which means they
can no longer access it themselves.</p>

<p>This isn’t unique to Ruby, it’s largely inspired by the <a href="https://en.wikipedia.org/wiki/Actor_model">Actor model</a>, like
the Ractor name suggests, and many languages in the same category as Ruby have a similar construct or are working on one.
For instance, JavaScript has <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API">Web Workers</a>,
and Python has been working on <a href="https://peps.python.org/pep-0734/">subinterpreters</a> for a while.</p>

<p>And it’s no surprise because it makes total sense from a language evolution perspective.
If you have a language that has prevented in-process parallelism for a long time, a Ractor-like API allows you to introduce (constrained) parallelism in a way that isn’t going to break existing code, without having to add mutexes everywhere.</p>

<p>But even in languages that have free threading, shared mutable state parallelism is seen as a major foot gun by many,
and message-passing parallelism is often deemed safer, for instance, channels in Go, etc.</p>

<p>Applied to Ruby, this means that instead of having a single Global VM Lock that synchronizes all threads,
you’d instead have many Ractor Locks, that each synchronize all threads that belong to a given Ractor.
So in a way, since the Ruby 3.0 release that introduced Ractors, on paper the GVL is somewhat already gone,
even though as we’ll see later, it’s more subtle than that.</p>

<p>And this can easily be confirmed experimentally with a simple test script:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">require</span> <span class="s2">"benchmark"</span>
<span class="no">Warning</span><span class="p">[</span><span class="ss">:experimental</span><span class="p">]</span> <span class="o">=</span> <span class="kp">false</span>

<span class="k">def</span> <span class="nf">fibonacci</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">n</span>
  <span class="k">else</span>
    <span class="n">fibonacci</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">fibonacci</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">synchronous_fib</span><span class="p">(</span><span class="n">concurrency</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
  <span class="n">concurrency</span><span class="p">.</span><span class="nf">times</span><span class="p">.</span><span class="nf">map</span> <span class="k">do</span>
    <span class="n">fibonacci</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">threaded_fib</span><span class="p">(</span><span class="n">concurrency</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
  <span class="n">concurrency</span><span class="p">.</span><span class="nf">times</span><span class="p">.</span><span class="nf">map</span> <span class="k">do</span>
    <span class="no">Thread</span><span class="p">.</span><span class="nf">new</span> <span class="p">{</span> <span class="n">fibonacci</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">}</span>
  <span class="k">end</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="o">&amp;</span><span class="ss">:value</span><span class="p">)</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">ractor_fib</span><span class="p">(</span><span class="n">concurrency</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
  <span class="n">concurrency</span><span class="p">.</span><span class="nf">times</span><span class="p">.</span><span class="nf">map</span> <span class="k">do</span>
    <span class="no">Ractor</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">{</span> <span class="o">|</span><span class="n">num</span><span class="o">|</span> <span class="n">fibonacci</span><span class="p">(</span><span class="n">num</span><span class="p">)</span> <span class="p">}</span>
  <span class="k">end</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="o">&amp;</span><span class="ss">:take</span><span class="p">)</span>
<span class="k">end</span>

<span class="nb">p</span> <span class="p">[</span><span class="ss">:sync</span><span class="p">,</span> <span class="no">Benchmark</span><span class="p">.</span><span class="nf">realtime</span> <span class="p">{</span> <span class="n">synchronous_fib</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">38</span><span class="p">)</span> <span class="p">}.</span><span class="nf">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
<span class="nb">p</span> <span class="p">[</span><span class="ss">:thread</span><span class="p">,</span> <span class="no">Benchmark</span><span class="p">.</span><span class="nf">realtime</span> <span class="p">{</span> <span class="n">threaded_fib</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">38</span><span class="p">)</span> <span class="p">}.</span><span class="nf">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
<span class="nb">p</span> <span class="p">[</span><span class="ss">:ractor</span><span class="p">,</span> <span class="no">Benchmark</span><span class="p">.</span><span class="nf">realtime</span> <span class="p">{</span> <span class="n">ractor_fib</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">38</span><span class="p">)</span> <span class="p">}.</span><span class="nf">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
</code></pre></div></div>

<p>Here we use the Fibonacci function as a classic CPU-bound workload and benchmark it in 3 different ways.
First without any concurrency, just serially, then concurrently using 5 threads, and finally concurrently using 5 Ractors.</p>

<p>If I run this script on my machine, I get these results:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="ss">:sync</span><span class="p">,</span> <span class="mf">2.26</span><span class="p">]</span>
<span class="p">[</span><span class="ss">:thread</span><span class="p">,</span> <span class="mf">2.29</span><span class="p">]</span>
<span class="p">[</span><span class="ss">:ractor</span><span class="p">,</span> <span class="mf">0.68</span><span class="p">]</span>
</code></pre></div></div>

<p>As we already knew, using threads for CPU-bound workloads doesn’t make anything faster because of the GVL, however using Ractors we can benefit from some parallelism.
So this script proves that, at least to some extent, the Ruby VM can execute code in parallel, hence the GVL is not so
global anymore.</p>

<p>But as always, the devil is in the details.
Running a pure function like <code class="language-plaintext highlighter-rouge">fibonacci</code>, that only deals with immutable integers, in parallel is one thing, running
a full-on web application, with hundreds of gems and a lot of global states, in parallel is another.</p>

<h2 id="shareable-objects">Shareable Objects</h2>

<p>Where Ruby ractors are significantly different from most similar features in other languages, is that Ractors share the global
namespace with other Ractors.</p>

<p>To create a <code class="language-plaintext highlighter-rouge">WebWorker</code> in JavaScript, you have to provide an entry script:</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">myWorker</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Worker</span><span class="p">(</span><span class="dl">"</span><span class="s2">worker.js</span><span class="dl">"</span><span class="p">)</span>
</code></pre></div></div>

<p>WebWorkers are created from a blank slate and have their own namespace, they don’t automatically inherit all the constants
defined by the caller.</p>

<p>Similarly, Python’s sub-interpreters as defined in PEP 734, start with a clean slate.</p>

<p>So both JavaScript’s WebWorker and Python’s sub-interpreters have very limited sharing capabilities and are more akin to light subprocesses, but with an API that allows passing each other’s objects without needing to serialize them.</p>

<p>Ruby’s Ractors are more ambitious than that.
From a secondary Ractor, you have visibility on all the constants and methods defined by the main Ractor:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="no">INT</span> <span class="o">=</span> <span class="mi">1</span>

<span class="no">Ractor</span><span class="p">.</span><span class="nf">new</span> <span class="k">do</span>
  <span class="nb">p</span> <span class="no">INT</span> <span class="c1"># prints 1</span>
<span class="k">end</span><span class="p">.</span><span class="nf">take</span>
</code></pre></div></div>

<p>But since Ruby cannot allow concurrent access to mutable objects, it has to limit this in some way:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="no">HASH</span> <span class="o">=</span> <span class="p">{}</span>

<span class="no">Ractor</span><span class="p">.</span><span class="nf">new</span> <span class="k">do</span>
  <span class="nb">p</span> <span class="no">HASH</span> <span class="c1"># Ractor::IsolationError</span>
  <span class="c1"># can not access non-shareable objects in constant Object::HASH by non-main Ractor.</span>
<span class="k">end</span><span class="p">.</span><span class="nf">take</span>
</code></pre></div></div>

<p>So all objects are divided into shareable and unshareable objects, and only shareable ones can be accessed by secondary ractors.
In general, objects that are frozen, or inherently immutable are shareable as long as they don’t reference a non-shareable object.</p>

<p>In addition, some other operations, such as assigning class instance variables aren’t allowed from any ractor other than
the main one:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="no">Ractor</span><span class="p">.</span><span class="nf">new</span> <span class="k">do</span> 
  <span class="k">class</span> <span class="nc">Foo</span>
    <span class="k">class</span> <span class="o">&lt;&lt;</span> <span class="nb">self</span>
      <span class="nb">attr_accessor</span> <span class="ss">:bar</span>
    <span class="k">end</span>
  <span class="k">end</span>
  <span class="no">Foo</span><span class="p">.</span><span class="nf">bar</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Ractor::IsolationError</span>
  <span class="c1"># can not set instance variables of classes/modules by non-main Ractors</span>
<span class="k">end</span><span class="p">.</span><span class="nf">take</span>
</code></pre></div></div>

<p>So Ractors’ design is a bit of a double-edged sword.
On one hand, by having access to all the loaded constants and methods, you don’t have to load the same code multiple
times, and it’s easier to pass complex objects from one ractor to the other, but it also means that not all code may be
able to run from a secondary ractor.
Actually, a lot, if not most, existing Ruby code can’t run from a secondary Ractor.
Something as mundane as accessing a constant that is technically mutable, like a String or Hash, will raise an <code class="language-plaintext highlighter-rouge">IsolationError</code>,
even if you never attempted to mutate it.</p>

<p>Something as mundane and idiomatic as having a constant with some defaults is enough to make your code not Ractor compatible,
e.g.:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Something</span>
  <span class="no">DEFAULTS</span> <span class="o">=</span> <span class="p">{</span> <span class="ss">config: </span><span class="mi">1</span> <span class="p">}</span> <span class="c1"># You'd need to explictly freeze that Hash.</span>

  <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="n">options</span> <span class="o">=</span> <span class="p">{})</span>
    <span class="vi">@options</span> <span class="o">=</span> <span class="no">DEFAULTS</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">options</span><span class="p">)</span> <span class="c1"># =&gt; Ractor::IsolationError</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p>That’s one of the main reasons why a Ractor-based web server isn’t really practical for anything more than a trivial application.</p>

<p>If you take Rails as an example, there is quite a lot of legitimate global states, such as the routes, the database schema
cache, or the logger. Some of it could probably be frozen to be accessible by secondary ractors, but for things
like the logger, the Active Record connection pool, and various caches, it’s tricky.</p>

<p>To be honest, I’m not even sure how you could implement a Ractor safe connection pool with the current API, but I may
be missing something. Actually, that’s probably a good illustration of the problem, let’s try to implement a Ractor-compatible connection pool.</p>

<h2 id="a-ractor-aware-connection-pool">A Ractor Aware Connection Pool</h2>

<p>The first challenge is that you’d need to be able to move connections from one ractor to another, something like:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">require</span> <span class="s2">"trilogy"</span>

<span class="n">db_client</span> <span class="o">=</span> <span class="no">Trilogy</span><span class="p">.</span><span class="nf">new</span>
<span class="n">ractor</span> <span class="o">=</span> <span class="no">Ractor</span><span class="p">.</span><span class="nf">new</span> <span class="p">{</span> <span class="n">receive</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="s2">"SELECT 1"</span><span class="p">)</span> <span class="p">}</span>
<span class="n">ractor</span><span class="p">.</span><span class="nf">send</span><span class="p">(</span><span class="n">db_client</span><span class="p">,</span> <span class="ss">move: </span><span class="kp">true</span><span class="p">)</span>
<span class="nb">p</span> <span class="n">ractor</span><span class="p">.</span><span class="nf">take</span>
</code></pre></div></div>

<p>If you try that you’ll get a <code class="language-plaintext highlighter-rouge">can not move Trilogy object. (Ractor::Error)</code>.
This is because as far as I’m aware, there is no way for classes implemented in C to define that they can be moved to
another ractor. Even the ones defined in Ruby’s core, like <code class="language-plaintext highlighter-rouge">Time</code> can’t:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="no">Ractor</span><span class="p">.</span><span class="nf">new</span><span class="p">{}.</span><span class="nf">send</span><span class="p">(</span><span class="no">Time</span><span class="p">.</span><span class="nf">now</span><span class="p">,</span> <span class="ss">move: </span><span class="kp">true</span><span class="p">)</span> <span class="c1"># can not move Time object. (Ractor::Error)</span>
</code></pre></div></div>

<p>The only thing C extensions can do is define that a type can be shared between Ractors once it is frozen, using the
<code class="language-plaintext highlighter-rouge">RUBY_TYPED_FROZEN_SHAREABLE</code> flag, but that wouldn’t make sense for a database connection.</p>

<p>A way around this is to encapsulate that object inside its own Ractor:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">require</span> <span class="s2">"trilogy"</span>

<span class="k">class</span> <span class="nc">RactorConnection</span>
  <span class="k">def</span> <span class="nf">initialize</span>
    <span class="vi">@ractor</span> <span class="o">=</span> <span class="no">Ractor</span><span class="p">.</span><span class="nf">new</span> <span class="k">do</span>
      <span class="n">client</span> <span class="o">=</span> <span class="no">Trilogy</span><span class="p">.</span><span class="nf">new</span>
      <span class="k">while</span> <span class="n">args</span> <span class="o">=</span> <span class="no">Ractor</span><span class="p">.</span><span class="nf">receive</span>
        <span class="n">ractor</span><span class="p">,</span> <span class="nb">method</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span> <span class="o">=</span> <span class="n">args</span>
        <span class="n">ractor</span><span class="p">.</span><span class="nf">send</span> <span class="n">client</span><span class="p">.</span><span class="nf">public_send</span><span class="p">(</span><span class="nb">method</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
      <span class="k">end</span>
    <span class="k">end</span>
  <span class="k">end</span>

  <span class="k">def</span> <span class="nf">query</span><span class="p">(</span><span class="n">sql</span><span class="p">)</span>
    <span class="vi">@ractor</span><span class="p">.</span><span class="nf">send</span><span class="p">([</span><span class="no">Ractor</span><span class="p">.</span><span class="nf">current</span><span class="p">,</span> <span class="ss">:query</span><span class="p">,</span> <span class="n">sql</span><span class="p">],</span> <span class="ss">move: </span><span class="kp">true</span><span class="p">)</span>
    <span class="no">Ractor</span><span class="p">.</span><span class="nf">receive</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p>When we need to perform an operation on the object, we send a message telling it what to do,
and give it our own ractor so it can send the result back.</p>

<p>It really is a huge hack, and perhaps there is a proper way to do this, but I don’t know of any.</p>

<p>Now that we have a “way” to pass database connections across ractors, we need to implement a pool.
Here again, it is tricky, because by definition a pool is a mutable data structure, hence it can’t
be referenced by multiple ractors.</p>

<p>So we somewhat need to use the same hack again:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RactorConnectionPool</span>
  <span class="k">def</span> <span class="nf">initialize</span>
    <span class="vi">@ractor</span> <span class="o">=</span> <span class="no">Ractor</span><span class="p">.</span><span class="nf">new</span> <span class="k">do</span>
      <span class="n">pool</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">while</span> <span class="n">args</span> <span class="o">=</span> <span class="no">Ractor</span><span class="p">.</span><span class="nf">receive</span>
        <span class="n">ractor</span><span class="p">,</span> <span class="nb">method</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span> <span class="o">=</span> <span class="n">args</span>
        <span class="k">case</span> <span class="nb">method</span>
        <span class="k">when</span> <span class="ss">:checkout</span>
          <span class="n">ractor</span><span class="p">.</span><span class="nf">send</span><span class="p">(</span><span class="n">pool</span><span class="p">.</span><span class="nf">pop</span> <span class="o">||</span> <span class="no">RactorConnection</span><span class="p">.</span><span class="nf">new</span><span class="p">)</span>
        <span class="k">when</span> <span class="ss">:checkin</span>
          <span class="n">pool</span> <span class="o">&lt;&lt;</span> <span class="n">args</span><span class="p">.</span><span class="nf">first</span>
        <span class="k">end</span>
      <span class="k">end</span>
    <span class="k">end</span>
    <span class="nb">freeze</span> <span class="c1"># so we're shareable</span>
  <span class="k">end</span>

  <span class="k">def</span> <span class="nf">checkout</span>
    <span class="vi">@ractor</span><span class="p">.</span><span class="nf">send</span><span class="p">([</span><span class="no">Ractor</span><span class="p">.</span><span class="nf">current</span><span class="p">,</span> <span class="ss">:checkout</span><span class="p">],</span> <span class="ss">move: </span><span class="kp">true</span><span class="p">)</span>
    <span class="no">Ractor</span><span class="p">.</span><span class="nf">receive</span>
  <span class="k">end</span>

  <span class="k">def</span> <span class="nf">checkin</span><span class="p">(</span><span class="n">connection</span><span class="p">)</span>
    <span class="vi">@ractor</span><span class="p">.</span><span class="nf">send</span><span class="p">([</span><span class="no">Ractor</span><span class="p">.</span><span class="nf">current</span><span class="p">,</span> <span class="ss">:checkin</span><span class="p">,</span> <span class="n">connection</span><span class="p">],</span> <span class="ss">move: </span><span class="kp">true</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="no">CONNECTION_POOL</span> <span class="o">=</span> <span class="no">RactorConnectionPool</span><span class="p">.</span><span class="nf">new</span>

<span class="n">ractor</span> <span class="o">=</span> <span class="no">Ractor</span><span class="p">.</span><span class="nf">new</span> <span class="k">do</span>
  <span class="n">db_client</span> <span class="o">=</span> <span class="no">CONNECTION_POOL</span><span class="p">.</span><span class="nf">checkout</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">db_client</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="s2">"SELECT 1"</span><span class="p">)</span>
  <span class="no">CONNECTION_POOL</span><span class="p">.</span><span class="nf">checkin</span><span class="p">(</span><span class="n">db_client</span><span class="p">)</span>
  <span class="n">result</span>
<span class="k">end</span>
<span class="nb">p</span> <span class="n">ractor</span><span class="p">.</span><span class="nf">take</span><span class="p">.</span><span class="nf">to_a</span> <span class="c1"># =&gt; [[1]]</span>
</code></pre></div></div>

<p>I’m not going to go further, as this implementation is quite ridiculous, I think this is enough to make my point.</p>

<p>For Ractors to be viable to run a full-on application in, Ruby would need to provide at least a few basic data structures
that would be shareable across ractors, so that we can implement useful constructs like connection pools.</p>

<p>Perhaps some <code class="language-plaintext highlighter-rouge">Ractor::Queue</code>, maybe even some <code class="language-plaintext highlighter-rouge">Ractor::ConcurrentMap</code>, and more importantly, C extensions
would need to be able to make their types movable.</p>

<h2 id="what-ractors-could-be-useful-for">What Ractors Could Be Useful For</h2>

<p>So while I don’t believe it makes sense to try to run a full application inside Ractors, I still think Ractors could be
very useful even with their current limitations.</p>

<p>For instance, in my previous post about the GVL, I mentioned how some gems do have background threads, one example being
<a href="https://github.com/Shopify/statsd-instrument/blob/6fd8c49d50803bbccfcc11b195f9e334a6e835e9/lib/statsd/instrument/batched_sink.rb#L163"><code class="language-plaintext highlighter-rouge">statsd-instrument</code></a>,
but there are others like open telemetry and such.</p>

<p>These gems all have a similar pattern, they collect information in memory, and periodically serialize and send it down
the wire. Currently, this is done using a thread, which is sometimes problematic because the serialization part holds
the GVL, hence can slow down the threads that are responding to incoming traffic.</p>

<p>This would be an excellent pattern for Ractors, as they’d be able to do the same thing without holding the main Ractor’s
GVL and it’s mostly fire and forget.</p>

<p>I only mean this as an example I know well, I’m sure there’s more.
The key point is that while Ractors in their current form can hardly be used as the main execution primitive, they can certainly be used for parallelizing lower-level functions inside libraries.</p>

<p>But unfortunately, in practice, it’s not really a good idea to do that today.</p>

<h2 id="also-there-are-many-implementation-issues">Also There Are Many Implementation Issues</h2>

<p>If you attempt to use Ractors, Ruby will display a warning:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>warning: Ractor is experimental, and the behavior may change in future versions of Ruby!
Also there are many implementation issues.
</code></pre></div></div>

<p>And that’s not an overstatement.
As I’m writing this article, there are 74 open issues about Ractors.
A handful are feature requests or minor things, but a significant part are really critical bugs such as
segmentation faults, or deadlocks.
As such, one cannot reasonably use Ractors for anything more than small experiments.</p>

<p>Another major reason not to use them even in these cases that are perfect for them, is that quite often, they’re not
really running in parallel as they’re supposed to.</p>

<h2 id="one-more-global-lock">One More Global Lock</h2>

<p>As mentioned previously, on paper, the true Global VM Lock is supposedly gone since the introduction of Ractors in Ruby 3.0
and instead, each ractor has its own “GVL”. But this isn’t actually true.</p>

<p>There are still a significant number of routines in the Ruby virtual machine that do lock all Ractors.
Let me show you an example.</p>

<p>Imagine you have 5 millions small JSON documents to parse:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># frozen_string_literal: true</span>
<span class="nb">require</span> <span class="s1">'json'</span>

<span class="n">document</span> <span class="o">=</span> <span class="o">&lt;&lt;~</span><span class="no">JSON</span><span class="sh">
  {"a": 1, "b": 2, "c": 3, "d": 4}
</span><span class="no">JSON</span>

<span class="mi">5_000_000</span><span class="p">.</span><span class="nf">times</span> <span class="k">do</span>
  <span class="no">JSON</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Doing so serially takes about 1.3 seconds on my machine:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">time </span>ruby <span class="nt">--yjit</span> /tmp/j.rb

real	0m1.292s
user	0m1.251s
sys	0m0.018s
</code></pre></div></div>

<p>As unrealistic as this script may look, it should be a perfect use case for Ractor. In theory, we could spawn
5 Ractors, have each of them parse 1 million documents, and be done in 1/5th of the time:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># frozen_string_literal: true</span>
<span class="nb">require</span> <span class="s1">'json'</span>

<span class="no">DOCUMENT</span> <span class="o">=</span> <span class="o">&lt;&lt;~</span><span class="no">JSON</span><span class="sh">
  {"a": 1, "b": 2, "c": 3, "d": 4}
</span><span class="no">JSON</span>

<span class="n">ractors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">.</span><span class="nf">times</span><span class="p">.</span><span class="nf">map</span> <span class="k">do</span>
  <span class="no">Ractor</span><span class="p">.</span><span class="nf">new</span> <span class="k">do</span>
    <span class="mi">1_000_000</span><span class="p">.</span><span class="nf">times</span> <span class="k">do</span>
      <span class="no">JSON</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="no">DOCUMENT</span><span class="p">)</span>
    <span class="k">end</span>
  <span class="k">end</span>
<span class="k">end</span>
<span class="n">ractors</span><span class="p">.</span><span class="nf">each</span><span class="p">(</span><span class="o">&amp;</span><span class="ss">:take</span><span class="p">)</span>
</code></pre></div></div>

<p>But somehow, it’s over twice as slow as doing it serially:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/tmp/jr.rb:9: warning: Ractor is experimental, and the behavior may change <span class="k">in </span>future versions of Ruby! Also there are many implementation issues.

real	0m3.191s
user	0m3.055s
sys	0m6.755s
</code></pre></div></div>

<p>What’s happening is that in this particular example, JSON has to acquire the true remaining VM lock for each key in
the JSON document.
With 4 keys, a million times, it means each Ractor has to acquire and release a lock 4 million times.
It’s almost surprising it only takes 3 seconds to do so.</p>

<p>For the keys, it needs to acquire the GVL because it inserts string keys into a Hash, and as I explained in
<a href="/ruby/json/2025/01/12/optimizing-ruby-json-part-6.html">Optimizing Ruby’s JSON, Part 6</a>, when you do that Ruby will
look inside the interned string table to search for an equivalent string that is already interned.</p>

<p>I used the following Ruby pseudo-code to explain how it works:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Hash</span>
  <span class="k">def</span> <span class="nf">[]=</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">entry</span> <span class="o">=</span> <span class="n">find_entry</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
      <span class="n">entry</span><span class="p">.</span><span class="nf">value</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">else</span>
      <span class="k">if</span> <span class="n">key</span><span class="p">.</span><span class="nf">is_a?</span><span class="p">(</span><span class="no">String</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">key</span><span class="p">.</span><span class="nf">interned?</span>
        <span class="k">if</span> <span class="n">interned_str</span> <span class="o">=</span> <span class="o">::</span><span class="no">RubyVM</span><span class="o">::</span><span class="no">INTERNED_STRING_TABLE</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
          <span class="n">key</span> <span class="o">=</span> <span class="n">interned_str</span>
        <span class="k">elsif</span> <span class="o">!</span><span class="n">key</span><span class="p">.</span><span class="nf">frozen?</span>
          <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">dup</span><span class="p">.</span><span class="nf">freeze</span>
        <span class="k">end</span>
      <span class="k">end</span>

      <span class="nb">self</span> <span class="o">&lt;&lt;</span> <span class="no">Entry</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">end</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p>In the above example <code class="language-plaintext highlighter-rouge">::RubyVM::INTERNED_STRING_TABLE</code> is a regular hash that could cause a crash if it was accessed
concurrently, so Ruby still acquires the GVL to look it up.</p>

<p>If you look at <a href="https://github.com/ruby/ruby/blob/d4b8da66ca9533782d2fed9762783c3e560f2998/string.c#L538-L570"><code class="language-plaintext highlighter-rouge">register_fstring</code> in <code class="language-plaintext highlighter-rouge">string.c</code></a>
(<code class="language-plaintext highlighter-rouge">fstring</code> is the internal name for interned strings), you can see the very obvious <code class="language-plaintext highlighter-rouge">RB_VM_LOCK_ENTER()</code> and
<code class="language-plaintext highlighter-rouge">RB_VM_LOCK_LEAVE()</code> calls.</p>

<p>As I’m writing this, there are 42 remaining calls to <code class="language-plaintext highlighter-rouge">RB_VM_LOCK_ENTER()</code> in the Ruby VM, many are very rarely hit and not
much of a problem, but this one demonstrates how even when you have what is a perfect use case for Ractors, besides their constraints,
it may still not be advantageous to use them yet.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In his RubyKaigi 2023 talk about the state of Ractors, Koichi Sasada who’s the main driving force behind them, mentioned that
Ractors suffered from <a href="https://youtu.be/Id706gYi3wk?si=DaECpXT2lEMO7kiA&amp;t=878">some sort of a chicken and egg problem</a>.
By his own admission, Ractors suffer from many bugs, and often don’t actually deliver the performance they’re supposed to,
hence very few people use them enough to be able to provide feedback on the API, and I’m afraid that almost two years later,
my assessment is the same on bugs and performance.</p>

<p>If Ractors bugs and performance problems were fixed, it’s likely that some of the provided feedback would lead to some of their
restrictions to be lifted over time.
I personally don’t think they’ll ever have little enough restrictions for it to be practical to run a full application inside a Ractor, hence that a Ractor-based web server would make sense, but who knows, I’d be happy to be proven wrong.</p>

<p>Ultimately, even if you are among the people who believe that Ruby should just try to remove its GVL for real rather
than to spend resources on Ractors, let me say that a large part of the work needed to make Ractors perform well,
like a concurrent hash map for interned strings, is work that would be needed to enable free threading anyway, so it’s not
wasted.</p>]]></content><author><name></name></author><category term="ruby" /><category term="performance" /><summary type="html"><![CDATA[I want to write a post about Pitchfork, explaining where it comes from, why it is like it is, and how I see its future. But before I can get to that, I think I need to share my mental model on a few things, in this case, Ractors.]]></summary></entry><entry><title type="html">There Isn’t Much Point to HTTP/2 Past The Load Balancer</title><link href="https://byroot.github.io/ruby/performance/2025/02/24/http2-past-the-load-balancer.html" rel="alternate" type="text/html" title="There Isn’t Much Point to HTTP/2 Past The Load Balancer" /><published>2025-02-24T19:47:51+00:00</published><updated>2025-02-24T19:47:51+00:00</updated><id>https://byroot.github.io/ruby/performance/2025/02/24/http2-past-the-load-balancer</id><content type="html" xml:base="https://byroot.github.io/ruby/performance/2025/02/24/http2-past-the-load-balancer.html"><![CDATA[<p>I want to write a post about <a href="https://rubygems.org/gems/pitchfork">Pitchfork</a>, explaining where it comes from, why it
is like it is, and how I see its future.
But before I can get to that, I think I need to share my mental model on a few things, in this case, HTTP/2.</p>

<p>From time to time, either online or at conferences, I hear people complain about the lack of support for HTTP/2 in
Ruby HTTP servers, generally Puma.
And every time I do the same, I ask them why they want that feature, and so far nobody had an actual use case for it.</p>

<p>Personally, this lack of support doesn’t bother me much, because the only use case I can see for it, is wanting to expose
your Ruby HTTP directly to the internet without any sort of load balancer or reverse proxy, which I understand may seem
tempting, as it’s “one less moving piece”, but not really worth the trouble in my opinion.</p>

<p>If you are not familiar with the HTTP protocol and what’s different in version 2 (and even 3 nowadays), you might
be surprised by this take, so let me try to explain what it is all about.</p>

<h2 id="what-does-http2-solve">What Does HTTP/2 Solve?</h2>

<p>HTTP/2 started under the name SPDY in 2009, with multiple goals, but mainly to reduce page load latency, by allowing it to
download more resources faster.
A major factor in page load time is that a page isn’t just a single HTTP request.
Once your browser has downloaded the HTML page and starts parsing it, it will find other resources it needs to also
download to render the page, be it stylesheets, scripts, or images.</p>

<p>So a page isn’t one HTTP request, but a cascade of them, and in the late 2000s, the number of resources on the average
page kept going up.
This bloat was in part offset by broadband getting better, but still, HTTP/1.1 wasn’t really adequate to download
many small files quickly for a few reasons.</p>

<p>The first one is that <a href="https://datatracker.ietf.org/doc/html/rfc2616">RFC 2616</a>, which introduced HTTP/1.1
specified that browsers were only allowed <em>two</em> concurrent connections to a given domain:</p>

<blockquote>
  <p>8.1.4 Practical Considerations</p>

  <p>Clients that use persistent connections SHOULD limit the number of simultaneous connections that they maintain to a
given server. A single-user client SHOULD NOT maintain more than 2 connections with any server or proxy.</p>
</blockquote>

<p>So if you can only request a single resource per connection, and are limited to two connections, even if you have a very
large bandwidth, the latency to the server will have a massive impact on performance whenever you need to download more than
a couple of resources.</p>

<p>Imagine you have an excellent 100Gb connection, but are trying to load a webpage hosted across the Atlantic ocean.
The roundtrip time to that server (your ping), will probably be around 60ms. If you need to download 100 small resources
through just two connections, it will take at least <code class="language-plaintext highlighter-rouge">ping * (resources / connections)</code>, so 3 seconds, which isn’t great.</p>

<p>That’s what made many frontend optimization techniques like assets bundling absolutely essentials back then, they
made a major difference in load time<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.
Similarly, some websites were using a technique called domain sharding, splitting assets into multiple domains to allow
more concurrency.</p>

<p>In theory, even these two connections could have been used much more effectively by pipelining requests,
the RFC 2616 has an entire section about it, and that was one of the big features added in HTTP/1.1 compared to 1.0.
The idea is simple, after sending your request, you don’t have to wait for the response before sending more requests.
You can send 10 requests immediately before having received a single response, and the server will send them one by one
in order.</p>

<p>But in practice most browsers ended up disabling that feature by default because they ran into misbehaving servers, dooming
the feature.
It also wasn’t perfect, as you could experience <em>head-of-line blocking</em>.
Since responses don’t have an identifier to map them to the request they’re the answer to, they have to be sent in order.
If one resource is slow to generate, all the subsequent resources can’t be sent yet.</p>

<p>That’s why as early as 2008, browsers stopped respecting the two concurrent connection rule.
Firefox 3 started raising the connection limit to 6 per domain, and most other browsers followed suit shortly after.</p>

<p>However, more concurrent connections isn’t an ideal solution, because TCP connections have a <em>slow start</em>.
When you connect to a remote address, your computer doesn’t know if the link to that other machine can support 10 gbit/s
or only 56 kbit/s.
Hence, to avoid flooding the network with tons of packets that will be dropped on the floor, it starts relatively slow
and periodically increase the throughput until it receives packet loss notifications, at that point it know it has more
or less reached the maximum throuhput the link can sustain.</p>

<p>That’s why persistent connections are a big deal, a freshly established connection has a much lower throughput than one
that has seen some use.</p>

<p>So by multiplying the number of connections, you can download more resources faster, but it would be preferable if they
were all downloaded from the same connection to not suffer as much from TCP slow start.</p>

<p>And that’s exactly the main thing HTTP/2 solved, by allowing multiplexing of requests inside a single TCP connection,
solving the head-of-line blocking issue<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>.</p>

<p>It also did a few other things, such as mandating the use of encryption<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> and also compressing request and response headers
with GZip, and “server push”, but multiplexing is really the big one.</p>

<h2 id="why-it-doesnt-matter-over-lan">Why It Doesn’t Matter Over LAN</h2>

<p>So the main motivation for HTTP/2 is multiplexing, and over the Internet, especially mobile Internet with somewhat more
spotty connections, it can have a massive impact.</p>

<p>But in the data center, not so much. If you think about it, the very big factor in the computation we did above was the
roundtrip time (ping) with the client.
Unless your infrastructure is terribly designed, that roundtrip time between your server (say Puma) and its client
(your load balancer or reverse proxy) should be extremely small, way under one millisecond, and totally dwarfed by the
actual request render time.</p>

<p>When you are serving mostly static assets over the Internet, latency may be high and HTTP/2 multiplexing is a huge deal.
But when you are serving application-generated responses over LAN (or even a UNIX socket), it won’t make a measurable
difference.</p>

<p>In addition to the low roundtrip time, the connections between your load balancer and application server likely have
a very long lifetime, hence don’t suffer from TCP slow start as much, and that’s assuming your operating system hasn’t
been tuned to disable slow start entirely, which is very common on servers.</p>

<h2 id="server-push-fail">Server Push Fail</h2>

<p>Another reason people may have wanted HTTP/2 all the way to the Ruby application server at one point was the “server push”
capability.</p>

<p>The idea was relatively simple, servers were allowed to send HTTP resources to the client without being prompted for it.
This way, when you request the landing page of a website, the server can send you all the associated resources up front
so your browser doesn’t have to parse the HTML to realize it needs them and start to ask for it.</p>

<p>However, that capability was actually removed from the spec and nowadays all browsers have removed it because was
actually doing more harm than good. It turns out that if the browser already had these resources in its cache, then
pushing them again would slow down the page load time.</p>

<p>People tried to find smart heuristics to know which resources may be in the cache or not, but in the end, none worked
and the feature was abandoned.</p>

<p>Today it has been superseded by <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/103">103 Early Hints</a>, which
is a much simpler and elegant spec, and is retro-compatible with HTTP/1.1.</p>

<p>So there isn’t any semantic difference left between HTTP/1.1 and HTTP/2.
From a <a href="https://github.com/rack/rack/blob/main/SPEC.rdoc"><code class="language-plaintext highlighter-rouge">Rack</code></a> application point of view, whether the request was
issued through an HTTP/2 or HTTP/1.1 connection makes no difference.
You can tunnel one into the other just fine.</p>

<h2 id="extra-complexity">Extra Complexity</h2>

<p>In addition to not providing much if any benefit over LAN, HTTP/2 adds some extra complexity.</p>

<p>First, the complexity of implementation, as HTTP/2 while not being crazy complicated at all, is still a largely binary
protocol, so it’s much harder to debug.</p>

<p><del>But also the complexity of deployment. HTTP/2 is fully encrypted, so you need all your application servers to have a key and
certificate, that’s not insurmountable, but is an extra hassle compared to just using HTTP/1.1, unless of course for some
reasons you are required to use only encrypted connections even over LAN.</del> Edit: The HTTP/2 spec doesn’t actually require
encryption, only browsers and some libraries, so you can do unencrypted HTTP/2 inside your datacenter.</p>

<p>So unless you are deploying to a single machine, hence don’t have a load balancer, bringing HTTP/2 all the way to
the Ruby app server is significantly complexifying your infrastructure for little benefit.</p>

<p>And even if you are on a single machine, it’s probably to leave that concern to a reverse proxy, which will also take
care of serving static assets, normalize inbound requests, and also probably fend off at least some malicious actors.</p>

<p>There are numerous battle-tested reverse proxies such as Nginx, Caddy, etc, and they’re pretty simple to setup,
might as well use these common middlewares rather than to try to do everything in a single Ruby application.</p>

<p>But if you think a reverse proxy is too much complexity and you’d rather do without, there are now zero config solutions
such as <a href="https://github.com/basecamp/thruster">thruster</a>, I haven’t tried it so I can’t vouch for it, but at least on
paper it solves that need.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I think HTTP/2 is better thought of not as an upgrade over HTTP/1.1, but as an alternative protocol to more efficiently
transport the same HTTP resources over the Internet. In a way, it’s similar to how HTTPS doesn’t change the semantics
of the HTTP protocol, it only changes how it’s serialized over the wire.</p>

<p>So I believe handling HTTP/2 is better left to your infrastructure entry point, typically the load balancer or reverse proxy, for the same
reason that TLS has been left to the load balancer or reverse proxy for ages. They have to decrypt and decompress
the request to know what to do with it, why re-encrypt and re-compress it to forward it to the app server?</p>

<p>Hence, in my opinion, HTTP/2 support in Ruby HTTP servers isn’t a critically important feature, would be nice to have it for a few
niche use cases, but overall, the lack of it isn’t hindering much of anything.</p>

<p>Note that I haven’t mentioned HTTP/3, but while the protocol is very different, its goals are largely the same as HTTP2, so I’d apply the same conclusion to it.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>Minifying and bundling still improve load time with HTTP/2, fewer requests and fewer bytes transferred are still positive, so they’re still useful, but it’s no longer critical to achieve a decent experience. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>At the HTTP layer at least, HTTP/2 still suffers from some forms of head-of-line blocking in lower layers, but it is beyond the scope of this post. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>The RFC doesn’t actually requires encryption, but all browser implementations do. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="ruby" /><category term="performance" /><summary type="html"><![CDATA[I want to write a post about Pitchfork, explaining where it comes from, why it is like it is, and how I see its future. But before I can get to that, I think I need to share my mental model on a few things, in this case, HTTP/2.]]></summary></entry><entry><title type="html">Guardrails Are Not Code Smells</title><link href="https://byroot.github.io/ruby/performance/2025/02/09/guard-rails-are-not-code-smells.html" rel="alternate" type="text/html" title="Guardrails Are Not Code Smells" /><published>2025-02-09T09:47:51+00:00</published><updated>2025-02-09T09:47:51+00:00</updated><id>https://byroot.github.io/ruby/performance/2025/02/09/guard-rails-are-not-code-smells</id><content type="html" xml:base="https://byroot.github.io/ruby/performance/2025/02/09/guard-rails-are-not-code-smells.html"><![CDATA[<p>I want to write a post about <a href="https://rubygems.org/gems/pitchfork">Pitchfork</a>, explaining where it comes from, why it
is like it is, and how I see its future.
But before I can get to that, I think I need to share my mental model on a few things, in this case, resiliency.</p>

<p>A few years ago, I read a tweet calling out <a href="https://github.com/kzk/unicorn-worker-killer"><code class="language-plaintext highlighter-rouge">unicorn-worker-killer</code></a> as a
major code smell.
I don’t quite remember who or when it was, it doesn’t matter anyway, but I think it is an interesting topic because,
depending on how that gem is used, I can either strongly agree with or vehemently oppose this tweet.</p>

<h2 id="what-does-it-do">What Does It Do?</h2>

<p><code class="language-plaintext highlighter-rouge">unicorn-worker-killer</code> provides two optional features for <a href="https://yhbt.net/unicorn/README.html">Unicorn</a>.</p>

<p>The first one allows to set a number of requests after which a Unicorn worker should be recycled, as in shutdown and
replaced by a fresh one, and the second allows to set the memory usage threshold after which a worker should be recycled.</p>

<p>In my opinion, the first setting is indeed a terrible code smell and there’s absolutely no valid reason to use it.
However the second one is absolutely essential, and in my opinion, it’s borderline irresponsible to deploy code to
production without something akin to it.</p>

<p>Both features are meant as ways to deal with memory leaks or bloat, so you may wonder why I have such radically different
opinions on what seem to be two fairly similar features.
It is because the former sweeps memory leaks under the rug, while the latter gracefully handles them.</p>

<h2 id="why-maxrequests-is-bad">Why MaxRequests is Bad</h2>

<p>One of the reasons why memory leaks are such a painful bug to deal with in Ruby applications is that unless they
are really bad, they’ll often take hours or days to bloat your process enough for the system OOM killer to trigger.
So contrary to most bugs, they’re not easy to correlate with an application deployment.
Assuming you deploy several times a day, you may only notice something went wrong over the weekend after you haven’t
deployed for a day or two. In some cases, you may even only notice it months later when most of the team is away for summer
or winter vacations and no deployment happens for a week.</p>

<p><code class="language-plaintext highlighter-rouge">Unicorn::WorkerKiller::MaxRequests</code> “solves” that by essentially continuously restarting your application on a regular
schedule, making it impossible to even know you are suffering from a memory leak.</p>

<p>And not only is it sweeping issues under the rug, but it also negatively impacts the performance of your application.
While restarting a Unicorn worker is a relatively cheap operation, Ruby processes need some warmup time.
Even if we ignore JIT warmup, the regular interpreter has lots of inline caches, so the first time a code path is
executed it needs to do some extra work, and that work tends to cause Copy-on-Write invalidations, further impacting latency.
And beyond the VM itself, while it’s a bad pattern, it’s very common for Ruby application to have some lazily initialized state.</p>

<p>That is why I believe <code class="language-plaintext highlighter-rouge">Unicorn::WorkerKiller::MaxRequests</code> is a major code smell.
It is an indication that it was acknowledged that a memory leak exists, but that the team has given up on trying to fix it.</p>

<p>Even if you are aware of this, you may think it’s an acceptable tradeoff for your use case, but it may only be a matter
of time until you don’t have one but two or ten memory leaks and then need to face the problem or keep lowering the
<code class="language-plaintext highlighter-rouge">max_requests</code> setting.</p>

<h2 id="why-maxmemory-is-good">Why MaxMemory is Good</h2>

<p>On the other hand, having a <code class="language-plaintext highlighter-rouge">max_memory</code> setting that gracefully restarts workers when they reach a given memory threshold
is an essential resiliency feature.</p>

<p>If your application doesn’t enforce a max memory policy by itself, the operating system will do it for you, and it won’t
be pretty.
What happens when there is no more available memory depends on the operating system and how it is configured but in the
context of Ruby programs, the most likely case is that you’ll run into the Linux OOM Killer.
In short, when Linux can no longer hand memory to a program, it looks at all the running processes on the machine, and
use a few heuristics to find a process to kill to free some memory.
Nothing guarantees that the process that will be killed is the one leaking, and the process will receive a <code class="language-plaintext highlighter-rouge">SIGKILL</code> so
it won’t be able to shut down cleanly and may cause huge problems.</p>

<p>For the anecdote, over a decade ago, I worked for a startup that had a PHP and Backbone.js application.
All of it, including the MySQL server, was running on a single machine.
One day there was a spike of traffic and the number of PHP processes caused the Linux OOM Killer to trigger, and its
heuristics thought that the big <code class="language-plaintext highlighter-rouge">mysqld</code> process was a much more attractive target than the hundreds of 30 MB PHP processes,
hence it decided to SIGKILL <code class="language-plaintext highlighter-rouge">mysqld</code> bringing the entire website to the ground.</p>

<p>That’s why if you don’t enforce a max memory policy yourself, you will be subject to the overall system memory limits
and if you ever reach them it won’t be pretty.</p>

<p>That’s why I believe <code class="language-plaintext highlighter-rouge">Unicorn::WorkerKiller::Oom</code> is an essential resiliency feature, even though it needs to be used
correctly.
It is crucial that every OOM event is reported somewhere and that there is alerting in place if it becomes
a frequent occurrence. If no one notices that suddenly OOM events are through the roof, then it’s no better than <code class="language-plaintext highlighter-rouge">max_request</code>.</p>

<p>You can even go farther than that.
Something I implemented in Shopify’s monolith, is that for a sample of OOM events before the worker is recycled,
the heap is first dumped using <a href="https://docs.ruby-lang.org/en/3.4/ObjectSpace.html#method-i-dump_all"><code class="language-plaintext highlighter-rouge">ObjectSpace.dump_all</code></a>
and uploaded in an object store, allowing us to investigate the root cause and identify leaks if any.</p>

<p>This is the best of both worlds.
OOM events are gracefully handled, any significant increase in such events is reported, and debugging is facilitated.</p>

<p>It might be that your application legitimately started using more memory and you just need to increase the threshold,
but it is preferable to have to review the policy once in a while than to be paged at night because a hard-to-diagnose
memory leak was introduced and started causing havoc.</p>

<p>On another note, I’m not a big fan of the exact implementation provided by <code class="language-plaintext highlighter-rouge">unicorn-worker-killer</code>, because it uses
<a href="https://en.wikipedia.org/wiki/Resident_set_size">RSS</a> as its memory metric, and I believe
<a href="https://en.wikipedia.org/wiki/Proportional_set_size">PSS</a> is a much better memory metrics for preforking applications<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p>

<p>In any case, cleanly shutting down workers on your own terms outside of the request cycle is much preferable to letting
the operating system sends <code class="language-plaintext highlighter-rouge">SIGKILL</code> to somewhat random processes.</p>

<h2 id="accept-it-bugs-will-happen">Accept It: Bugs Will Happen</h2>

<p>Here I used memory as an example because I think it’s a good illustration of where to draw the line.
But the point I’m trying to make is much more general.</p>

<p>Another example could be Unicorn’s built-in request timeout setting.
Whenever a worker is assigned a request, it is allotted a fixed amount of time to complete, if not, the worker will be
shut down and replaced.</p>

<p>You may think this is bad design and is working around a bug, but that’s the key to building resilient systems.
You must accept that bugs and other catastrophic events will happen eventually, hence systems should be designed in a
way that limits the blast radius of such events.</p>

<p>This isn’t to say bugs are acceptable.
You absolutely should strive to prevent as many bugs as reasonably possible upfront through testing, code reviews and what not.
Having a resilient system is in no way a license to be complacent and ship crap software.</p>

<p>But it is fanciful to believe you can prevent all bugs via sheer competency or process.</p>

<p>If you are telling me your car doesn’t have seatbelts because you are such a good driver that you don’t need them,
I will run in the opposite direction as fast as I can.</p>

<p>This attitude can function for a while at a small scale but is untenable whenever a project grows to have
a larger team, hence a faster rate of change.</p>

<h2 id="the-resilient-nature-of-share-nothing-architecture">The Resilient Nature of Share-Nothing Architecture</h2>

<p>This is why as an infrastructure engineer, I’m quite a fan of “share-nothing” architecture: it has inherent resiliency benefits.</p>

<p>Take PHP for instance.
As much as I dislike it as a programming language, I have to admit its classic execution model is really nice.
Every request starts with a fresh new process, making it close to impossible to leak state between requests.</p>

<p>That model wouldn’t perform well with Ruby though, because of the necessary warmup I mentioned previously,
but Unicorn’s model is somewhat halfway through.
Workers do process more than one request, but never concurrently, hence you still benefit from quite a high level of
isolation, and if something goes really wrong, killing the worker is always an option and the operating system will take
care of reclaiming the resources.</p>

<p>Whereas cleanly interrupting a thread is basically impossible.
You probably heard how <a href="https://jvns.ca/blog/2015/11/27/why-rubys-timeout-is-dangerous-and-thread-dot-raise-is-terrifying/">Ruby’s <code class="language-plaintext highlighter-rouge">Timeout</code> module should be avoided like the plague</a>,
well that’s because it is trying to interrupt a thread, and that can go really wrong.</p>

<p>If you look at <a href="https://github.com/zombocom/rack-timeout"><code class="language-plaintext highlighter-rouge">rack-timeout</code></a>, which is the Puma equivalent to Unicorn’s
request timeout.
You’ll see that whenever it has to timeout a request, it shutdowns the entire Puma worker because there’s no way to
cleanly shut down a single thread.</p>

<h2 id="conclusion">Conclusion</h2>

<p>As I tried to explain here, even if <a href="/ruby/performance/2025/01/29/so-you-want-to-remove-the-gvl.html">Ruby didn’t have a GVL</a>,
and even considering <a href="/ruby/performance/2025/01/25/why-does-everyone-hate-fork.html">fork has its share of problems</a>,
I strongly believe that this execution model has some very attractive properties that aren’t easily replaced.</p>

<p>As with everything, it’s of course a tradeoff, you gain some and lose some, but it shouldn’t be lazily discarded as some
sort of deprecated model for badly written applications.</p>

<p>It also has its benefits, and resiliency is just one of them.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>I wrote about the difference between RSS and PSS in <a href="https://shopify.engineering/ruby-execution-models">a post on Shopify’s engineering blog</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="ruby" /><category term="performance" /><summary type="html"><![CDATA[I want to write a post about Pitchfork, explaining where it comes from, why it is like it is, and how I see its future. But before I can get to that, I think I need to share my mental model on a few things, in this case, resiliency.]]></summary></entry><entry><title type="html">So You Want To Remove The GVL?</title><link href="https://byroot.github.io/ruby/performance/2025/01/29/so-you-want-to-remove-the-gvl.html" rel="alternate" type="text/html" title="So You Want To Remove The GVL?" /><published>2025-01-29T09:47:51+00:00</published><updated>2025-01-29T09:47:51+00:00</updated><id>https://byroot.github.io/ruby/performance/2025/01/29/so-you-want-to-remove-the-gvl</id><content type="html" xml:base="https://byroot.github.io/ruby/performance/2025/01/29/so-you-want-to-remove-the-gvl.html"><![CDATA[<p>I want to write a post about <a href="https://rubygems.org/gems/pitchfork">Pitchfork</a>, explaining where it comes from, why it
is like it is, and how I see its future.
But before I can get to that, I think I need to share my mental model on a few things, in this case, Ruby’s GVL.</p>

<p>For quite a long time, it has been said that Rails applications are mostly IO-bound, hence Ruby’s GVL isn’t that big of
a deal and that has influenced the design of some cornerstone pieces of Ruby infrastructure like Puma and Sidekiq.
As <a href="/ruby/performance/2025/01/23/the-mythical-io-bound-rails-app.html">I explained in a previous post, I don’t think it’s quite true for most Rails applications</a>.
Regardless, <a href="/ruby/performance/2025/01/25/why-does-everyone-hate-fork.html">the existence of the GVL still requires these threaded systems to use <code class="language-plaintext highlighter-rouge">fork(2)</code></a> in order to exploit all the cores of a server: one process per core.
To avoid all this, some people have been calling for the GVL to simply be removed.</p>

<p>But is it that simple?</p>

<h2 id="gvl-and-thread-safety">GVL and Thread Safety</h2>

<p>If you read posts about the GVL, you may have heard that it’s not there to protect your code from race conditions, but
to protect the Ruby VM from your code.
Put another way, GVL or not, your code can be subject to race conditions, and this is absolutely true.</p>

<p><strong>But that doesn’t mean the GVL isn’t an important component of the thread safety of the Ruby code in your applications</strong>.
Let’s use a simple code sample to illustrate:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="no">QUOTED_COLUMN_NAMES</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">def</span> <span class="nf">quote_column_name</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="no">QUOTED_COLUMN_NAMES</span><span class="p">[</span><span class="nb">name</span><span class="p">]</span> <span class="o">||=</span> <span class="n">quote</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Would you say this code is thread-safe? Or not?</p>

<p>Well, if you answered “It’s thread-safe”, you’re not quite correct.
But if you answered “It’s not thread safe”, you’re not quite correct either.</p>

<p>The actual answer is: “It depends”.</p>

<p>First, it depends on how strict of a definition of thread-safe you are thinking of,
then it depends on whether that <code class="language-plaintext highlighter-rouge">quote</code> method is idempotent and finally, it depends on which implementation of Ruby you are using.</p>

<p>Let me explain.</p>

<p>First <code class="language-plaintext highlighter-rouge">||=</code> is syntax sugar that is hiding a bit how this code actually works, so let’s desugar it:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="no">QUOTED_COLUMN_NAMES</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">def</span> <span class="nf">quote_column_name</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="n">quoted</span> <span class="o">=</span> <span class="no">QUOTED_COLUMN_NAMES</span><span class="p">[</span><span class="nb">name</span><span class="p">]</span>

  <span class="c1"># Ruby could switch threads here</span>

  <span class="k">if</span> <span class="n">quoted</span>
    <span class="n">quoted</span>
  <span class="k">else</span>
    <span class="no">QUOTED_COLUMN_NAMES</span><span class="p">[</span><span class="nb">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">quote</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p>In this form it’s easier to see that <code class="language-plaintext highlighter-rouge">||=</code> isn’t a single operation but multiple, so even on MRI<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, with a GVL, it’s
technically possible that Ruby would preempt a thread after evaluating <code class="language-plaintext highlighter-rouge">quoted = ...</code>, and resume another thread that will
enter the same method with the same argument.</p>

<p>In other words, this code is subject to race conditions, even with a GVL.
To be even more precise, it’s subject to a <em>check-then-act</em> race condition.</p>

<p>If it’s subject to race conditions, you can logically deduce that it’s not thread-safe.
But here again, it depends.
If <code class="language-plaintext highlighter-rouge">quote(name)</code> is idempotent, then yes there’s technically a race-condition, but it has no real negative impact.
The <code class="language-plaintext highlighter-rouge">name</code> will be quoted twice instead of once, and one of the resulting strings will be discarded, who cares?
That is why in my opinion the above code is effectively thread-safe regardless.</p>

<p>And we can verify this experimentally by using a few threads:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="no">QUOTED_COLUMN_NAMES</span> <span class="o">=</span> <span class="mi">20</span><span class="p">.</span><span class="nf">times</span><span class="p">.</span><span class="nf">to_h</span> <span class="p">{</span> <span class="o">|</span><span class="n">i</span><span class="o">|</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="p">}</span>

<span class="k">def</span> <span class="nf">quote_column_name</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="no">QUOTED_COLUMN_NAMES</span><span class="p">[</span><span class="nb">name</span><span class="p">]</span> <span class="o">||=</span> <span class="s2">"`</span><span class="si">#{</span><span class="nb">name</span><span class="p">.</span><span class="nf">to_s</span><span class="p">.</span><span class="nf">gsub</span><span class="p">(</span><span class="s1">'`'</span><span class="p">,</span> <span class="s1">'``'</span><span class="p">)</span><span class="si">}</span><span class="s2">`"</span><span class="p">.</span><span class="nf">freeze</span>
<span class="k">end</span>

<span class="n">threads</span> <span class="o">=</span> <span class="mi">4</span><span class="p">.</span><span class="nf">times</span><span class="p">.</span><span class="nf">map</span> <span class="k">do</span>
  <span class="no">Thread</span><span class="p">.</span><span class="nf">new</span> <span class="k">do</span>
    <span class="mi">10_000</span><span class="p">.</span><span class="nf">times</span> <span class="k">do</span>
      <span class="k">if</span> <span class="n">quote_column_name</span><span class="p">(</span><span class="s2">"foo"</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">"`foo`"</span>
        <span class="k">raise</span> <span class="s2">"There was a bug"</span>
      <span class="k">end</span>
      <span class="no">QUOTED_COLUMN_NAMES</span><span class="p">.</span><span class="nf">delete</span><span class="p">(</span><span class="s2">"foo"</span><span class="p">)</span>
    <span class="k">end</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="n">threads</span><span class="p">.</span><span class="nf">each</span><span class="p">(</span><span class="o">&amp;</span><span class="ss">:join</span><span class="p">)</span>
</code></pre></div></div>

<p>If you run this script with MRI, it will work fine, it won’t crash, and <code class="language-plaintext highlighter-rouge">quote_column_name</code> will always return what
you expect.</p>

<p>However, if you try to run it with either TruffleRuby or JRuby, which are alternative
implementations of Ruby that don’t have a GVL, you’ll get <a href="https://gist.github.com/byroot/1470a8fc71c2712a1f3ae875a9a40710">about 300 lines of errors</a>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ruby <span class="nt">-v</span> /tmp/quoted.rb 
truffleruby 24.1.2, like ruby 3.2.4, Oracle GraalVM Native <span class="o">[</span>arm64-darwin20]
java.lang.RuntimeException: Ruby Thread <span class="nb">id</span><span class="o">=</span>51 from /tmp/quoted.rb:20 terminated with internal error:
    at org.truffleruby.core.thread.ThreadManager.printInternalError<span class="o">(</span>ThreadManager.java:316<span class="o">)</span>
    ... 20 more
Caused by: java.lang.NullPointerException
    at org.truffleruby.core.hash.library.PackedHashStoreLibrary.getHashed<span class="o">(</span>PackedHashStoreLibrary.java:78<span class="o">)</span>
    ... 120 more
java.lang.RuntimeException: Ruby Thread <span class="nb">id</span><span class="o">=</span>52 from /tmp/quoted.rb:20 terminated with internal error:
    at org.truffleruby.core.thread.ThreadManager.printInternalError<span class="o">(</span>ThreadManager.java:316<span class="o">)</span>
    ... 20 more
... etc
</code></pre></div></div>

<p>The error isn’t always exactly the same, and sometimes it seems worse than others.
But in general, it crashes deep inside the TruffleRuby or JRuby interpreters because the concurrent access to the same
hash causes them to hit a <code class="language-plaintext highlighter-rouge">NullPointerException</code>.</p>

<p>So we can say this code is thread-safe on the reference implementation of Ruby, but not on all implementations of Ruby.</p>

<p>The reason it is that way is that on MRI, the thread scheduler can only switch the running thread when executing pure
Ruby code.
Whenever you call into a builtin method that is implemented in C, you are implicitly protected by the GVL.
Hence all methods implemented in C are essentially “atomic” unless they explicitly release the GVL.
But generally speaking, only IO methods will release it.</p>

<p>That’s why the real version of this code, that <a href="https://github.com/rails/rails/blob/0643592211dec558f93e57451a34393941144c8e/activerecord/lib/active_record/connection_adapters/sqlite3/quoting.rb#L9">I took from Active Record</a>,
doesn’t use a <code class="language-plaintext highlighter-rouge">Hash</code>, but a <code class="language-plaintext highlighter-rouge">Concurrent::Map</code>.
On MRI that class is pretty much just an alias for <code class="language-plaintext highlighter-rouge">Hash</code>, but on JRuby and TruffleRuby it’s defined as a hash table
with a mutex.
Officially Rails doesn’t support TruffleRuby or JRuby, but in practice, we tend to accommodate them with this sort of
small changes.</p>

<h2 id="just-remove-it-already">Just Remove It Already</h2>

<p>That’s why there’s “removing the GVL” and “removing the GVL”.</p>

<p>The <em>simple</em> way would be to do what TruffleRuby and JRuby do: nothing. Or close to nothing.</p>

<p>Since these alternative implementations are based on the Java Virtual Machine, which is memory-safe, they delegate to
the JVM runtime the hard job of failing but not hard crashing in such cases.
Given MRI is implemented in C, which is famously not memory-safe, just removing the GVL would cause the virtual machine
to run into a segmentation fault (or worse) when your code triggers this sort of race condition, so it wouldn’t be as simple.</p>

<p>Ruby would need to do something similar to what the JVM does, having some sort of atomic counter on every object that
could be subject to race conditions. Whenever you access an object you increment it and check it is set to <code class="language-plaintext highlighter-rouge">1</code> to ensure
nobody else is currently using it.</p>

<p>This in itself is quite a challenging task, as it means going over all the methods implemented in C (in Ruby itself but
also popular C extensions), to insert all these atomic increments and decrements.</p>

<p>It would also require some extra space in most Ruby objects for that new counter, likely 4 or 8 bytes, because atomic
operations aren’t easily done on smaller integer types. Unless of course there’s some smart trick I’m not privy of.</p>

<p>It would also result in a slow-down of the virtual machine, as all these atomic increments and decrements likely would
have a noticeable overhead, because atomic operations mean that the CPU has to ensure all cores see the operation at
the same time, so it essentially locks that part of the CPU cache.
I won’t try to guess how much that overhead would be in practice, but it certainly isn’t free.</p>

<p>And then the result would be that a lot of existing pure Ruby code, that used to be effectively thread safe, would no longer be.
So beyond the work ruby-core would have to do, Ruby users would also likely need to debug a bunch of thread safety issues
in their code, gems, etc.</p>

<p>That’s why despite the impressive efforts of JRuby and TruffleRuby teams to be as compatible as possible with MRI,
the absence of a GVL, which is a feature, makes it so that most non-trivial codebases likely need at least some debugging
before they can run properly on either of them. It’s not necessarily a ton of effort, it depends, but it’s more work
than your average yearly Ruby upgrade.</p>

<h2 id="replace-it-by-something">Replace It By Something</h2>

<p>But that’s not the only way to remove the GVL, another way that is often envisioned is to replace the one global lock,
by a myriad of small locks, one per every mutable object.</p>

<p>In terms of work needed, it’s fairly similar to the previous approach, you’d need to go over all the C code and insert
explicitly lock and unlock statements whenever you touch a mutable object.
It would also require some space on every object, likely a bit more than just a counter though.</p>

<p>With such approach, C extensions would still likely need some work, but pure Ruby code would remain fully compatible.</p>

<p>If you’ve heard about the semi-recent effort to remove Python’s GIL (that’s what they call their GVL), that’s the approach
they’re using. So let’s look at the sort of changes they made, starting with <a href="https://github.com/python/cpython/blob/180ee43bde99b8ce4c4f1d5237ab191e26118061/Include/object.h#L109-L162">their base object layout that is defined
in <code class="language-plaintext highlighter-rouge">object.h</code></a></p>

<p>It has lots of ceremonial code, so here’s a stripped-down and simplified version:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Nothing is actually declared to be a PyObject, but every pointer to
 * a Python object can be cast to a PyObject*.  This is inheritance built by hand.
 */</span>
<span class="cp">#ifndef Py_GIL_DISABLED
</span><span class="k">struct</span> <span class="n">_object</span> <span class="p">{</span>
    <span class="n">Py_ssize_t</span> <span class="n">ob_refcnt</span>
    <span class="n">PyTypeObject</span> <span class="o">*</span><span class="n">ob_type</span><span class="p">;</span>
<span class="p">};</span>
<span class="cp">#else
</span><span class="c1">// Objects that are not owned by any thread use a thread id (tid) of zero.</span>
<span class="c1">// This includes both immortal objects and objects whose reference count</span>
<span class="c1">// fields have been merged.</span>
<span class="cp">#define _Py_UNOWNED_TID             0
</span>
<span class="k">struct</span> <span class="n">_object</span> <span class="p">{</span>
    <span class="c1">// ob_tid stores the thread id (or zero). It is also used by the GC and the</span>
    <span class="c1">// trashcan mechanism as a linked list pointer and by the GC to store the</span>
    <span class="c1">// computed "gc_refs" refcount.</span>
    <span class="kt">uintptr_t</span> <span class="n">ob_tid</span><span class="p">;</span>
    <span class="kt">uint16_t</span> <span class="n">ob_flags</span><span class="p">;</span>
    <span class="n">PyMutex</span> <span class="n">ob_mutex</span><span class="p">;</span>           <span class="c1">// per-object lock</span>
    <span class="kt">uint8_t</span> <span class="n">ob_gc_bits</span><span class="p">;</span>         <span class="c1">// gc-related state</span>
    <span class="kt">uint32_t</span> <span class="n">ob_ref_local</span><span class="p">;</span>      <span class="c1">// local reference count</span>
    <span class="n">Py_ssize_t</span> <span class="n">ob_ref_shared</span><span class="p">;</span>   <span class="c1">// shared (atomic) reference count</span>
    <span class="n">PyTypeObject</span> <span class="o">*</span><span class="n">ob_type</span><span class="p">;</span>
<span class="p">};</span>
<span class="cp">#endif
</span></code></pre></div></div>

<p>There’s quite a lot in there, so let me describe it all. My entire explanation will assume a 64-bit architecture, to make things simpler.</p>

<p>Also note that while I used to be a Pythonista, that was 15 years ago, and nowadays I’m just spectating Python’s
development from afar. All this to say, I’ll do my best to correctly describe what they are doing, but it’s entirely
possible I get some of it wrong.</p>

<p>Anyway, when the GIL isn’t disabled as part of compilation, every single Python object starts with a header of <code class="language-plaintext highlighter-rouge">16B</code>,
the first <code class="language-plaintext highlighter-rouge">8B</code> called <code class="language-plaintext highlighter-rouge">ob_refcnt</code> is used for reference counting as the name implies, but actually only <code class="language-plaintext highlighter-rouge">4B</code> is used as a
counter, the other <code class="language-plaintext highlighter-rouge">4B</code> is used as a bitmap to set flags on the object, just like in Ruby.
Then the remaining <code class="language-plaintext highlighter-rouge">8B</code> is simply a pointer to the object’s class.</p>

<p>For comparison, Ruby’s object header, called <code class="language-plaintext highlighter-rouge">struct RBasic</code> is also <code class="language-plaintext highlighter-rouge">16B</code>. Similarly, it has one pointer to the class,
and the other <code class="language-plaintext highlighter-rouge">8B</code> is used as a big bitmap that stores many different things.</p>

<p>However, when the GIL is disabled during compilation, the object header is now <code class="language-plaintext highlighter-rouge">32B</code>, double the size.
It starts with an <code class="language-plaintext highlighter-rouge">8B</code> <code class="language-plaintext highlighter-rouge">ob_tid</code>, for thread ID, which stores which thread owns that particular object.
Then <code class="language-plaintext highlighter-rouge">ob_flags</code> is explicitly laid out, but has been reduced to <code class="language-plaintext highlighter-rouge">2B</code> instead of <code class="language-plaintext highlighter-rouge">4B</code>, to make space for a <code class="language-plaintext highlighter-rouge">1B</code> <code class="language-plaintext highlighter-rouge">ob_mutex</code>,
and another <code class="language-plaintext highlighter-rouge">1B</code> for some GC state I don’t know much about.</p>

<p>The <code class="language-plaintext highlighter-rouge">4B</code> <code class="language-plaintext highlighter-rouge">ob_refcnt</code> field is still there, but this time named <code class="language-plaintext highlighter-rouge">ob_ref_local</code>, and there is another <code class="language-plaintext highlighter-rouge">8B</code> <code class="language-plaintext highlighter-rouge">ob_ref_shared</code>,
and finally, the pointer to the object class.</p>

<p>Just with the change in the object layout, you can already have a sense of the extra complexity, as well as the memory
overhead. Sixteen extra bytes per object isn’t negligible.</p>

<p>Now, as you may have guessed from the <code class="language-plaintext highlighter-rouge">refcnt</code> field, Python’s memory is mainly managed via reference counting.
They also have a mark and sweep collector, but it’s only there to deal with circular references.
In that way, it’s quite different from Ruby, but looking at what they had to do to make this thread safe is interesting
regardless.</p>

<p>Let’s look at <a href="https://github.com/python/cpython/blob/180ee43bde99b8ce4c4f1d5237ab191e26118061/Include/refcount.h#L245-L294"><code class="language-plaintext highlighter-rouge">Py_INCREF</code>, defined in <code class="language-plaintext highlighter-rouge">refcount.h</code></a>.
Here again, it’s full of <code class="language-plaintext highlighter-rouge">ifdef</code> for various architecture and such, so here’s a stripped-down version, with only the code
executed when the GIL is active, and some debug code removed:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#define _Py_IMMORTAL_MINIMUM_REFCNT ((Py_ssize_t)(1L &lt;&lt; 30))
</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="n">Py_ALWAYS_INLINE</span> <span class="kt">int</span> <span class="nf">_Py_IsImmortal</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">op</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">return</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_refcnt</span> <span class="o">&gt;=</span> <span class="n">_Py_IMMORTAL_MINIMUM_REFCNT</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="n">Py_ALWAYS_INLINE</span> <span class="kt">void</span> <span class="nf">Py_INCREF</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">op</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">_Py_IsImmortal</span><span class="p">(</span><span class="n">op</span><span class="p">))</span> <span class="p">{</span>
        <span class="k">return</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_refcnt</span><span class="o">++</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>It’s extremely simple, even if you are unfamiliar with C you should be able to read it. But basically, it checks
if the refcount is set to a magical value that marks immortal objects, and if it isn’t immortal, it simply does a regular,
non-atomic, hence very cheap, increment of the counter.</p>

<p>A sidenote on immortal objects, it’s <a href="https://instagram-engineering.com/copy-on-write-friendly-python-garbage-collection-ad6ed5233ddf">a very cool concept introduced by Instagram engineers</a>
which I’ve been meaning to introduce in Ruby too. It’s well worth a read if you are interested in things like Copy-on-Write
and memory savings.</p>

<p>Now let’s look at that same <code class="language-plaintext highlighter-rouge">Py_INCREF</code> function, with the GIL removed:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#define _Py_IMMORTAL_REFCNT_LOCAL UINT32_MAX
# define _Py_REF_SHARED_SHIFT        2
</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="n">Py_ALWAYS_INLINE</span> <span class="kt">int</span> <span class="nf">_Py_IsImmortal</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">op</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_Py_atomic_load_uint32_relaxed</span><span class="p">(</span><span class="o">&amp;</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_ref_local</span><span class="p">)</span> <span class="o">==</span>
            <span class="n">_Py_IMMORTAL_REFCNT_LOCAL</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="n">Py_ALWAYS_INLINE</span> <span class="kt">int</span>
<span class="nf">_Py_IsOwnedByCurrentThread</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">ob</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">return</span> <span class="n">ob</span><span class="o">-&gt;</span><span class="n">ob_tid</span> <span class="o">==</span> <span class="n">_Py_ThreadId</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="n">Py_ALWAYS_INLINE</span> <span class="kt">void</span> <span class="nf">Py_INCREF</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">op</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">uint32_t</span> <span class="n">local</span> <span class="o">=</span> <span class="n">_Py_atomic_load_uint32_relaxed</span><span class="p">(</span><span class="o">&amp;</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_ref_local</span><span class="p">);</span>
    <span class="kt">uint32_t</span> <span class="n">new_local</span> <span class="o">=</span> <span class="n">local</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">new_local</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// local is equal to _Py_IMMORTAL_REFCNT_LOCAL: do nothing</span>
        <span class="k">return</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">_Py_IsOwnedByCurrentThread</span><span class="p">(</span><span class="n">op</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">_Py_atomic_store_uint32_relaxed</span><span class="p">(</span><span class="o">&amp;</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_ref_local</span><span class="p">,</span> <span class="n">new_local</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">else</span> <span class="p">{</span>
        <span class="n">_Py_atomic_add_ssize</span><span class="p">(</span><span class="o">&amp;</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_ref_shared</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">_Py_REF_SHARED_SHIFT</span><span class="p">));</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This is now way more involved.
First the <code class="language-plaintext highlighter-rouge">ob_ref_local</code> needs to be loaded atomically, which as mentioned previously is more costly than loading it
normally as it requires CPU cache synchronization.
Then we still have the check for immortal objects, nothing new.</p>

<p>The interesting part is the final <code class="language-plaintext highlighter-rouge">if</code>, as there are two different cases, the case where the object is owned by the
current thread and the case where it isn’t. Hence the first step is to compare the <code class="language-plaintext highlighter-rouge">ob_tid</code> with <code class="language-plaintext highlighter-rouge">_Py_ThreadId()</code>.
That function is way too big to include here, but you can check <a href="https://github.com/python/cpython/blob/180ee43bde99b8ce4c4f1d5237ab191e26118061/Include/object.h#L183-L246">its implementation in <code class="language-plaintext highlighter-rouge">object.h</code></a>,
on most platform it’s essentially free because the thread ID is always stored in a CPU register.</p>

<p>When the object is owned by the current thread, Python can get away with a non-atomic increment followed
by an atomic store.
Whereas in the opposite case, the entire increment has to be atomic, which is way more expensive as
it involves <a href="https://en.wikipedia.org/wiki/Compare-and-swap">compare and swap</a> operations.
Meaning that in case of a race condition, the CPU will retry the incrementation until it happens without a race condition.</p>

<p>In pseudo-Ruby it could look like this:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">atomic_compare_and_swap</span><span class="p">(</span><span class="n">was</span><span class="p">,</span> <span class="n">now</span><span class="p">)</span>
  <span class="c1"># assume this method is a single atomic CPU operation</span>
  <span class="k">if</span> <span class="vi">@memory</span> <span class="o">==</span> <span class="n">was</span>
    <span class="vi">@memory</span> <span class="o">=</span> <span class="n">now</span>
    <span class="k">return</span> <span class="kp">true</span>
  <span class="k">else</span>
    <span class="k">return</span> <span class="kp">false</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">atomic_increment</span><span class="p">(</span><span class="n">add</span><span class="p">)</span>
  <span class="kp">loop</span> <span class="k">do</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">atomic_load</span><span class="p">(</span><span class="vi">@memory</span><span class="p">)</span>
    <span class="k">break</span> <span class="k">if</span> <span class="n">atomic_compare_and_swap</span><span class="p">(</span><span class="n">value</span> <span class="o">+</span> <span class="n">add</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p>So you can see how what used to be a very mundane operation, that is a major Python hotspot,
became something noticeably more complex.
Ruby doesn’t use reference counting, so this particular case wouldn’t immediately translate to Ruby if there was an
attempt to remove the GVL, but Ruby still has a bunch of similar routines that are very frequently called and would
be similarly impacted.</p>

<p>For instance, because Ruby’s GC is generational and incremental, whenever a new reference is created between two objects,
say <code class="language-plaintext highlighter-rouge">A</code> towards <code class="language-plaintext highlighter-rouge">B</code>, Ruby might need to mark <code class="language-plaintext highlighter-rouge">A</code> as needing to be rescanned, and it is done by flipping one bit in a bitmap.
That’s one example of something that would need to be changed to use atomic operations.</p>

<p>But we still haven’t got to talk about the actual locking.
When I first heard about Python’s renewed attempt to remove their GIL, I expected they’d leverage the existing reference counting API to shove the locking in it, but clearly, they didn’t.
I’m not certain why, but I suppose the semantics don’t fully match.</p>

<p>Instead, they had to do what I mentioned earlier, go over all the methods implemented in C to add explicit lock and unlock
calls. To illustrate, we can look at the <code class="language-plaintext highlighter-rouge">list.clear()</code> method, which is the Python equivalent to <code class="language-plaintext highlighter-rouge">Array#clear</code>.</p>

<p>Prior to the GIL removal effort, it looked like this:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span>
<span class="nf">PyList_Clear</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">self</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">PyList_Check</span><span class="p">(</span><span class="n">self</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">PyErr_BadInternalCall</span><span class="p">();</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">list_clear</span><span class="p">((</span><span class="n">PyListObject</span><span class="o">*</span><span class="p">)</span><span class="n">self</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>It looks simpler than it actually is because most of the complexity is in the <code class="language-plaintext highlighter-rouge">list_clear</code> routine, but regardless,
it’s fairly straightforward.</p>

<p>Quite a while after the project started, <a href="https://github.com/python/cpython/issues/127536">Python developers noticed they forgot to add some locks to <code class="language-plaintext highlighter-rouge">list.clear</code> and
a few other methods</a>, so they changed it for:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span>
<span class="nf">PyList_Clear</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">self</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">PyList_Check</span><span class="p">(</span><span class="n">self</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">PyErr_BadInternalCall</span><span class="p">();</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">Py_BEGIN_CRITICAL_SECTION</span><span class="p">(</span><span class="n">self</span><span class="p">);</span>
    <span class="n">list_clear</span><span class="p">((</span><span class="n">PyListObject</span><span class="o">*</span><span class="p">)</span><span class="n">self</span><span class="p">);</span>
    <span class="n">Py_END_CRITICAL_SECTION</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Not that much worse, they managed to encapsulate it all in two macros that are just noops when Python is built with the GIL enabled.</p>

<p>I’m not going to explain everything happening in <code class="language-plaintext highlighter-rouge">Py_BEGIN_CRITICAL_SECTION</code>, some of it flies over my head anyway, but long story short it ends up in <code class="language-plaintext highlighter-rouge">_PyCriticalSection_BeginMutex</code>, which has a fast path and a slow path:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span>
<span class="nf">_PyCriticalSection_BeginMutex</span><span class="p">(</span><span class="n">PyCriticalSection</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="n">PyMutex</span> <span class="o">*</span><span class="n">m</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">PyMutex_LockFast</span><span class="p">(</span><span class="n">m</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">PyThreadState</span> <span class="o">*</span><span class="n">tstate</span> <span class="o">=</span> <span class="n">_PyThreadState_GET</span><span class="p">();</span>
        <span class="n">c</span><span class="o">-&gt;</span><span class="n">_cs_mutex</span> <span class="o">=</span> <span class="n">m</span><span class="p">;</span>
        <span class="n">c</span><span class="o">-&gt;</span><span class="n">_cs_prev</span> <span class="o">=</span> <span class="n">tstate</span><span class="o">-&gt;</span><span class="n">critical_section</span><span class="p">;</span>
        <span class="n">tstate</span><span class="o">-&gt;</span><span class="n">critical_section</span> <span class="o">=</span> <span class="p">(</span><span class="kt">uintptr_t</span><span class="p">)</span><span class="n">c</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">else</span> <span class="p">{</span>
        <span class="n">_PyCriticalSection_BeginSlow</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">m</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>What the fast path does, is that it assumes the object’s <code class="language-plaintext highlighter-rouge">ob_mutex</code> field is set to <code class="language-plaintext highlighter-rouge">0</code>, and tries
to set it to <code class="language-plaintext highlighter-rouge">1</code> with an atomic compare and swap:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//_Py_UNLOCKED is defined as 0 and _Py_LOCKED as 1 in Include/cpython/lock.h</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span>
<span class="nf">PyMutex_LockFast</span><span class="p">(</span><span class="n">PyMutex</span> <span class="o">*</span><span class="n">m</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">uint8_t</span> <span class="n">expected</span> <span class="o">=</span> <span class="n">_Py_UNLOCKED</span><span class="p">;</span>
    <span class="kt">uint8_t</span> <span class="o">*</span><span class="n">lock_bits</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">m</span><span class="o">-&gt;</span><span class="n">_bits</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">_Py_atomic_compare_exchange_uint8</span><span class="p">(</span><span class="n">lock_bits</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">expected</span><span class="p">,</span> <span class="n">_Py_LOCKED</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>If that works, it knows the object was unlocked so it can just to a little bit of book keeping.</p>

<p>If that doesn’t work, however, it enters the slow path, and there it starts to become quite complicated but to describe it quickly, it first uses a spin-lock with 40 iterations. So in a way, it does the same compare and swap logic 40 times in a raw with the hope that it might work eventually.
And if that still doesn’t work, it then “parks” the thread and will wait for a signal to resume.
If you are interested in knowing more you can look at <a href="https://github.com/python/cpython/blob/7dd0a7e52ee832559b89d5ccba732c8e91260df8/Python/lock.c#L50-L135"><code class="language-plaintext highlighter-rouge">_PyMutex_LockTimed</code> in <code class="language-plaintext highlighter-rouge">Python/lock.c</code></a>
and follow the code from there. Ultimately the mutex code isn’t that interesting for our current topic,
because the assumption is that most objects are only ever accessed by a single thread, so the fast path is what matters
the most.</p>

<p>But beyond the cost of that fast path, what is also important is how to integrate the lock and unlock statements
in an existing codebase. If you forget one <code class="language-plaintext highlighter-rouge">lock()</code>, you might cause a VM crash, and if you forget one <code class="language-plaintext highlighter-rouge">unlock()</code>, you
might cause a VM dead-lock, which is arguably even worse.</p>

<p>So let’s go back to that <code class="language-plaintext highlighter-rouge">list.clear()</code> example:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span>
<span class="nf">PyList_Clear</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">self</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">PyList_Check</span><span class="p">(</span><span class="n">self</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">PyErr_BadInternalCall</span><span class="p">();</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">Py_BEGIN_CRITICAL_SECTION</span><span class="p">(</span><span class="n">self</span><span class="p">);</span>
    <span class="n">list_clear</span><span class="p">((</span><span class="n">PyListObject</span><span class="o">*</span><span class="p">)</span><span class="n">self</span><span class="p">);</span>
    <span class="n">Py_END_CRITICAL_SECTION</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>You may have noticed how Python does error checking. When a bad precondition is found, it generates an exception
with a <code class="language-plaintext highlighter-rouge">PyErr_*</code> function and returns <code class="language-plaintext highlighter-rouge">-1</code>. That’s because <code class="language-plaintext highlighter-rouge">list.clear()</code> always returns <code class="language-plaintext highlighter-rouge">None</code> (Python’s <code class="language-plaintext highlighter-rouge">nil</code>),
so the return type of its C implementation is just an <code class="language-plaintext highlighter-rouge">int</code>.
For a method that returns a Ruby object, on an error condition it would return a <code class="language-plaintext highlighter-rouge">NULL</code> pointer.</p>

<p>For instance <code class="language-plaintext highlighter-rouge">list.__getitem__</code>, which is Python’s equivalent to <code class="language-plaintext highlighter-rouge">Array#fetch</code> is defined as:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PyObject</span> <span class="o">*</span>
<span class="nf">PyList_GetItem</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">op</span><span class="p">,</span> <span class="n">Py_ssize_t</span> <span class="n">i</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">PyList_Check</span><span class="p">(</span><span class="n">op</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">PyErr_BadInternalCall</span><span class="p">();</span>
        <span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">valid_index</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">Py_SIZE</span><span class="p">(</span><span class="n">op</span><span class="p">)))</span> <span class="p">{</span>
        <span class="n">_Py_DECLARE_STR</span><span class="p">(</span><span class="n">list_err</span><span class="p">,</span> <span class="s">"list index out of range"</span><span class="p">);</span>
        <span class="n">PyErr_SetObject</span><span class="p">(</span><span class="n">PyExc_IndexError</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">_Py_STR</span><span class="p">(</span><span class="n">list_err</span><span class="p">));</span>
        <span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">PyListObject</span> <span class="o">*</span><span class="p">)</span><span class="n">op</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ob_item</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div>

<p>You can see that error if you try accessing a Python list with an out-of-bound index:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="p">[]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="p">[</span><span class="mi">12</span><span class="p">]</span>
<span class="nc">Traceback </span><span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">):</span>
  <span class="n">File</span> <span class="sh">"</span><span class="s">&lt;stdin&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">1</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="nb">IndexError</span><span class="p">:</span> <span class="nb">list</span> <span class="n">index</span> <span class="n">out</span> <span class="n">of</span> <span class="nb">range</span>
</code></pre></div></div>

<p>You can recognize the same <code class="language-plaintext highlighter-rouge">IndexError</code> and the same <code class="language-plaintext highlighter-rouge">list index out of range</code> message.</p>

<p>So in both cases, when the Python methods implemented in C need to raise an exception, they build the exception object, store it in some thread local state, and then return a specific value to let the interpreter know that an exception happened.
When the interpreter notices the return value of the function is one of these special values, it starts unwinding the stack.
In a way, Python exceptions are syntactic sugar for the classic <code class="language-plaintext highlighter-rouge">if (error) { return error }</code> pattern.</p>

<p>Now let’s look at Ruby’s <code class="language-plaintext highlighter-rouge">Array#fetch</code>, and see if you notice any difference in how the out-of-bound case is handled:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="n">VALUE</span>
<span class="nf">rb_ary_fetch</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="n">VALUE</span> <span class="o">*</span><span class="n">argv</span><span class="p">,</span> <span class="n">VALUE</span> <span class="n">ary</span><span class="p">)</span>
<span class="p">{</span>
    <span class="c1">// snip...</span>
    <span class="kt">long</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">NUM2LONG</span><span class="p">(</span><span class="n">pos</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">RARRAY_LEN</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">idx</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">block_given</span><span class="p">)</span> <span class="k">return</span> <span class="n">rb_yield</span><span class="p">(</span><span class="n">pos</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">rb_raise</span><span class="p">(</span><span class="n">rb_eIndexError</span><span class="p">,</span> <span class="s">"index %ld outside of..."</span><span class="p">,</span> <span class="cm">/* snip... */</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">ifnone</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">RARRAY_AREF</span><span class="p">(</span><span class="n">ary</span><span class="p">,</span> <span class="n">idx</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Did you notice how there is no explicit <code class="language-plaintext highlighter-rouge">return</code> after <code class="language-plaintext highlighter-rouge">rb_raise</code>?</p>

<p>That’s because Ruby exceptions are very different from Python exceptions, as they rely on <a href="https://man7.org/linux/man-pages/man3/setjmp.3.html"><code class="language-plaintext highlighter-rouge">setjmp(3)</code></a>
and <a href="https://man7.org/linux/man-pages/man3/longjmp.3p.html"><code class="language-plaintext highlighter-rouge">longjmp(3)</code></a>.</p>

<p>Without going into too much detail, these two functions essentially allow you to make some sort of “savepoint” of the stack
and jump back to it. When they are used, it’s a bit like a non-local <code class="language-plaintext highlighter-rouge">goto</code>, you directly jump back to a parent function
and all the intermediate functions never return.</p>

<p>As a consequence, a Ruby equivalent of <code class="language-plaintext highlighter-rouge">Py_BEGIN_CRITICAL_SECTION</code> would need to call <code class="language-plaintext highlighter-rouge">setjmp</code>, and push the associated
checkpoint on the execution context (essentially the current fiber) using <a href="https://github.com/ruby/ruby/blob/4a06ef98bfd480a3d724b16c2d7da071e373a69c/eval_intern.h#L98-L110">the <code class="language-plaintext highlighter-rouge">EC_PUSH_TAG</code> macro</a>,
so essentially every core method would now need a <code class="language-plaintext highlighter-rouge">rescue</code> clause, and that’s not free.
It’s doable, but likely more costly than <code class="language-plaintext highlighter-rouge">Py_BEGIN_CRITICAL_SECTION</code>.</p>

<h2 id="shall-we">Shall We?</h2>

<p>But we were so preoccupied with whether or not we could remove the GVL, we didn’t stop to think if we should.</p>

<p>In the case of Python, from my understanding, the driving force behind the effort to remove the GIL is mostly the machine learning community, in big part, because feeding graphic cards efficiently requires a fairly high level of
parallelism, and <code class="language-plaintext highlighter-rouge">fork(2)</code> isn’t very suitable for it.</p>

<p>But, again from my understanding, the Python Web community, such as Django users, seem to be content with <code class="language-plaintext highlighter-rouge">fork(2)</code>,
even though Python is at a major disadvantage over Ruby in terms of Copy-on-Write effectiveness, because as we saw previously, its reference counting implementation means most objects are constantly written to, so CoW pages are very quickly invalidated.</p>

<p>On the other hand, Ruby’s mark-and-sweep GC is much more Copy-On-Write friendly, as almost all the GC tracking data
isn’t stored in the objects themselves but inside external bitmaps.
Hence, one of the main arguments for GVL free threading, which is to reduce memory usage, is much less important in the
case of Ruby.</p>

<p>Given that Ruby (for better or for worse) is predominantly used for the Web use case, it can at least partially explain why the pressure to remove the GVL isn’t as strong as it has been with Python.
Similarly, Node.js and PHP don’t have free threading either, but as far as I know their respective communities
aren’t complaining much about it, unless I missed it.</p>

<p>Also if Ruby were to adopt some form of free threading, it would probably need to add some form of lock in all objects,
and would frequently mutate it, likely severely reducing Copy-on-Write efficiency.
So it wouldn’t be purely an additive feature.</p>

<p>Similarly, one of the main blocker for removing Python’s GIL has always been the negative impact on single-thread performance.
When you are dealing with easily parallelizable algorithms, even if single-thread performance is degraded, you can
probably come out on top by using more parallelism.
But if the sort of thing you use Python for isn’t easily parallelizable, free-threading may not be particularly appealing to you.</p>

<p>Historically, Guido van Rossum’s stance on removing the GIL was that he’d welcome it as long as it had no impact on
single-thread performance, hence why it never happened.
Now that Guido is no longer Python’s benevolent dictator, it seems that the Python steering council is willing to accept
some regression on single-thread performance, but it isn’t yet clear how much it will actually be.
There are some numbers flying around, but mostly from synthetic benchmarks and such.
Personally, I’d be interested to see the impact on Web applications before I’d be enthusiastic about such change happening to Ruby.
It is also important to note that <a href="https://peps.python.org/pep-0703/">the removal has been accepted but with some proviso</a>,
so it isn’t yet done and it’s not impossible that they might decide to backtrack at one point.</p>

<p>Another thing to consider is that the performance impact on Ruby might be worse than for Python,
because the objects that need the extra overhead are the mutable ones, and contrary to Python, in Ruby that includes strings.
Think of how many string operations the average web application is doing.</p>

<p>On the other side, one argument I can think of in favor of removing the GVL though, would be YJIT.
Given the native code YJIT generates, and the associated metadata it keeps are scoped to the process, no longer
relying on <code class="language-plaintext highlighter-rouge">fork(2)</code> for parallelism would save quite a lot of memory, just by sharing all this memory, that being said,
removing the GVL would also make YJIT’s life much harder, so it may just as much hinder its progress.</p>

<p>Another argument in favor of free threading is that forked processes can’t easily share connections.
So when you start scaling Rails application to a large number of CPU cores, you end up with a lot more connections
to your datastore than with stacks that have free threading, and this can be a big bottleneck, particularly with
some databases with costly connections like PostgreSQL.
Currently, this is largely solved by using external connection poolers, like PgBouncer or ProxySQL, which I understand
aren’t perfect. It’s one more moving piece that can go wrong, but I think it’s much less trouble than free threading.</p>

<p>And finally, I’d like to note that the GVL isn’t the whole picture.
If the goal is to replace <code class="language-plaintext highlighter-rouge">fork(2)</code> by free-threading, even once the GVL is removed, we might still not quite be
there because Ruby’s GC is “stop the world”, so with much more code execution happening in a single process,
hence much more allocations, we may find out that it would become the new contention point.
So personally, I’d rather aim for a fully concurrent GC before wishing the GVL removed.</p>

<h2 id="so-it-is-urgent-to-do-nothing">So It Is Urgent To Do Nothing?</h2>

<p>At this point, some of you may feel like I’m trying to gaslight people into thinking that the GVL is never a problem,
but that’s not exactly my opinion.</p>

<p>I do absolutely think the GVL is currently causing some very real problems in real world applications, namely contention.
But this is very different from wanting the GVL removed and I beleive the situation could be noticeably improved in other ways.</p>

<p>If you’ve read <a href="/ruby/performance/2025/01/23/io-instrumentation.html">my short article on how to properly measure IO time in Ruby</a>,
you may be familiar with the GVL contention problem, but let me include the same test script here:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">require</span> <span class="s2">"bundler/inline"</span>

<span class="n">gemfile</span> <span class="k">do</span>
  <span class="n">gem</span> <span class="s2">"bigdecimal"</span> <span class="c1"># for trilogy</span>
  <span class="n">gem</span> <span class="s2">"trilogy"</span>
  <span class="n">gem</span> <span class="s2">"gvltools"</span>
<span class="k">end</span>

<span class="no">GVLTools</span><span class="o">::</span><span class="no">LocalTimer</span><span class="p">.</span><span class="nf">enable</span>

<span class="k">def</span> <span class="nf">measure_time</span>
  <span class="n">realtime_start</span> <span class="o">=</span> <span class="no">Process</span><span class="p">.</span><span class="nf">clock_gettime</span><span class="p">(</span><span class="no">Process</span><span class="o">::</span><span class="no">CLOCK_MONOTONIC</span><span class="p">,</span> <span class="ss">:float_millisecond</span><span class="p">)</span>
  <span class="n">gvl_time_start</span> <span class="o">=</span> <span class="no">GVLTools</span><span class="o">::</span><span class="no">LocalTimer</span><span class="p">.</span><span class="nf">monotonic_time</span>
  <span class="k">yield</span>

  <span class="n">realtime</span> <span class="o">=</span> <span class="no">Process</span><span class="p">.</span><span class="nf">clock_gettime</span><span class="p">(</span><span class="no">Process</span><span class="o">::</span><span class="no">CLOCK_MONOTONIC</span><span class="p">,</span> <span class="ss">:float_millisecond</span><span class="p">)</span> <span class="o">-</span> <span class="n">realtime_start</span>
  <span class="n">gvl_time</span> <span class="o">=</span> <span class="no">GVLTools</span><span class="o">::</span><span class="no">LocalTimer</span><span class="p">.</span><span class="nf">monotonic_time</span> <span class="o">-</span> <span class="n">gvl_time_start</span>
  <span class="n">gvl_time_ms</span> <span class="o">=</span> <span class="n">gvl_time</span> <span class="o">/</span> <span class="mf">1_000_000.0</span>
  <span class="n">io_time</span> <span class="o">=</span> <span class="n">realtime</span> <span class="o">-</span> <span class="n">gvl_time_ms</span>
  <span class="nb">puts</span> <span class="s2">"io: </span><span class="si">#{</span><span class="n">io_time</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">ms, gvl_wait: </span><span class="si">#{</span><span class="n">gvl_time_ms</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">ms"</span>
<span class="k">end</span>

<span class="n">trilogy</span> <span class="o">=</span> <span class="no">Trilogy</span><span class="p">.</span><span class="nf">new</span>

<span class="c1"># Measure a first time with just the main thread</span>
<span class="n">measure_time</span> <span class="k">do</span>
  <span class="n">trilogy</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="s2">"SELECT 1"</span><span class="p">)</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">fibonacci</span><span class="p">(</span> <span class="n">n</span> <span class="p">)</span>
  <span class="k">return</span>  <span class="n">n</span>  <span class="k">if</span> <span class="p">(</span> <span class="mi">0</span><span class="o">..</span><span class="mi">1</span> <span class="p">).</span><span class="nf">include?</span> <span class="n">n</span>
  <span class="p">(</span> <span class="n">fibonacci</span><span class="p">(</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">)</span> <span class="o">+</span> <span class="n">fibonacci</span><span class="p">(</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">2</span> <span class="p">)</span> <span class="p">)</span>
<span class="k">end</span>

<span class="c1"># Spawn 5 CPU-heavy threads</span>
<span class="n">threads</span> <span class="o">=</span> <span class="mi">5</span><span class="p">.</span><span class="nf">times</span><span class="p">.</span><span class="nf">map</span> <span class="k">do</span>
  <span class="no">Thread</span><span class="p">.</span><span class="nf">new</span> <span class="k">do</span>
    <span class="kp">loop</span> <span class="k">do</span>
      <span class="n">fibonacci</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
    <span class="k">end</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="c1"># Measure again with the background threads</span>
<span class="n">measure_time</span> <span class="k">do</span>
  <span class="n">trilogy</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="s2">"SELECT 1"</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div></div>

<p>If you run it, you should get something like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>realtime: 0.22ms, gvl_wait: 0.0ms, io: 0.2ms
realtime: 549.29ms, gvl_wait: 549.22ms, io: 0.1ms
</code></pre></div></div>

<p>This script demonstrates how GVL contention can cause havoc on your application latency.
And even if you use a single-threaded server like Unicorn or Pitchfork, it doesn’t mean the applications only use
a single thread.
It’s incredibly common to have various background threads to perform some service tasks, such as monitoring.
One example of that is <a href="https://github.com/Shopify/statsd-instrument/blob/6fd8c49d50803bbccfcc11b195f9e334a6e835e9/lib/statsd/instrument/batched_sink.rb#L163">the <code class="language-plaintext highlighter-rouge">statsd-instrument</code> gem</a>.
When you emit a metric, it’s collected in memory, and then a background thread takes care of serializing and sending these metrics
in batch. It’s supposed to be largely IO work, hence shouldn’t have too much impact on the main threads, but in practice,
it can happen that these sorts of background threads hold the GVL for much longer than you’d like.</p>

<p>So while my demo script is extreme, you can absolutely experience some level of GVL contention in production,
regardless of the server you use.</p>

<p>But I don’t think trying to remove the GVL is necessarily the best way to tame that problem, as it would take years of
tears and sweat before you’d reap any benefits.</p>

<p>Prior to something like 2006, multi-core CPUs were basically non-existent, and yet, you were perfectly able to multi-task
on your computer in a relatively smooth way, crunching numbers in Excel while playing some music in Winamp, and this without any parallelism.</p>

<p>That’s because even Windows 95 had a somewhat decent thread scheduler, but Ruby still doesn’t.
What Ruby does when a thread is ready to execute and has to wait for the GVL, is that it puts it in a FIFO queue,
and whenever the running thread releases the GVL, either because it did some IO or because it ran for its allocated 100ms,
Ruby’s thread scheduler pops the next one.</p>

<p>There is no notion of priority or anything. A semi-decent scheduler should be able to notice that a thread is mostly IO and that interrupting the current thread to schedule the IO-heavy thread faster is likely worth it.</p>

<p>So before trying to remove the GVL, it would be worth trying to implement a proper thread scheduler.
Credit goes to <a href="https://github.com/jhawthorn/">John Hawthorn</a> for that idea.</p>

<p>In the meantime, <a href="https://github.com/tenderlove">Aaron Patterson</a> shipped <a href="https://bugs.ruby-lang.org/issues/20861">a change in Ruby 3.4 to allow reducing the
100ms quantum via an environment variable</a>. It doesn’t solve everything, but
it can probably already help in some cases, so it’s a start.</p>

<p>Another idea John shared in one of our conversations<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, would be to allow more CPU operations with the GVL released.
Currently, most database clients only really release the GVL around the IO, think of it like it:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">query</span><span class="p">(</span><span class="n">sql</span><span class="p">)</span>
  <span class="n">response</span> <span class="o">=</span> <span class="kp">nil</span>
  <span class="n">request</span> <span class="o">=</span> <span class="n">build_network_packet</span><span class="p">(</span><span class="n">sql</span><span class="p">)</span>

  <span class="n">release_gvl</span> <span class="k">do</span>
    <span class="n">socket</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">socket</span><span class="p">.</span><span class="nf">read</span>
  <span class="k">end</span>

  <span class="n">parse_db_response</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div></div>

<p>For simple queries that return a non-trivial amount of data, it is likely that you are actually spending much more time
building the Ruby objects with the GVL acquired, than waiting on the DB response with the GVL released.</p>

<p>This is because very very few of the Ruby C API can be used with the GVL released, notably, anything that allocates
and object, or could potentially raise an exception MUST acquire the GVL.</p>

<p>If this constraint was removed, such that you could create basic Ruby objects such as String, Array, and Hashes with
the GVL released, it would likely allow the GVL to be released much longer and significantly reduce contention.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I’m personally not really in favor of removing the GVL, I don’t think the tradeoff is quite worth it, at least not yet,
nor do I think it would be as much of a game-changer as some may imagine.</p>

<p>If it didn’t have any impact on the classic (mostly) single-threaded performance, I wouldn’t mind it,
but it is almost guaranteed to degrade single-threaded performance significantly, hence this feels a bit like
“a bird in the hand is worth two in the bush” kind of proposition.</p>

<p>Instead, I believe there are some much easier and smaller changes we could make to Ruby that would improve the situation
on a much shorter timeline and with much less effort both for Ruby-core and for Ruby users.</p>

<p>But of course that is just the perspective of a single Ruby user with mostly my own use case in mind,
and ultimately this is for Matz to decide, based on what he thinks the community wants and needs.</p>

<p>For now, Matz doesn’t want to remove the GVL and He instead accepted the Ractor proposal<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>. 
Perhaps his opinion may change one day, we’ll see.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>MRI: Matz’s Ruby Interpreter, the reference implementation of Ruby, sometimes referred to as CRuby. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>If you didn’t notice, John is incredibly clever. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Ractors which I also wanted to discuss in this post, but it’s already too long, so maybe another time. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="ruby" /><category term="performance" /><summary type="html"><![CDATA[I want to write a post about Pitchfork, explaining where it comes from, why it is like it is, and how I see its future. But before I can get to that, I think I need to share my mental model on a few things, in this case, Ruby’s GVL.]]></summary></entry><entry><title type="html">Why Does Everyone Hate fork(2)?</title><link href="https://byroot.github.io/ruby/performance/2025/01/25/why-does-everyone-hate-fork.html" rel="alternate" type="text/html" title="Why Does Everyone Hate fork(2)?" /><published>2025-01-25T09:47:51+00:00</published><updated>2025-01-25T09:47:51+00:00</updated><id>https://byroot.github.io/ruby/performance/2025/01/25/why-does-everyone-hate-fork</id><content type="html" xml:base="https://byroot.github.io/ruby/performance/2025/01/25/why-does-everyone-hate-fork.html"><![CDATA[<p>I want to write a post about <a href="https://rubygems.org/gems/pitchfork">Pitchfork</a>, explaining where it comes from, why it
is like it is, and how I see its future.
But before I can get to that, I think I need to explain a few things, namely why in many circles <code class="language-plaintext highlighter-rouge">fork</code> is seen as a
relic of the past, if not outright the devil’s creation.
And yet, it’s ubiquitous in the Ruby ecosystem.</p>

<p>Note that if you have some system programming experience, you probably won’t learn much here.</p>

<p>If you’ve ever deployed a Ruby application to production, it is almost certain you’ve interacted with
<a href="https://man7.org/linux/man-pages/man2/fork.2.html"><code class="language-plaintext highlighter-rouge">fork(2)</code></a> whether you realize it or not.
Have you configured Puma’s <code class="language-plaintext highlighter-rouge">worker</code> setting? Well, Puma uses <code class="language-plaintext highlighter-rouge">fork(2)</code> to spawn these workers, more accurately the Ruby
<a href="https://docs.ruby-lang.org/en/3.4/Process.html#method-c-fork"><code class="language-plaintext highlighter-rouge">Process.fork</code></a> method, which is the Ruby API for
the underlying <code class="language-plaintext highlighter-rouge">fork(2)</code> syscall.</p>

<p>And even if you’re not a Rubyist, if you’ve used PHP, Nginx, Apache HTTPd, Redis, and many others you’ve used a system
that is heavily relient on <code class="language-plaintext highlighter-rouge">fork(2)</code>, if not entirely architectured around it.</p>

<p>Yet, <a href="https://www.microsoft.com/en-us/research/uploads/prod/2019/04/fork-hotos19.pdf">many people would argue that <code class="language-plaintext highlighter-rouge">fork(2)</code> is evil and shouldn’t be used</a>.
Personally I kinda both agree and disagree with that point of view, and I’ll try to explain why.</p>

<h2 id="a-bit-of-history">A Bit Of History</h2>

<p>According to Wikipedia, the first occurrence of the fork concept dates all the way back to 1962 by the same guy who
coined <a href="https://en.wikipedia.org/wiki/Conway%27s_law">Conway’s law</a>, and was later introduced in the first versions of UNIX.</p>

<p>Initially, it was meant as a primitive to create a new process. You’d call <code class="language-plaintext highlighter-rouge">fork(2)</code> to make a copy of the current process
and from there would mutate that new process into what you want it to be, quickly ending up with an <code class="language-plaintext highlighter-rouge">exec(2)</code>.
You can still do this today in Ruby:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">child_pid</span> <span class="o">=</span> <span class="no">Process</span><span class="p">.</span><span class="nf">fork</span><span class="p">)</span>
  <span class="c1"># We're in the parent process, and we know the child process ID.</span>
  <span class="c1"># We can wait for the child to exit or send signals etc.</span>
  <span class="no">Process</span><span class="p">.</span><span class="nf">wait</span><span class="p">(</span><span class="n">child_pid</span><span class="p">)</span>
<span class="k">else</span>
  <span class="c1"># We're in the child process.</span>
  <span class="c1"># We can change the current user and other attributes.</span>
  <span class="no">Process</span><span class="p">.</span><span class="nf">uid</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="c1"># And then we can replace the current program with another.</span>
  <span class="no">Process</span><span class="p">.</span><span class="nf">exec</span><span class="p">(</span><span class="s2">"echo"</span><span class="p">,</span> <span class="s2">"hello"</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div></div>

<p>In a way that design was quite elegant. You have a handful of simple primitives you can compose together to get
exactly the behavior you need, instead of one huge function that takes a myriad of arguments.</p>

<p>But it is also very inefficient, as entirely duplicating a process to create a new one is generally overkill.
In the example above, if you imagine that our parent program has gigabytes of addressable memory, it’s a huge waste
to copy all of that just to throw it all out almost immediately to replace it with an extremely small program like <code class="language-plaintext highlighter-rouge">/bin/echo</code>.</p>

<p>Of course, modern operating systems don’t actually copy all that, and instead use <a href="https://en.wikipedia.org/wiki/Copy-on-write#In_virtual_memory_management">Copy-on-Write</a>,
but that’s still very costly, and can easily take hundreds of milliseconds if the parent process is big.</p>

<p>That’s why this historical usage of <code class="language-plaintext highlighter-rouge">fork(2)</code> to spawn other programs is mostly considered deprecated today, and most
newer software will use more modern APIs such as <code class="language-plaintext highlighter-rouge">posix_spawn(3)</code> or <code class="language-plaintext highlighter-rouge">vfork(2)+exec(2)</code>.</p>

<p>But that’s not the only use of <code class="language-plaintext highlighter-rouge">fork(2)</code>. I have no idea if this was envisioned right from the start, or if it just became
a thing, but all the software I listed in the introduction uses <code class="language-plaintext highlighter-rouge">fork(2)</code> without ever following it with an <code class="language-plaintext highlighter-rouge">exec(2)</code> call.</p>

<h2 id="fork-as-a-parallelism-primitive">Fork as a Parallelism Primitive</h2>

<p>Again, I wasn’t even born in the early seventies, so I’m not too sure when this practice really started but at some
point <code class="language-plaintext highlighter-rouge">fork(2)</code> started being used as a parallelism primitive, particularly for servers.</p>

<p>Let’s say you want to implement a simple “echo” server from scratch, in Ruby it might look like this:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">require</span> <span class="s1">'socket'</span>

<span class="n">server</span> <span class="o">=</span> <span class="no">TCPServer</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span><span class="s1">'localhost'</span><span class="p">,</span> <span class="mi">8000</span><span class="p">)</span>

<span class="k">while</span> <span class="n">socket</span> <span class="o">=</span> <span class="n">server</span><span class="p">.</span><span class="nf">accept</span>
  <span class="k">while</span> <span class="n">line</span> <span class="o">=</span> <span class="n">socket</span><span class="p">.</span><span class="nf">gets</span>
    <span class="n">socket</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
  <span class="k">end</span>
  <span class="n">socket</span><span class="p">.</span><span class="nf">close</span>
<span class="k">end</span>
</code></pre></div></div>

<p>This script first opens a listening socket on port <code class="language-plaintext highlighter-rouge">8000</code>, then blocks on the <code class="language-plaintext highlighter-rouge">accept(2)</code> syscall to wait for a client
to connect. When that method returns, it gives us a bidirectional socket, from which we can read, in this case with <code class="language-plaintext highlighter-rouge">#gets</code>,
and also write back to the client.</p>

<p>While this is using modern Ruby, that’s very similar to how various servers would be written back then, but overly simplified.</p>

<p>If you want to play with it, you can use <code class="language-plaintext highlighter-rouge">telnet localhost 8000</code> and start writing things.</p>

<p>But there’s one big issue with that server: it only supports a single concurrent user.
If you try to have two <code class="language-plaintext highlighter-rouge">telnet</code> sessions active, you’ll see the second one can’t connect.</p>

<p>So what people started doing, was to leverage <code class="language-plaintext highlighter-rouge">fork(2)</code> to be able to support more users:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">require</span> <span class="s1">'socket'</span>

<span class="n">server</span> <span class="o">=</span> <span class="no">TCPServer</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span><span class="s1">'localhost'</span><span class="p">,</span> <span class="mi">8000</span><span class="p">)</span>
<span class="n">children</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">while</span> <span class="n">socket</span> <span class="o">=</span> <span class="n">server</span><span class="p">.</span><span class="nf">accept</span>
  <span class="c1"># prune exited children</span>
  <span class="n">children</span><span class="p">.</span><span class="nf">reject!</span> <span class="p">{</span> <span class="o">|</span><span class="n">pid</span><span class="o">|</span> <span class="no">Process</span><span class="p">.</span><span class="nf">wait</span><span class="p">(</span><span class="n">pid</span><span class="p">,</span> <span class="no">Process</span><span class="o">::</span><span class="no">WNOHANG</span><span class="p">)}</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">child_pid</span> <span class="o">=</span> <span class="no">Process</span><span class="p">.</span><span class="nf">fork</span><span class="p">)</span>
    <span class="n">children</span> <span class="o">&lt;&lt;</span> <span class="n">child_pid</span>
    <span class="n">socket</span><span class="p">.</span><span class="nf">close</span>
  <span class="k">else</span>
    <span class="k">while</span> <span class="n">line</span> <span class="o">=</span> <span class="n">socket</span><span class="p">.</span><span class="nf">gets</span>
      <span class="n">socket</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
    <span class="k">end</span>
    <span class="n">socket</span><span class="p">.</span><span class="nf">close</span>
    <span class="no">Process</span><span class="p">.</span><span class="nf">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p>The logic is the same as before, but now once <code class="language-plaintext highlighter-rouge">accept(2)</code> returns us a socket, instead of blocking on it,
we <code class="language-plaintext highlighter-rouge">fork(2)</code> a new child process, and let that child do the blocking operations until the client closes the connection.</p>

<p>If you are an astute reader (or simply already knowledgeable about <code class="language-plaintext highlighter-rouge">fork(2)</code> semantics), you may have noticed that after
the call to <code class="language-plaintext highlighter-rouge">fork</code>, both the parent and the new children have access to the socket. That is because, in UNIX, sockets are
“files”, hence represented by a “file descriptor”, and part of the <code class="language-plaintext highlighter-rouge">fork(2)</code> semantic is that all file descriptors are
also inherited.</p>

<p>That’s why it is important that the parent close the socket, otherwise, it will stay open forever in the parent process<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>,
And this is one of the first reasons why many people hate <code class="language-plaintext highlighter-rouge">fork(2)</code>.</p>

<h2 id="a-double-edged-sword">A Double-Edged Sword</h2>

<p>As showcased above, the fact that child processes inherit all open file descriptors allows to implement some very useful things,
but it can also cause catastrophic bugs if you forget to close a file descriptor you didn’t mean to share.</p>

<p>For instance, if you are forking a process that has an active connection to a SQL database, and you keep using that
connection in both processes, weird things will happen:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">require</span> <span class="s2">"bundler/inline"</span>
<span class="n">gemfile</span> <span class="k">do</span>
  <span class="n">gem</span> <span class="s2">"trilogy"</span>
  <span class="n">gem</span> <span class="s2">"bigdecimal"</span> <span class="c1"># for trilogy</span>
<span class="k">end</span>

<span class="n">client</span> <span class="o">=</span> <span class="no">Trilogy</span><span class="p">.</span><span class="nf">new</span>
<span class="n">client</span><span class="p">.</span><span class="nf">ping</span>

<span class="k">if</span> <span class="n">child_pid</span> <span class="o">=</span> <span class="no">Process</span><span class="p">.</span><span class="nf">fork</span>
  <span class="nb">sleep</span> <span class="mf">0.1</span> <span class="c1"># Give some time to the child</span>

  <span class="mi">5</span><span class="p">.</span><span class="nf">times</span> <span class="k">do</span> <span class="o">|</span><span class="n">i</span><span class="o">|</span>
    <span class="nb">p</span> <span class="n">client</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="s2">"SELECT </span><span class="si">#{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span><span class="p">).</span><span class="nf">first</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">end</span>
  <span class="no">Process</span><span class="p">.</span><span class="nf">kill</span><span class="p">(</span><span class="ss">:KILL</span><span class="p">,</span> <span class="n">child_pid</span><span class="p">)</span>
  <span class="no">Process</span><span class="p">.</span><span class="nf">wait</span><span class="p">(</span><span class="n">child_pid</span><span class="p">)</span>
<span class="k">else</span>
  <span class="kp">loop</span> <span class="k">do</span>
    <span class="n">client</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="s1">'SELECT "oops"'</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Here the script establishes a connection to MySQL, using the <code class="language-plaintext highlighter-rouge">trilogy</code> client, then forks a child
that queries <code class="language-plaintext highlighter-rouge">SELECT "oops"</code> indefinitely in a loop. Once the child is spawned, the parent issues 5 queries,
each one supposed to return a single number from 0 to 4, and print their result.</p>

<p>If you run this script, you’ll get a somewhat random output, similar to this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"oops"
1
"oops"
"oops"
3
</code></pre></div></div>

<p>What’s happening here is that both processes are writing inside the same socket. For the MySQL server, it’s not a big
deal because our queries are small, so they’re somewhat “atomically” written into the socket if we were to issue larger
queries, two queries might end up interleaved, which would cause the server to close the connection with some form of
protocol error.</p>

<p>But for the client, it’s really bad. Because the responses of both processes are sent back in the same socket, and
each client is issuing <code class="language-plaintext highlighter-rouge">read(2)</code> and might be getting the response to the query it just issued, but the response of
another unrelated query issued by the other process.</p>

<p>When two processes try to <code class="language-plaintext highlighter-rouge">read(2)</code> on the same socket, they each get part of the data, but you don’t have proper control
over which process gets what, and it’s unrealistic to try to synchronize the two processes so they each get the response
they expect.</p>

<p>With this in mind, you can imagine how much of a hassle it can be to properly close all the sockets and other open files
of an application before you call <code class="language-plaintext highlighter-rouge">fork(2)</code>. Perhaps you can be diligent in your own code, but you likely are using some
libraries that may not expect <code class="language-plaintext highlighter-rouge">fork(2)</code> to be called and don’t allow you to close their file descriptors.</p>

<p>For the <code class="language-plaintext highlighter-rouge">fork+exec</code> use case, there’s a nice feature that makes this much easier, you can mark a file descriptor as needing
to be closed when <code class="language-plaintext highlighter-rouge">exec</code> is called, and the operating system takes care of that for you, <code class="language-plaintext highlighter-rouge">O_CLOEXEC</code> (for close on exec),
which in Ruby is conveniently exposed as a method on the <code class="language-plaintext highlighter-rouge">IO</code> class:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="no">STDIN</span><span class="p">.</span><span class="nf">close_on_exec</span> <span class="o">=</span> <span class="kp">true</span>
</code></pre></div></div>

<p>But there’s no such flag for the <code class="language-plaintext highlighter-rouge">fork</code> system call when it’s not followed by an <code class="language-plaintext highlighter-rouge">exec</code>. Or more accurately
there is one, <code class="language-plaintext highlighter-rouge">O_CLOFORK</code>, which has existed on a few UNIX systems, mostly IBM ones, and <a href="https://austingroupbugs.net/view.php?id=1318">was added to the POSIX spec in 2020</a>.
But it isn’t widely supported today, most importantly Linux doesn’t support it.
<a href="https://lore.kernel.org/all/1304749754.2821.712.camel@edumazet-laptop/T/#m2a7dbb0f0f6106b3d9ecc8c485a683ea6b2e02ee">Someone submitted a patch to add it to Linux in 2011</a>,
but it seems there wasn’t much appetite for it, and <a href="https://lore.kernel.org/lkml/20200525081626.GA16796@amd/T/#m4ef81228aba3f9524329f83a124d0322ed53f834">someone else made another attempt in 2020</a>,
but it encountered some strong opposition, which is a shame, because it would be tremendously useful.</p>

<p>Instead, what most code that wants to be fork-safe does, it either trying to detect a fork happened by continuously checking
the current process ID:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">query</span>
  <span class="k">if</span> <span class="no">Process</span><span class="p">.</span><span class="nf">pid</span> <span class="o">!=</span> <span class="vi">@old_pid</span>
    <span class="vi">@connection</span><span class="p">.</span><span class="nf">close</span>
    <span class="vi">@connection</span> <span class="o">=</span> <span class="kp">nil</span>
    <span class="vi">@old_pid</span> <span class="o">=</span> <span class="no">Process</span><span class="p">.</span><span class="nf">pid</span>
  <span class="k">end</span>

  <span class="vi">@connection</span> <span class="o">||=</span> <span class="n">connect</span>
  <span class="vi">@connection</span><span class="p">.</span><span class="nf">query</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Or alternatively rely on some <code class="language-plaintext highlighter-rouge">at_fork</code> callback, in C land usually it is <a href="https://man7.org/linux/man-pages/man3/pthread_atfork.3.html"><code class="language-plaintext highlighter-rouge">pthread_atfork</code></a>,
and <a href="https://bugs.ruby-lang.org/issues/17795">since Ruby since 3.1, you can decorate <code class="language-plaintext highlighter-rouge">Process._fork</code></a> (note the <code class="language-plaintext highlighter-rouge">_</code>):</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">module</span> <span class="nn">MyLibraryAtFork</span>
  <span class="k">def</span> <span class="nf">_fork</span>
    <span class="n">pid</span> <span class="o">=</span> <span class="k">super</span>
    <span class="k">if</span> <span class="n">pid</span> <span class="o">==</span> <span class="mi">0</span>
      <span class="c1"># in child</span>
    <span class="k">else</span>
      <span class="c1"># in parent</span>
      <span class="no">MyLibrary</span><span class="p">.</span><span class="nf">close_all_ios</span>
    <span class="k">end</span>
    <span class="n">pid</span>
  <span class="k">end</span>
<span class="k">end</span>
<span class="no">Process</span><span class="p">.</span><span class="nf">singleton_class</span><span class="p">.</span><span class="nf">prepend</span><span class="p">(</span><span class="no">MyLibraryAtFork</span><span class="p">)</span>
</code></pre></div></div>

<p>Since <code class="language-plaintext highlighter-rouge">fork(2)</code> is quite ubiquitous in Ruby, many popular libraries that deal with sockets, such as Active Record, or
the <code class="language-plaintext highlighter-rouge">redis</code> gem, do their best to take care of this transparently, so you don’t have to think about it.
Hence in most Ruby programs it just works.</p>

<p>But with native languages, it can be quite tedious and that’s one of the reasons why many people absolutely hate <code class="language-plaintext highlighter-rouge">fork(2)</code>.
Any code that makes use of files or sockets might be utterly broken after <code class="language-plaintext highlighter-rouge">fork(2)</code> has been called, unless special attention
was paid to fork safety, which is rarely the case.</p>

<h2 id="some-of-your-threads-may-die">Some of Your Threads May Die</h2>

<p>Going back to our small echo server, you may wonder why one would use <code class="language-plaintext highlighter-rouge">fork(2)</code> instead of a thread here.
Again, I wasn’t there at the time, but my understanding is that threads became a thing much later (late eighties?),
and even once they existed, they took quite a while to be standardized and ironed out, hence usable across platforms.</p>

<p>There is also probably an argument that multi-processing with <code class="language-plaintext highlighter-rouge">fork(2)</code> is easier to reason about. Each process has its
own memory space, so you don’t have to concern yourself as much with race conditions and other thread pitfalls, so I can
see why even when threads became an option, some may have preferred to stick with <code class="language-plaintext highlighter-rouge">fork(2)</code>.</p>

<p>But since threads became a thing long after <code class="language-plaintext highlighter-rouge">fork(2)</code>, it seems that the people in charge of implementing and
standardizing them ran into a bit of a pickle, and didn’t find a way to make them both play well together.</p>

<p>Here’s what <a href="https://pubs.opengroup.org/onlinepubs/009696799/functions/fork.html">the POSIX standard fork entry</a> says about that:</p>

<blockquote>
  <p>A process shall be created with a single thread.
If a multi-threaded process calls fork(), the new process shall contain a replica of the calling thread and its entire
address space, possibly including the states of mutexes and other resources.
Consequently, to avoid errors, the child process may only execute async-signal-safe operations until such time as one
of the exec functions is called.</p>
</blockquote>

<p>In other words, the standard acknowledges that the classic <code class="language-plaintext highlighter-rouge">fork+exec</code> dance can be done from a multi-threaded process,
but kind of wash its hands about the use of <code class="language-plaintext highlighter-rouge">fork</code> not followed by <code class="language-plaintext highlighter-rouge">exec</code>. They recommend only using async-signal-safe
operations, which is really just a very small subset of things. So really, according to the standard, if you call
<code class="language-plaintext highlighter-rouge">fork(2)</code> after some threads have been spawned, without the intention to call <code class="language-plaintext highlighter-rouge">exec</code> quickly, then here be dragons.</p>

<p>The reason is that only the thread which called <code class="language-plaintext highlighter-rouge">fork(2)</code> remains alive in the children, all the other threads
are present but dead. If another thread had locked a mutex or something like that, it would stay locked forever,
which might lead to a deadlock if a new thread tries to acquire it.</p>

<p>The standard also includes a rationale section about why it is this way, which is a bit long but interesting:</p>

<blockquote>
  <p>The general problem with making fork() work in a multi-threaded world is what to do with all of the threads.
There are two alternatives. 
One is to copy all of the threads into the new process.
This causes the programmer or implementation to deal with threads that are suspended on system calls or that might be
about to execute system calls that should not be executed in the new process.
The other alternative is to copy only the thread that calls fork().
This creates the difficulty that the state of process-local resources is usually held in process memory.
If a thread that is not calling fork() holds a resource, that resource is never released in the child process because
the thread whose job it is to release the resource does not exist in the child process.</p>

  <p>When a programmer is writing a multi-threaded program, […]
<strong>The fork() function is thus used only to run new programs</strong>, and the effects of calling functions that require certain
resources between the call to fork() and the call to an exec function are undefined.</p>

  <p>The addition of the forkall() function to the standard was considered and rejected.</p>
</blockquote>

<p>So they did consider the possibility of having another version of <code class="language-plaintext highlighter-rouge">fork(2)</code>, called <code class="language-plaintext highlighter-rouge">forkall()</code> which would also have
copied other threads, but they couldn’t come up with a clear semantic on what happens in some cases.</p>

<p>Instead, they gave users a way to have a callback invoked around <code class="language-plaintext highlighter-rouge">fork</code> to restore state, for instance, re-initialize mutexes.
However, if you go look at <a href="https://man7.org/linux/man-pages/man3/pthread_atfork.3.html">that callback man page <code class="language-plaintext highlighter-rouge">pthread_atfork(3)</code></a>,
you can read:</p>

<blockquote>
  <p>The original intention of pthread_atfork() was to allow the child process to be returned to a consistent state. […]
In practice, this task is generally too difficult to be practicable.</p>
</blockquote>

<p>So while <code class="language-plaintext highlighter-rouge">pthread_atfork</code> is still there and you can use it, the standard acknowledges that it is very hard to use correctly.</p>

<p>That’s why many system programmers will tell you to never mix <code class="language-plaintext highlighter-rouge">fork(2)</code> with multi-threaded programs, or at least never
to call <code class="language-plaintext highlighter-rouge">fork(2)</code> ever after a thread was spawned, because then, all bets are off. Hence, you somewhat had to choose your
camp, and it seems threads clearly won.</p>

<p>But that’s for C or C++ programmers.</p>

<p>In the case of today’s Ruby programmers, however, the reason to use <code class="language-plaintext highlighter-rouge">fork(2)</code> over threads, is that it’s the only way
to get true parallelism <sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> on MRI, the default and most commonly used implementation of Ruby.
Because of the infamous GVL, Ruby threads only really allow to parallelize IO operations,
and can’t parallelize Ruby code execution, hence pretty much all Ruby application servers integrate with <code class="language-plaintext highlighter-rouge">fork(2)</code> in
some way so they can exploit more than a single CPU core.</p>

<p>Luckily, some of the pitfalls of mixing threads with <code class="language-plaintext highlighter-rouge">fork(2)</code> are alleviated by Ruby.
For instance, Ruby mutexes are automatically released when their owner dies, due to how they are implemented.
In pseudo Ruby code they’d look like this:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Mutex</span>
  <span class="k">def</span> <span class="nf">lock</span>
    <span class="k">if</span> <span class="vi">@owner</span> <span class="o">==</span> <span class="no">Fiber</span><span class="p">.</span><span class="nf">current</span>
      <span class="k">raise</span> <span class="no">ThreadError</span><span class="p">,</span> <span class="s2">"deadlock; recursive locking"</span>
    <span class="k">end</span>

    <span class="k">while</span> <span class="vi">@owner</span><span class="o">&amp;</span><span class="p">.</span><span class="nf">alive?</span>
      <span class="nb">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">end</span>

    <span class="vi">@owner</span> <span class="o">=</span> <span class="no">Fiber</span><span class="p">.</span><span class="nf">current</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Of course in reality they’re not sleeping in a loop to wait, they use a much more efficient way to block, but it’s to
give you the general idea.
The important point is that Ruby mutexes keep a reference to the fiber (hence thread) that acquired the lock,
and automatically ignore it if it’s dead.
Hence upon fork, all mutexes held by the background thread are immediately released, which avoids most
deadlock scenarios.</p>

<p>It’s not perfect of course, if a thread died while holding a mutex, it’s very possible that it left the resource that was protected
by the mutex in an inconsistent state, in practice however I’ve never experienced something like that, granted it’s likely
because the existence of the GVL somewhat reduces the need for mutexes.</p>

<p>Now, Ruby threads aren’t fully exempt from these pitfalls, because ultimately on MRI, Ruby threads are backed by native threads,
so you can end up with a nasty deadlock after forking if another thread released the GVL and called a C API that locks a mutex.</p>

<p>While I never got hard proof of it, I suspect this was happening to some Ruby users because from my understanding,
glibc’s <a href="https://man7.org/linux/man-pages/man3/getaddrinfo.3.html"><code class="language-plaintext highlighter-rouge">getaddrinfo(3)</code></a>,
which Ruby uses to resolve host names, does use a global mutex, and Ruby calls it with the GVL released, allowing for a
fork to happen concurrently.</p>

<p>To prevent this, <a href="https://bugs.ruby-lang.org/issues/20590">I added another lock inside MRI</a>, to prevent <code class="language-plaintext highlighter-rouge">Process.fork</code>
from happening while a <code class="language-plaintext highlighter-rouge">getaddrinfo(3)</code> call is ongoing.
This is far from perfect, but given how much Ruby relies on <code class="language-plaintext highlighter-rouge">Process.fork</code>, that seemed like a sensible thing to do.</p>

<p>It’s also not rare for Ruby programs that rely on fork to run into crashes on macOS, because numerous macOS system APIs
do implicitly spawn threads or lock mutexes, and <a href="https://github.com/rails/rails/issues/38560">macOS chose to consistenty crash when it happens</a>.</p>

<p>So even with pure Ruby code, you occasionally run into <code class="language-plaintext highlighter-rouge">fork(2)</code>’s pitfalls, you can’t just use it willy-nilly.</p>

<h2 id="conclusion">Conclusion</h2>

<p>So to answer the question in the title, the reason <code class="language-plaintext highlighter-rouge">fork(2)</code> is hated is because it doesn’t compose well, particularly
in native code.
If you wish to use it, you have to be extremely careful about the code you are writing and linking to.
Whenever you use a library you have to make sure it won’t spawn some threads, or hold onto file descriptors,
and given the choice between <code class="language-plaintext highlighter-rouge">fork(2)</code> and threads, most systems programmers will choose threads. They have their own
pitfalls, but they compose better, and it is likely that you are calling into APIs that are using threads under the
hoods, so the choice is somewhat already made for you.</p>

<p>But the situation isn’t nearly as bad for Ruby code, as it makes it much easier to write fork-safe code, and the Ruby
philosophy makes it so libraries like Active Record take it upon themselves to deal with these gnarly details for you.
So problems mostly come up when you want to bind to some native libraries that spawn threads, like <code class="language-plaintext highlighter-rouge">grpc</code> or <code class="language-plaintext highlighter-rouge">libvips</code>,
as they generally don’t expect <code class="language-plaintext highlighter-rouge">fork(2)</code> to happen and aren’t generally kin in accepting it as a constraint.</p>

<p>Especially since fork is mostly used at the end of the application initialization, even libraries
that are technically not fork-safe, will work because they generally initialize their threads and file descriptors
lazily upon the first request.</p>

<p>Anyway, even if you still think <code class="language-plaintext highlighter-rouge">fork(2)</code> is evil, until Ruby offers another usable primitive for true parallelism
(which should be the subject of the next post), it will remain a necessary evil.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>Technically, Ruby will automatically close it once the object is garbage collected, but you get the idea. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Yes, there are also Ractors to some extent, but that will be the subject of the next post. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="ruby" /><category term="performance" /><summary type="html"><![CDATA[I want to write a post about Pitchfork, explaining where it comes from, why it is like it is, and how I see its future. But before I can get to that, I think I need to explain a few things, namely why in many circles fork is seen as a relic of the past, if not outright the devil’s creation. And yet, it’s ubiquitous in the Ruby ecosystem.]]></summary></entry><entry><title type="html">Instrumenting Thread Stalling in Ruby Applications</title><link href="https://byroot.github.io/ruby/performance/2025/01/23/io-instrumentation.html" rel="alternate" type="text/html" title="Instrumenting Thread Stalling in Ruby Applications" /><published>2025-01-23T13:50:51+00:00</published><updated>2025-01-23T13:50:51+00:00</updated><id>https://byroot.github.io/ruby/performance/2025/01/23/io-instrumentation</id><content type="html" xml:base="https://byroot.github.io/ruby/performance/2025/01/23/io-instrumentation.html"><![CDATA[<p>In <a href="https://byroot.github.io/ruby/performance/2025/01/23/the-mythical-io-bound-rails-app.html">my previous post about how IO-bound Rails applications really are</a>,
I pointed at a common pitfall, how CPU starvation can look like slow IOs.</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">start</span> <span class="o">=</span> <span class="no">Time</span><span class="p">.</span><span class="nf">now</span>
<span class="n">database_connection</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="s2">"SELECT ..."</span><span class="p">)</span>
<span class="n">query_duration</span> <span class="o">=</span> <span class="p">(</span><span class="no">Time</span><span class="p">.</span><span class="nf">now</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1000.0</span>
<span class="nb">puts</span> <span class="s2">"Query took: </span><span class="si">#{</span><span class="n">query_duration</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">ms"</span>
</code></pre></div></div>

<p>In the above example, the instrumentation tells you how long it took for the database to answer your query, but may
also include the time needed for the Ruby thread to re-acquire the GVL, or perhaps the Ruby GC to run, or even the
operating system’s scheduler to resume the process.</p>

<p>Thankfully, in recent years Ruby added some new APIs that help measure these things.</p>

<h2 id="gctotal_time">GC.total_time</h2>

<p>Database queries and other IOs can often result in lots of allocations. For instance, if you select 100 database rows,
with a dozen columns each, you can probably expect a thousand or more allocations and any of these might trigger a GC
run inside the code block you are timing.</p>

<p>As such it can be a good idea to keep an eye on how much time Ruby is spending in GC. To help with that, <a href="https://bugs.ruby-lang.org/issues/10917#note-4">back in 2021
I resurrected an old feature request on the Ruby bug tracker</a> and
convinced <a href="https://github.com/ruby/ruby/pull/4757">Koichi Sadasa to implement the new <code class="language-plaintext highlighter-rouge">GC.total_time</code> API</a>.</p>

<p>This accessor is a monotonic counter, that represents the number of nanoseconds spent in the GC. So to tell how much
time a particular block of code spent in GC, you can do a simple subtraction:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">time_in_gc</span>
  <span class="n">before</span> <span class="o">=</span> <span class="no">GC</span><span class="p">.</span><span class="nf">total_time</span>
  <span class="k">yield</span>
  <span class="n">diff_ms</span> <span class="o">=</span> <span class="p">(</span><span class="no">GC</span><span class="p">.</span><span class="nf">total_time</span> <span class="o">-</span> <span class="n">before</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1_000_000.0</span>

  <span class="nb">puts</span> <span class="s2">"gc_time: </span><span class="si">#{</span><span class="n">diff_ms</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">ms"</span>
<span class="k">end</span>

<span class="n">time_in_gc</span> <span class="k">do</span>
  <span class="mi">2_000_000</span><span class="p">.</span><span class="nf">times</span> <span class="p">{</span> <span class="no">Object</span><span class="p">.</span><span class="nf">new</span> <span class="p">}</span>
<span class="k">end</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gc_time: 24.18ms
</code></pre></div></div>

<p>Now of course, if you are running a multi-threaded application, you can’t just subtract the time spent in GC from the
measured IO time, because another thread might be responsible for it. But it’s still a good idea to instrument it
and display it next to the IO duration.</p>

<p>That’s why starting from Rails 7.2, <a href="https://github.com/rails/rails/pull/51770">I added this measurement into Rails instrumentation API</a>.
Every <code class="language-plaintext highlighter-rouge">ActiveSupport::Notifications</code> event now has an associated <code class="language-plaintext highlighter-rouge">gc_time</code>, and Rails request logs include the overall time spent in GC.</p>

<h2 id="gvl-instrumentation-api">GVL Instrumentation API</h2>

<p>Even more common than GC, is GVL contention. If you configured your application to use too many threads, it can cause
long delays for a thread to resume after finishing some IOs.</p>

<p>That’s why <a href="https://bugs.ruby-lang.org/issues/18339">in Ruby 3.2 I added a new C API to allow instrumenting the GVL</a>.</p>

<p>This is quite a low-level API, and you need a C extension to integrate with it, but I wrote
<a href="https://github.com/Shopify/gvltools"><code class="language-plaintext highlighter-rouge">gvltools</code></a> for that, and John Hawthorn wrote the
<a href="https://github.com/jhawthorn/gvl_timing"><code class="language-plaintext highlighter-rouge">gvl_timing</code> gem</a>, and there’s
also <a href="https://github.com/ivoanjo/gvl-tracing"><code class="language-plaintext highlighter-rouge">gvl-tracing</code></a> from Ivo Anjo.</p>

<p>Here’s how <code class="language-plaintext highlighter-rouge">gvltools</code> can be used to distinguish actual IO time from GVL wait time:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">require</span> <span class="s2">"bundler/inline"</span>

<span class="n">gemfile</span> <span class="k">do</span>
  <span class="n">gem</span> <span class="s2">"bigdecimal"</span> <span class="c1"># for trilogy</span>
  <span class="n">gem</span> <span class="s2">"trilogy"</span>
  <span class="n">gem</span> <span class="s2">"gvltools"</span>
<span class="k">end</span>

<span class="no">GVLTools</span><span class="o">::</span><span class="no">LocalTimer</span><span class="p">.</span><span class="nf">enable</span>

<span class="k">def</span> <span class="nf">measure_time</span>
  <span class="n">realtime_start</span> <span class="o">=</span> <span class="no">Process</span><span class="p">.</span><span class="nf">clock_gettime</span><span class="p">(</span><span class="no">Process</span><span class="o">::</span><span class="no">CLOCK_MONOTONIC</span><span class="p">,</span> <span class="ss">:float_millisecond</span><span class="p">)</span>
  <span class="n">gvl_time_start</span> <span class="o">=</span> <span class="no">GVLTools</span><span class="o">::</span><span class="no">LocalTimer</span><span class="p">.</span><span class="nf">monotonic_time</span>
  <span class="k">yield</span>

  <span class="n">realtime</span> <span class="o">=</span> <span class="no">Process</span><span class="p">.</span><span class="nf">clock_gettime</span><span class="p">(</span><span class="no">Process</span><span class="o">::</span><span class="no">CLOCK_MONOTONIC</span><span class="p">,</span> <span class="ss">:float_millisecond</span><span class="p">)</span> <span class="o">-</span> <span class="n">realtime_start</span>
  <span class="n">gvl_time</span> <span class="o">=</span> <span class="no">GVLTools</span><span class="o">::</span><span class="no">LocalTimer</span><span class="p">.</span><span class="nf">monotonic_time</span> <span class="o">-</span> <span class="n">gvl_time_start</span>
  <span class="n">gvl_time_ms</span> <span class="o">=</span> <span class="n">gvl_time</span> <span class="o">/</span> <span class="mf">1_000_000.0</span>
  <span class="n">io_time</span> <span class="o">=</span> <span class="n">realtime</span> <span class="o">-</span> <span class="n">gvl_time_ms</span>
  <span class="nb">puts</span> <span class="s2">"io: </span><span class="si">#{</span><span class="n">io_time</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">ms, gvl_wait: </span><span class="si">#{</span><span class="n">gvl_time_ms</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">ms"</span>
<span class="k">end</span>

<span class="n">trilogy</span> <span class="o">=</span> <span class="no">Trilogy</span><span class="p">.</span><span class="nf">new</span>

<span class="c1"># Measure a first time with just the main thread</span>
<span class="n">measure_time</span> <span class="k">do</span>
  <span class="n">trilogy</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="s2">"SELECT 1"</span><span class="p">)</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">fibonacci</span><span class="p">(</span> <span class="n">n</span> <span class="p">)</span>
  <span class="k">return</span>  <span class="n">n</span>  <span class="k">if</span> <span class="p">(</span> <span class="mi">0</span><span class="o">..</span><span class="mi">1</span> <span class="p">).</span><span class="nf">include?</span> <span class="n">n</span>
  <span class="p">(</span> <span class="n">fibonacci</span><span class="p">(</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">)</span> <span class="o">+</span> <span class="n">fibonacci</span><span class="p">(</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">2</span> <span class="p">)</span> <span class="p">)</span>
<span class="k">end</span>

<span class="c1"># Spawn 5 CPU-heavy threads</span>
<span class="n">threads</span> <span class="o">=</span> <span class="mi">5</span><span class="p">.</span><span class="nf">times</span><span class="p">.</span><span class="nf">map</span> <span class="k">do</span>
  <span class="no">Thread</span><span class="p">.</span><span class="nf">new</span> <span class="k">do</span>
    <span class="kp">loop</span> <span class="k">do</span>
      <span class="n">fibonacci</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
    <span class="k">end</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="c1"># Measure again with the background threads</span>
<span class="n">measure_time</span> <span class="k">do</span>
  <span class="n">trilogy</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="s2">"SELECT 1"</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div></div>

<p>If you run this example, you can see that on the first measurement, the GVL wait time is pretty much zero,
but on the second, it adds a massive half-a-second overhead:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>realtime: 0.22ms, gvl_wait: 0.0ms, io: 0.2ms
realtime: 549.29ms, gvl_wait: 549.22ms, io: 0.1ms
</code></pre></div></div>

<p>The downside of this API however, is that it adds some overhead to Ruby’s thread scheduler. I never really managed to
come up with a precise figure of how much overhead, perhaps it’s negligible, but until then, it’s a bit hard to justify
integrating it as a Rails default.</p>

<p>That being said, recently <a href="https://github.com/speedshop/gvl_metrics_middleware/tree/main">Yuki Nishijima from Speedshop open-sourced a middleware</a>
to hook this new instrumentation API into various APM services, so it might progressively see broader usage.</p>

<h2 id="operating-system-scheduler">Operating System Scheduler</h2>

<p>The one remaining thing that could cause IO operations to appear longer than they really are is the operating scheduler.
Unless you are running your application on dedicated hardware, and spawn no more than one Ruby process per core, then
it can happen that the operating system doesn’t immediately resume a process after it is done blocking on some IO.</p>

<p>I’m unfortunately not aware of a really good way to measure this.</p>

<p>The best I’ve found is <code class="language-plaintext highlighter-rouge">/proc/&lt;pid&gt;/schedstat</code> on Linux:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># cat /proc/1/schedstat</span>
40933713 1717706 178
</code></pre></div></div>

<p>The second number in that list is the amount of nanosecond the given process spent in the “runqueue”, in other words,
waiting to be assigned a CPU core so it can resume work.</p>

<p>But reading <code class="language-plaintext highlighter-rouge">/proc</code> around every IO would be a bit heavy-handed, so it’s not something I’ve ever integrated into an
application monitoring. Instead, we monitor it more globally on a per-machine basis as an indication that we’re running
too many processes in our containers.</p>

<h2 id="conclusion">Conclusion</h2>

<p>So while it’s very hard to measure exactly how long a particular IO operation took, there are a few general metrics
that can help you have a better pitcture of how much “thread stalling” your application is experiencing.</p>

<p>Also, I don’t keep a close eye on the various application performance monitoring services out there, but my hope when
adding these new APIs was that they would eventually integrate it in their offering. Not sure if some did already.</p>]]></content><author><name></name></author><category term="ruby" /><category term="performance" /><summary type="html"><![CDATA[Recent Ruby releases added some nice tools to better instrument applications]]></summary></entry><entry><title type="html">The Mythical IO-Bound Rails App</title><link href="https://byroot.github.io/ruby/performance/2025/01/23/the-mythical-io-bound-rails-app.html" rel="alternate" type="text/html" title="The Mythical IO-Bound Rails App" /><published>2025-01-23T10:17:51+00:00</published><updated>2025-01-23T10:17:51+00:00</updated><id>https://byroot.github.io/ruby/performance/2025/01/23/the-mythical-io-bound-rails-app</id><content type="html" xml:base="https://byroot.github.io/ruby/performance/2025/01/23/the-mythical-io-bound-rails-app.html"><![CDATA[<p>I want to write a post about <a href="https://rubygems.org/gems/pitchfork">Pitchfork</a>, explaining where it comes from, why it
is like it is, and how I see its future.
But before I can get to that, I think I need to explain a few things.</p>

<p>When the topic of Rails performance comes up, it is commonplace to hear that the database is the bottleneck,
so Rails applications are <a href="https://en.wikipedia.org/wiki/I/O_bound">IO-bound</a> anyway, hence Ruby performance doesn’t
matter that much, and all you need is a healthy dose of concurrency to make your service scale.</p>

<p>But how true is this in general?</p>

<h2 id="conflating-scale-and-performance">Conflating scale and performance</h2>

<p>First, it is true that when scaling a Rails application, the first major bottleneck you will encounter will generally
be the database.</p>

<p>Rails, like the overwhelming majority of modern web frameworks, is stateless, hence it is trivial to scale it
horizontally to handle as much load as you want.
As long as you can keep adding server capacity, Rails will keep scaling. It might not be the cheapest stack to scale,
but it will scale as well as any other stateless web framework.
To handle 10x more traffic, you need roughly 10x more server capacity, that’s simple and not what teams who need to
scale a Rails application will struggle with.</p>

<p>Relational databases, however, are much harder to scale. By default, you can’t scale them horizontally unless you
implement data sharding in some way, and depending on your
data model, it can sometimes be very challenging, so generally relational databases are initially scaled vertically,
meaning migrating to increasingly more powerful servers.
Vertical scaling allows you to scale pretty far, much farther than most Rails users will ever need, but the cost
increase won’t be linear, and if you are successful enough, it won’t be viable anymore and you’ll have to shard or use
another type of data store.</p>

<p>That’s what the database being the bottleneck means. Not that querying the database is slow, nor that it is the most significant factor in overall service latency, but that it’s the part of your infrastructure you’ll have to care about
the most when your service witnesses increasingly more usage.</p>

<p>That’s what a lot of public discourse gets wrong, they conflate scale and performance, or more precisely, throughput and latency.</p>

<p>Being able to scale means maintaining some level of service while serving more users with close-to-linear costs.
It doesn’t mean being particularly fast, cheap, or efficient, it simply means being able to grow without hitting a
bottleneck, and without costing you exponentially more money.</p>

<p>As such, the database being the bottleneck is true, but it doesn’t imply that the application is spending the majority
of its time waiting on IO.</p>

<h2 id="most-rails-performance-issues-are-database-issues">Most Rails performance issues are database issues</h2>

<p>Another fact that is often stated to explain how Rails applications are IO-bound,
is that the most common performance issues with Rails applications are missing database indexes, N+1 queries and other
data access issues.</p>

<p>From my personal observations, that definitely rings true, but these are bugs, not a given property of the system.
They’re supposed to be identified and fixed, as such, it doesn’t really make sense to design your infrastructure to
accomodate that characteristic.</p>

<p>When properly indexed, and assuming the database isn’t overloaded, the vast majority of queries, especially the mundane
lookups by primary key, take less than a couple of milliseconds, often just a fraction of a millisecond.
If the application does any substantial amount of transformations on that data to render it in HTML or JSON, it will
without a doubt spend as much or more time executing Ruby code than waiting for IOs.</p>

<h2 id="evidence-yjit-effectiveness">Evidence: YJIT effectiveness</h2>

<p>Now of course, every application is different, and I can only really speak with confidence about the ones I’ve worked on.</p>

<p>However, over the last couple of years, many people reported how YJIT reduced their application latency by 15 to 30%.
Like Discourse seeing a <a href="https://blog.discourse.org/2023/05/running-ruby-3-2s-yjit-in-production-at-discourse/">15.8-19.6% speedup with JIT 3.2</a>,
<a href="https://lobste.rs/s/m9ivcf/we_turned_lobste_rs_into_rails_benchmark#c_fkud5w">Lobsters seeing a 26% speedup</a>,
<a href="https://xcancel.com/dhh/status/1711422643256303867">Basecamp and Hey seeing a 26% speedup</a>
or <a href="https://railsatscale.com/2025-01-10-yjit-3-4-even-faster-and-more-memory-efficient/">Shopify’s Storefront Renderer app seeing a 17% speedup</a>.</p>

<p>If these applications were really spending the overwhelming majority of their time waiting on IO, it would be impossible
for YJIT to perform this well overall.</p>

<p>Even on very JIT-friendly benchmarks with no IO at all, YJIT <em>only</em> speeds up Ruby by 2 or 3x.
On more realistic benchmarks like <code class="language-plaintext highlighter-rouge">lobsters</code>, it’s more around 1.7x.
Based on this, we can assume with fairly good confidence that all these applications are certainly not spending 80%
of their time waiting on IO.</p>

<p>To me, that’s enough to consider that most Rails applications aren’t IO-bound.
Some applications out there most definitely are IO-bound, and I spoke with a few people maintaining such applications.
But just like flying fishes, they exist, yet they don’t constitute the majority of the genus.</p>

<h2 id="cpu-starvation-looks-like-io-to-most-eyes">CPU starvation looks like IO to most eyes</h2>

<p>One thing that can cause people to overestimate how much IO their app is doing is that, in most cases, CPU starvation
will look like Ruby is waiting on IO.</p>

<p>If you look at how the overwhelming majority of IO durations are measured, including in Rails logs and in all the
most popular application performance managers, it’s generally done in a very simple, obvious way:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">start</span> <span class="o">=</span> <span class="no">Time</span><span class="p">.</span><span class="nf">now</span>
<span class="n">database_connection</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="s2">"SELECT ..."</span><span class="p">)</span>
<span class="n">query_duration</span> <span class="o">=</span> <span class="p">(</span><span class="no">Time</span><span class="p">.</span><span class="nf">now</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1000.0</span>
<span class="nb">puts</span> <span class="s2">"Query took: </span><span class="si">#{</span><span class="n">query_duration</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">ms"</span>
</code></pre></div></div>

<p>Logically, if this code logs: <code class="language-plaintext highlighter-rouge">Query took: 20.0ms</code>, you might legitimately think it took 20 milliseconds to perform
the SQL query, but that’s not necessarily true.</p>

<p>It actually means that performing the query <strong>and</strong> getting the thread scheduled again took 20 milliseconds, and you
cannot possibly tell how much each part took individually (Edit: At John Duff’s request, I wrote <a href="/ruby/performance/2025/01/23/io-instrumentation.html">a very quick guide
on how you can tell if your application is experiencing some form of CPU starvation</a>).</p>

<p>So for all you know, the query might have been performed in under a millisecond, and all the remaining time was spent
waiting to acquire the GVL, running GC, or waiting for the operating system scheduler to resume your process.</p>

<p>Knowing which is which is crucial:</p>

<ul>
  <li>If all this time was spent performing the query, it suggests that your application is IO-heavy and that you may be
able to use more concurrency (processes, threads, or fibers) to get some extra throughput.</li>
  <li>If all this time was spent waiting on the scheduler, then you might want to do the absolute opposite and use
less concurrency to reduce latency.</li>
</ul>

<p>This issue can often lead people to believe their application is more IO-heavy than it really is, and it’s
particularly vicious because it’s a self-fulfilling prophecy.
If you assume your application is IO-bound, given that it is commonplace to hear so, you’ll logically run it in
production with a threaded or async server using a decent amount of concurrency,
and production logs will confirm your assumption by showing large amounts of time waiting on IO.</p>

<p>This problem is even more common on shared hosting platforms because they don’t always guarantee that you can use
the “virtual CPU” at 100%. On these platforms, your application has to share the physical CPU cores with other
applications, and depending on what these other applications are doing and how busy they are, you may be able to use
substantially more or substantially less than the one “virtual CPU” you are paying for.</p>

<p>This issue is not unique to Ruby, whenever you have a workload that mixes IO and CPU work, you have to arbitrate
between allowing more concurrency and risk degrading latency, or reducing concurrency to ensure a low latency but
decrease utilization.
The more heterogeneous your workload is, as typical in a monolith, the harder it is to find the best compromise.
That’s one of the benefits of micro-services, getting more homogenous workloads, making it easier to achieve
higher server utilization without impacting latency too much.</p>

<p>However, given that the default implementation of Ruby has a GVL, this problem is even more pronounced.
Instead of all threads on the server having to share all CPU cores, you end up with multiple small buckets of threads
that each have to share one CPU, hence you can end up with threads that can’t be resumed even though there are some
free cores or the server.</p>

<p>The thing to keep in mind is that, as a general rule that doesn’t only apply to Ruby, CPU-bound and IO-bound work
loads don’t mix well, and should ideally be handled by distinct systems.
For small projects, you can likely tolerate the latency impact of collocating all your workloads in a one-size-fits-all
system, but as you scale you will increasingly need to segregate IO-intensive workloads from CPU-intensive ones.</p>

<h2 id="job-queues-are-different">Job Queues Are Different</h2>

<p>One important thing to note is that what I’m saying above is only aimed at the web server part of Rails applications.
Most apps also use a background job runner, Sidekiq being the most popular, and background jobs often take care of
lots of slow IO operations, such as sending e-mails, performing API calls, etc.</p>

<p>As such job runners are generally much more IO intensive, and latency is generally a bit less important for them,
so they usually can get away with higher concurrency than web servers.</p>

<p>But even then, it’s common for users to crank their job runner concurrency too high and cause all IOs to appear much slower.
A good example of that is how <a href="https://github.com/redis/hiredis-rb/issues/74">Sidekiq’s maintainer asked me to implement a way to measure the round trip delay in C
so that it’s not skewed by GVL contention</a>.</p>

<h2 id="why-does-it-matter">Why does it matter?</h2>

<p>At that point, you might wonder why it matters how much time Rails applications spend waiting on IO.</p>

<p>For the average Rails user, it is important to know this, because it is what defines which execution model is best
suited to deploy their application:</p>

<ul>
  <li>If an application is truly IO-bound, as in spending more than 95% of its time waiting for IO, then using an
asynchronous execution model is likely what will get you the best results.</li>
  <li>If an application isn’t fully IO-bound, but still is quite IO-heavy, then using a threaded server, with a
reasonable number of threads per process, is probably what will get you the best tradeoff between latency and
throughput.</li>
  <li>If an application doesn’t spend significantly more than half its time on IO, then it might be preferable to use a
purely process-based solution.</li>
</ul>

<p>What the ratio of IO to CPU is likely to be like in an average Rails app is also what drove <a href="https://github.com/rails/rails/issues/50450">Rails to change the
default generated Puma configuration from 5 threads to only 3</a>.</p>

<p>But also for the Ruby community at large, I think it’s important not to disregard Ruby’s performance under the pretext
that it doesn’t matter since it’s all about the database anyway.
This isn’t to say that Ruby is slow, it is without a doubt more than fast enough for writing web applications that
provide a good user experience.</p>

<p>But it is also possible to write Ruby code in a way that doesn’t perform well at all, be it for the sake of usability
or just because it’s fun to use meta-programing or that no one took the time to use a profiler to see if it is possible
to do the same thing more efficiently.</p>

<p>As someone who spends a considerable amount of time looking at production profiles of Rails applications, I can say
with confidence, there are a number of things in Rails and other commonly used gems that could be significantly faster,
but can’t because their public API prevents any further optimization.</p>

<p>As Ruby developers, we naturally have the tendency to put developer happiness first, which is great, but we should also
make sure not to disregard performance. Usability and performance don’t have to be mutually exclusive.
APIs can be both very convenient and performant, but both have to be considered right from the start for it to happen.
Once a public API has been defined and is widely used, there’s only so much you can do to make it perform faster
unless you are willing to deprecate it in favor of a more performant one, but the community appetite for deprecations
and breaking change is no longer what it used to be.</p>]]></content><author><name></name></author><category term="ruby" /><category term="performance" /><summary type="html"><![CDATA[When the topic of Rails performance comes up, it is commonplace to hear that the database is the bottleneck, so Rails applications are IO-bound anyway, hence Ruby performance doesn't matter that much, and all you need is a healthy dose of concurrency to make your service scale. But how true is this in general?]]></summary></entry><entry><title type="html">Optimizing Ruby’s JSON, Part 7</title><link href="https://byroot.github.io/ruby/json/2025/01/14/optimizing-ruby-json-part-7.html" rel="alternate" type="text/html" title="Optimizing Ruby’s JSON, Part 7" /><published>2025-01-14T21:28:51+00:00</published><updated>2025-01-14T21:28:51+00:00</updated><id>https://byroot.github.io/ruby/json/2025/01/14/optimizing-ruby-json-part-7</id><content type="html" xml:base="https://byroot.github.io/ruby/json/2025/01/14/optimizing-ruby-json-part-7.html"><![CDATA[<p>In <a href="/ruby/json/2025/01/12/optimizing-ruby-json-part-6.html">the previous post</a>, we started covering some parser optimizations.
There’s just a handful more to cover until we reached what’s the state of the currently released version of <code class="language-plaintext highlighter-rouge">ruby/json</code>.</p>

<h2 id="batch-apis">Batch APIs</h2>

<p>But as always, let’s start with a flame graph of <code class="language-plaintext highlighter-rouge">twitter.json</code>, to see what was left to optimize:</p>

<p><img src="/assets/articles/json-7/flamegraph-twitter.png" alt="" /></p>

<p><a href="https://share.firefox.dev/4gRJXjb">Full profile</a>.</p>

<p>Something that was bothering me in that profile, was the whopping <code class="language-plaintext highlighter-rouge">26.6%</code> of time spent in <code class="language-plaintext highlighter-rouge">rb_hash_aset</code>,
which is the C API for <code class="language-plaintext highlighter-rouge">Hash#[]=</code>.</p>

<p>It wasn’t really surprising to me though. I’m sure you’ve heard about some super fast JSON parsers like <code class="language-plaintext highlighter-rouge">simdjson</code>, <code class="language-plaintext highlighter-rouge">rapidJSON</code> etc,
Some of you may have wondered why I didn’t just do a binding of one of these to make <code class="language-plaintext highlighter-rouge">ruby/json</code> faster.
Aside from many technical and legal restrictions, a big reason is that actually parsing JSON isn’t that much of a bottleneck,
even the fairly naive Ragel parser in <code class="language-plaintext highlighter-rouge">ruby/json</code> isn’t that slow (It could be way better though, but more on that later).</p>

<p>No, the really expansive part is building the Ruby objects tree, as evidenced by the time spent in <code class="language-plaintext highlighter-rouge">rb_hash_aset</code> on that
flame graph, but also in <code class="language-plaintext highlighter-rouge">rb_ary_push</code> on benchmarks that use a lot of arrays. So a custom parser can potentially end up faster
overall by being better tailored to make efficient use of Ruby APIs, such as how we do unescaping inside Ruby strings to avoid
extra copies and cache string keys.</p>

<p>But let’s focus on that part of the flame graph to see why <code class="language-plaintext highlighter-rouge">rb_hash_aset</code> is slow, and what we could do about it:</p>

<p><img src="/assets/articles/json-7/flamegraph-hash-aset.png" alt="" /></p>

<p>I want to draw your attention to two things in this flame graph.</p>

<p>The first is <code class="language-plaintext highlighter-rouge">ar_force_convert_table</code> on the left. You may have noticed that many functions in the graphs are prefixed with
<code class="language-plaintext highlighter-rouge">st_</code> or <code class="language-plaintext highlighter-rouge">rb_st</code> and a few with <code class="language-plaintext highlighter-rouge">ar_</code>.</p>

<p>The <code class="language-plaintext highlighter-rouge">st_</code> ones are referring to <a href="https://github.com/ruby/ruby/blob/23fc0fc22d0f066938387f3397fb8ee9358744e5/st.c"><code class="language-plaintext highlighter-rouge">st.c</code>, Ruby’s internal Hash Table implementation</a>.
What the <code class="language-plaintext highlighter-rouge">st</code> name stands for however I don’t know. <code class="language-plaintext highlighter-rouge">st.c</code> is a relatively well-optimized hash table that evolved over the years,
which is to be expected given how much Ruby relies on hash tables, even a modest performance gain in that data structure can have a
big impact overall. You can read its preamble if you want more details.</p>

<p>As for <code class="language-plaintext highlighter-rouge">ar_</code>, which I believe stands for “array”, refers to an optimization Ruby hashes do under the hood.
Hash tables are great, and offer a good access performance when the dataset is large, but when it’s very small they use quite a
lot of memory and aren’t really any better than a linear search. And Ruby code uses a lot of very small hash tables.</p>

<p>So Ruby hashes have an internal limit, <a href="https://github.com/ruby/ruby/blob/23fc0fc22d0f066938387f3397fb8ee9358744e5/internal/hash.h#L17C9-L17C32"><code class="language-plaintext highlighter-rouge">RHASH_AR_TABLE_MAX_SIZE</code></a>,
which on 64-bit platforms is <code class="language-plaintext highlighter-rouge">8</code>. Any Ruby Hash that contains <code class="language-plaintext highlighter-rouge">8</code> or fewer entries is actually lying to you, and is just an array
in a trenchcoat. That’s what <code class="language-plaintext highlighter-rouge">ar_table</code> is, a simple array of key and value pairs being used as an associative array.
And yes, its algorithmic complexity is technically <code class="language-plaintext highlighter-rouge">O(n)</code>, but with such a small <code class="language-plaintext highlighter-rouge">n</code>, it often is faster than doing all the hashing.</p>

<p>And if you add enough items in a Hash backed by an <code class="language-plaintext highlighter-rouge">ar_table</code>, it will be converted automatically to a <code class="language-plaintext highlighter-rouge">st_table</code>. That’s what
the <code class="language-plaintext highlighter-rouge">ar_force_convert_table</code> function does.</p>

<p>You can somewhat see this using one of my favorite APIs, <code class="language-plaintext highlighter-rouge">ObjectSpace.dump</code>:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="o">&gt;&gt;</span> <span class="nb">require</span> <span class="s2">"objspace"</span>
<span class="o">&gt;&gt;</span> <span class="nb">puts</span> <span class="no">ObjectSpace</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="mi">8</span><span class="p">.</span><span class="nf">times</span><span class="p">.</span><span class="nf">to_h</span> <span class="p">{</span> <span class="o">|</span><span class="n">i</span><span class="o">|</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="p">})</span>
<span class="p">{</span><span class="s2">"address"</span><span class="ss">:"0xf3d8"</span><span class="p">,</span> <span class="s2">"type"</span><span class="ss">:"HASH"</span><span class="p">,</span> <span class="s2">"slot_size"</span><span class="p">:</span><span class="mi">160</span><span class="p">,</span> <span class="s2">"size"</span><span class="p">:</span><span class="mi">8</span><span class="p">,</span> <span class="s2">"memsize"</span><span class="p">:</span><span class="mi">160</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span></code></pre></figure>

<p>This tells us a Hash with 8 keys fits neatly in a 160B object slot. Each object reference is 8B, 2 references per entry,
<code class="language-plaintext highlighter-rouge">16 * 8 =&gt; 128</code>, so with a few extra metadata and perhaps a bit of wasted space, it checks out.</p>

<p>But if we do the same with a Hash with <code class="language-plaintext highlighter-rouge">9</code> items, the result is very different:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="o">&gt;&gt;</span> <span class="nb">puts</span> <span class="no">ObjectSpace</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="mi">9</span><span class="p">.</span><span class="nf">times</span><span class="p">.</span><span class="nf">to_h</span> <span class="p">{</span> <span class="o">|</span><span class="n">i</span><span class="o">|</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="p">})</span>
<span class="p">{</span><span class="s2">"address"</span><span class="ss">:"0x1230"</span><span class="p">,</span> <span class="s2">"type"</span><span class="ss">:"HASH"</span><span class="p">,</span> <span class="s2">"slot_size"</span><span class="p">:</span><span class="mi">160</span><span class="p">,</span> <span class="s2">"size"</span><span class="p">:</span><span class="mi">9</span><span class="p">,</span> <span class="s2">"memsize"</span><span class="p">:</span><span class="mi">544</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span></code></pre></figure>

<p><code class="language-plaintext highlighter-rouge">544B</code>, that’s a big jump, but not so surprising. Hash tables almost by definition need to be somewhat large to not collide too much.</p>

<p>The other function on the flame graph I’d like to point out is <code class="language-plaintext highlighter-rouge">rebuild_table_if_necessary</code>, which stems from the same concern,
when you append to a Hash table, and it starts to get a bit too full, you have to increase its size, and contrary to an array,
it’s not just a matter of calling <code class="language-plaintext highlighter-rouge">realloc</code>, you have to essentially allocate a larger table, and then re-insert all the pairs
which means hashing the keys again, and that’s costly.</p>

<p>The problem though, is that exactly what we’re doing. When we encounter the start of a JSON object (<code class="language-plaintext highlighter-rouge">{</code>), we allocate a Ruby
Hash with <code class="language-plaintext highlighter-rouge">rb_hash_new</code>, and then every time we’re done parsing a key-value pair, we call <code class="language-plaintext highlighter-rouge">rb_hash_aset</code> to append to the hash.</p>

<p>So if a JSON document contains an object with 30 keys, we first allocate an <code class="language-plaintext highlighter-rouge">ar_table</code> with a capacity of <code class="language-plaintext highlighter-rouge">8</code> pairs, get it rebuilt
as a <code class="language-plaintext highlighter-rouge">st_table</code> that can hold <code class="language-plaintext highlighter-rouge">16</code> entries, and finally a third time with <code class="language-plaintext highlighter-rouge">32</code> entries. Meaning we’re hashing every key 3 times,
and wasting time in <code class="language-plaintext highlighter-rouge">malloc</code> and <code class="language-plaintext highlighter-rouge">free</code>.</p>

<p>When parsing a format like <code class="language-plaintext highlighter-rouge">msgpack</code>, or Ruby’s Marshal, the byte that signals the start of a Hash is followed by the expected
size of the Hash, allowing you to pre-allocate it with the right size, which helps a lot.
But JSON doesn’t have that, we have no choice but to parse as we go, and we’ll only know how big the Hash is once we’re done
parsing it.</p>

<p>The problem is the same with large arrays, they start embedded, and then double in size every time they run out of space, it’s just
not quite as bad because at least we don’t hash to re-hash the keys.</p>

<p>But looking at the flame graph above, we can see that all this resizing is a majority of the time spent in <code class="language-plaintext highlighter-rouge">rb_hash_aset</code>, and
<code class="language-plaintext highlighter-rouge">rb_hash_aset</code> over a quarter of the overall time, so there was really a big opportunity here.</p>

<p>If it’s a bad idea to directly append to a Hash before we know how large it will be, why don’t we just wait to be done parsing it
before we build it? Basic.</p>

<p>But that means we need to store its content somewhere else in the meantime, and the ideal structure for that is a stack, which 
is just a fancy name for an array.</p>

<p>You can look at <a href="https://github.com/ruby/json/pull/678">the full patch in C</a>, but I’ll try to explain the key concept with some
Ruby code here.</p>

<p>Before the parsing code was something like this:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">parse_object</span>
  <span class="nb">hash</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">until</span> <span class="n">object_done?</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">parse_object_key</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">parse_json</span>
    <span class="nb">hash</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
  <span class="k">end</span>
  <span class="nb">hash</span>
<span class="k">end</span></code></pre></figure>

<p>We simply parse until we find the object terminator, and until then we append to the Hash whenever we get a complete pair,
and each parsing function simply returns the parsed object.</p>

<p>After the change it now looks more something like this:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">parse_object</span><span class="p">(</span><span class="n">stack</span><span class="p">)</span>
  <span class="n">previous_size</span> <span class="o">=</span> <span class="n">stack</span><span class="p">.</span><span class="nf">size</span>
  <span class="k">until</span> <span class="n">object_done?</span>
    <span class="n">stack</span> <span class="o">&lt;&lt;</span> <span class="n">parse_object_key</span>
    <span class="n">stack</span> <span class="o">&lt;&lt;</span> <span class="n">parse_json</span>
  <span class="k">end</span>
  <span class="nb">hash</span> <span class="o">=</span> <span class="n">stack</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="n">stack</span><span class="p">.</span><span class="nf">size</span> <span class="o">-</span> <span class="n">previous_size</span><span class="p">).</span><span class="nf">to_h</span>
  <span class="n">stack</span> <span class="o">&lt;&lt;</span> <span class="nb">hash</span>
  <span class="nb">hash</span>
<span class="k">end</span></code></pre></figure>

<p>Every parse function now receives an array to use as the parsing stack, whatever they parse, they push on the stack.
The <code class="language-plaintext highlighter-rouge">parse_object</code> function is no exception, it first records how large the stack is, then it parses keys and values
and push them both on the stack.
Once the end of the object is found, all the pairs are popped from the back of the stack, and a hash is immediately created with
the right size, ensuring each key is only hashed once.</p>

<p>In C, it looks like this:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c">    <span class="kt">long</span> <span class="n">count</span> <span class="o">=</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">stack</span><span class="o">-&gt;</span><span class="n">head</span> <span class="o">-</span> <span class="n">stack_head</span><span class="p">;</span>

    <span class="n">VALUE</span> <span class="n">hash</span><span class="p">;</span>
<span class="cp">#ifdef HAVE_RB_HASH_NEW_CAPA
</span>    <span class="n">hash</span> <span class="o">=</span> <span class="n">rb_hash_new_capa</span><span class="p">(</span><span class="n">count</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">);</span>
<span class="cp">#else
</span>    <span class="n">hash</span> <span class="o">=</span> <span class="n">rb_hash_new</span><span class="p">();</span>
<span class="cp">#endif
</span>    <span class="n">rb_hash_bulk_insert</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">rvalue_stack_peek</span><span class="p">(</span><span class="n">json</span><span class="o">-&gt;</span><span class="n">stack</span><span class="p">,</span> <span class="n">count</span><span class="p">),</span> <span class="n">hash</span><span class="p">);</span>
    <span class="o">*</span><span class="n">result</span> <span class="o">=</span> <span class="n">hash</span><span class="p">;</span>
    <span class="n">rvalue_stack_pop</span><span class="p">(</span><span class="n">json</span><span class="o">-&gt;</span><span class="n">stack</span><span class="p">,</span> <span class="n">count</span><span class="p">);</span></code></pre></figure>

<p>You should be able to recognize the pattern, we create a hash of the right size using the <code class="language-plaintext highlighter-rouge">rb_hash_new_capa</code> API,
which unfortunately we have to test for its existence because unfortunately <a href="https://bugs.ruby-lang.org/issues/18683">I only exposed it to C extensions a few years ago</a>.
Then we insert all pairs at once with <code class="language-plaintext highlighter-rouge">rb_hash_bulk_insert</code>.</p>

<p>And that’s it, with just that change, the <code class="language-plaintext highlighter-rouge">twitter.json</code> benchmark was sped up by <code class="language-plaintext highlighter-rouge">22%</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>== Parsing twitter.json (567916 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after    90.000 i/100ms
Calculating -------------------------------------
               after    897.170 (± 0.4%) i/s    (1.11 ms/i) -      4.500k in   5.015857s

Comparison:
              before:      737.1 i/s
               after:      897.2 i/s - 1.22x  faster
</code></pre></div></div>

<h2 id="avoid-double-scanning">Avoid Double Scanning</h2>

<p>After the value stack patch was so effective, <code class="language-plaintext highlighter-rouge">json_string_unescape</code> was back to being the biggest bottleneck at <code class="language-plaintext highlighter-rouge">22%</code> of total runtime:</p>

<p><img src="/assets/articles/json-7/flamegraph-str-unescape.png" alt="" /></p>

<p><a href="https://share.firefox.dev/4hfnvjD">Full profile</a>.</p>

<p>I had recently optimized it by optimistically assuming most strings don’t contain any escape character, but something still<br />
bothered me. The Ragel parser calls our <code class="language-plaintext highlighter-rouge">JSON_parse_string</code> callback with both the start and end pointer of the string, to do
that it has to scan the string, so it’s a bit silly that the first thing we immediately do right after that is to scan it all over
again.</p>

<p>It would be way better if while it is looking for the end of the string, the Ragel parser would record if it had seen any
backslash, and if not, we’d save on re-scanning it again.</p>

<p>Here’s how the JSON strings grammar was defined:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="o">%%</span><span class="p">{</span>
    <span class="n">machine</span> <span class="n">JSON_string</span><span class="p">;</span>
    <span class="n">include</span> <span class="n">JSON_common</span><span class="p">;</span>

    <span class="n">write</span> <span class="n">data</span><span class="p">;</span>

    <span class="n">action</span> <span class="n">parse_string</span> <span class="p">{</span>
        <span class="o">*</span><span class="n">result</span> <span class="o">=</span> <span class="n">json_string_unescape</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">memo</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">parsing_name</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">parsing_name</span> <span class="o">||</span> <span class="n">json</span><span class="o">-&gt;</span> <span class="n">freeze</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">parsing_name</span> <span class="o">&amp;&amp;</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">symbolize_names</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">NIL_P</span><span class="p">(</span><span class="o">*</span><span class="n">result</span><span class="p">))</span> <span class="p">{</span>
            <span class="n">fhold</span><span class="p">;</span>
            <span class="n">fbreak</span><span class="p">;</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">fexec</span> <span class="n">p</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="n">action</span> <span class="n">exit</span> <span class="p">{</span> <span class="n">fhold</span><span class="p">;</span> <span class="n">fbreak</span><span class="p">;</span> <span class="p">}</span>

    <span class="n">main</span> <span class="o">:=</span> <span class="sc">'"'</span> <span class="p">((</span><span class="o">^</span><span class="p">([</span><span class="err">\</span><span class="s">"</span><span class="se">\\</span><span class="s">] | 0..0x1f) | '</span><span class="se">\\</span><span class="s">'[</span><span class="se">\"\\</span><span class="s">/bfnrt] | '</span><span class="se">\\</span><span class="s">u'[0-9a-fA-F]{4} | '</span><span class="se">\\</span><span class="s">'^([</span><span class="se">\"\\</span><span class="s">/bfnrtu]|0..0x1f))* %parse_string) '"</span><span class="err">'</span> <span class="err">@</span><span class="n">exit</span><span class="p">;</span>
<span class="p">}</span><span class="o">%%</span></code></pre></figure>

<p>If you don’t understand it, don’t worry, me neither.</p>

<p>That’s where I kinda need to confess that I don’t have a proper computer science education, more some sort of very applied
software engineering curriculum, and not a particularly good one, so terms like “formal grammar” and parser generators like Ragel
et al kind of fly over my head, hence I’m kinda struggling to improve the core parsing parts. And I also can’t rely on
my usual tricks to work with things I don’t fully grasp because Ragel outputs absolutely disgusting code with lots of <code class="language-plaintext highlighter-rouge">goto</code>,
which makes it super hard to learn by experimentation.</p>

<p>Yet, even with my limited understanding, I can say something is really off here.
We can see on the last line that we’re basically instructing Ragel about all the possible escape sequences inside a JSON string,
which to me doesn’t make much sense. All we need the parser to do for us is to know enough to find the end of the string, 
tell us it ran into the end of the stream without finding it, or if it ran into an invalid character (e.g. a newline).</p>

<p>It absolutely doesn’t need to validate that <code class="language-plaintext highlighter-rouge">\u</code> is followed by 4 hexadecimal characters, we can do that during unescaping.</p>

<p>But anyway, this is for strings with escape sequences, and we don’t have that many of those, I had to figure out a way
to have a fast path for simple strings, and after a few hours of struggling and begging some properly educated people for help
I managed to get this:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="o">%%</span><span class="p">{</span>
    <span class="n">machine</span> <span class="n">JSON_string</span><span class="p">;</span>
    <span class="n">include</span> <span class="n">JSON_common</span><span class="p">;</span>

    <span class="n">write</span> <span class="n">data</span><span class="p">;</span>

    <span class="n">action</span> <span class="n">parse_complex_string</span> <span class="p">{</span>
        <span class="o">*</span><span class="n">result</span> <span class="o">=</span> <span class="n">json_string_unescape</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">memo</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">parsing_name</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">parsing_name</span> <span class="o">||</span> <span class="n">json</span><span class="o">-&gt;</span> <span class="n">freeze</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">parsing_name</span> <span class="o">&amp;&amp;</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">symbolize_names</span><span class="p">);</span>
        <span class="n">fexec</span> <span class="n">p</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
        <span class="n">fhold</span><span class="p">;</span>
        <span class="n">fbreak</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="n">action</span> <span class="n">parse_simple_string</span> <span class="p">{</span>
        <span class="o">*</span><span class="n">result</span> <span class="o">=</span> <span class="n">json_string_fastpath</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">memo</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">parsing_name</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">parsing_name</span> <span class="o">||</span> <span class="n">json</span><span class="o">-&gt;</span> <span class="n">freeze</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">parsing_name</span> <span class="o">&amp;&amp;</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">symbolize_names</span><span class="p">);</span>
        <span class="n">fexec</span> <span class="n">p</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
        <span class="n">fhold</span><span class="p">;</span>
        <span class="n">fbreak</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="n">double_quote</span> <span class="o">=</span> <span class="sc">'"'</span><span class="p">;</span>
    <span class="n">escape</span> <span class="o">=</span> <span class="sc">'\\'</span><span class="p">;</span>
    <span class="n">control</span> <span class="o">=</span> <span class="mi">0</span><span class="p">..</span><span class="mh">0x1f</span><span class="p">;</span>
    <span class="n">simple</span> <span class="o">=</span> <span class="n">any</span> <span class="o">-</span> <span class="n">escape</span> <span class="o">-</span> <span class="n">double_quote</span> <span class="o">-</span> <span class="n">control</span><span class="p">;</span>

    <span class="n">main</span> <span class="o">:=</span> <span class="n">double_quote</span> <span class="p">(</span>
         <span class="p">(</span><span class="n">simple</span><span class="o">*</span><span class="p">)(</span>
            <span class="p">(</span><span class="n">double_quote</span><span class="p">)</span> <span class="err">@</span><span class="n">parse_simple_string</span> <span class="o">|</span>
            <span class="p">((</span><span class="o">^</span><span class="p">([</span><span class="err">\</span><span class="s">"</span><span class="se">\\</span><span class="s">] | control) | escape[</span><span class="se">\"\\</span><span class="s">/bfnrt] | '</span><span class="se">\\</span><span class="s">u'[0-9a-fA-F]{4} | escape^([</span><span class="se">\"\\</span><span class="s">/bfnrtu]|0..0x1f))* double_quote) @parse_complex_string</span><span class="err">
</span><span class="s">         )</span><span class="err">
</span><span class="s">    );</span><span class="err">
</span><span class="s">}%%</span></code></pre></figure>

<p>The idea is simple, start by only looking for a double quote not preceded by any backslash, that’s the optimistic path, and if it
matches we enter <code class="language-plaintext highlighter-rouge">parse_simple_string</code>. If it doesn’t, we fall back to the previous pattern and end up in <code class="language-plaintext highlighter-rouge">parse_complex_string</code>.</p>

<p>The fast path is a much simpler function:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="k">static</span> <span class="n">VALUE</span> <span class="nf">json_string_fastpath</span><span class="p">(</span><span class="n">JSON_Parser</span> <span class="o">*</span><span class="n">json</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">string</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">stringEnd</span><span class="p">,</span> <span class="n">bool</span> <span class="n">is_name</span><span class="p">,</span> <span class="n">bool</span> <span class="n">intern</span><span class="p">,</span> <span class="n">bool</span> <span class="n">symbolize</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">size_t</span> <span class="n">bufferSize</span> <span class="o">=</span> <span class="n">stringEnd</span> <span class="o">-</span> <span class="n">string</span><span class="p">;</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">is_name</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">VALUE</span> <span class="n">cached_key</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">RB_UNLIKELY</span><span class="p">(</span><span class="n">symbolize</span><span class="p">))</span> <span class="p">{</span>
            <span class="n">cached_key</span> <span class="o">=</span> <span class="n">rsymbol_cache_fetch</span><span class="p">(</span><span class="o">&amp;</span><span class="n">json</span><span class="o">-&gt;</span><span class="n">name_cache</span><span class="p">,</span> <span class="n">string</span><span class="p">,</span> <span class="n">bufferSize</span><span class="p">);</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">cached_key</span> <span class="o">=</span> <span class="n">rstring_cache_fetch</span><span class="p">(</span><span class="o">&amp;</span><span class="n">json</span><span class="o">-&gt;</span><span class="n">name_cache</span><span class="p">,</span> <span class="n">string</span><span class="p">,</span> <span class="n">bufferSize</span><span class="p">);</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">RB_LIKELY</span><span class="p">(</span><span class="n">cached_key</span><span class="p">))</span> <span class="p">{</span>
            <span class="k">return</span> <span class="n">cached_key</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">build_string</span><span class="p">(</span><span class="n">string</span><span class="p">,</span> <span class="n">stringEnd</span><span class="p">,</span> <span class="n">intern</span><span class="p">,</span> <span class="n">symbolize</span><span class="p">);</span>
<span class="p">}</span></code></pre></figure>

<p>Nothing particularly fancy. Unfortunately the impact on <code class="language-plaintext highlighter-rouge">twitter.json</code> wasn’t that big:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>== Parsing twitter.json (567916 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after    91.000 i/100ms
Calculating -------------------------------------
               after    913.740 (± 0.3%) i/s    (1.09 ms/i) -      4.641k in   5.079191s

Comparison:
              before:      886.9 i/s
               after:      913.7 i/s - 1.03x  faster
</code></pre></div></div>

<p>I was a bit disappointed, but still, I progressed a bit in my understanding of Ragel and knew there was
lots of fishy things in <code class="language-plaintext highlighter-rouge">ruby/json</code>’s Ragel parser, so that would be useful.</p>

<h2 id="avoid-useless-copies">Avoid Useless Copies</h2>

<p>After that disappointment, I needed a bit of a break, so I went back to a function where I knew I could get better
results on, integer parsing.</p>

<p>I didn’t have a micro-benchmark dedicated to integers, but the small array one would do:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">benchmark_parsing</span> <span class="s2">"small nested array"</span><span class="p">,</span> <span class="no">JSON</span><span class="p">.</span><span class="nf">dump</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]]</span><span class="o">*</span><span class="mi">10</span><span class="p">)</span></code></pre></figure>

<p>So 10 arrays, with 5 integers each, quite simple.</p>

<p>And you know the drill, it started with some profiling.</p>

<p><img src="/assets/articles/json-7/flamegraph-integer.png" alt="" /></p>

<p><a href="https://share.firefox.dev/4jeyFXA">Full profile</a>.</p>

<p>As <a href="/ruby/json/2025/01/12/optimizing-ruby-json-part-6.html#more-stack-allocation">we mentioned in the previous part</a>,
<code class="language-plaintext highlighter-rouge">rb_cstr2inum</code>, the API Ruby gives us to turn a C string into a Ruby Integer, isn’t very efficient.</p>

<p>First, because it expects a C string, it forces us to first copy the string into a buffer so we can append a <code class="language-plaintext highlighter-rouge">NULL</code> to it,
but also because it has to deal with quite a lot of cases we don’t care about, such as a variable base. For instance <code class="language-plaintext highlighter-rouge">0xff</code> is a
valid number for <code class="language-plaintext highlighter-rouge">rb_cstr2inum</code>, but not for JSON. It also has to support arbitrary long integers, which slows it down, but the
overwhelming majority of the numbers we parse fit in 64 bits.</p>

<p>So we have an opportunity here for another fast path type of function, that would deal with the crux of integer parsing, and
for the rare and complex cases, continue to rely on <code class="language-plaintext highlighter-rouge">rb_cstr2inum</code>.</p>

<p>The implementation is very straightforward, you can see <a href="https://github.com/ruby/json/pull/692">the full patch</a>,
but I’ll detail the key parts:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="k">static</span> <span class="kr">inline</span> <span class="n">VALUE</span> <span class="nf">fast_parse_integer</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">pe</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">bool</span> <span class="n">negative</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">*</span><span class="n">p</span> <span class="o">==</span> <span class="sc">'-'</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">negative</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
        <span class="n">p</span><span class="o">++</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="kt">long</span> <span class="kt">long</span> <span class="n">memo</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">p</span> <span class="o">&lt;</span> <span class="n">pe</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">memo</span> <span class="o">*=</span> <span class="mi">10</span><span class="p">;</span>
        <span class="n">memo</span> <span class="o">+=</span> <span class="o">*</span><span class="n">p</span> <span class="o">-</span> <span class="sc">'0'</span><span class="p">;</span>
        <span class="n">p</span><span class="o">++</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">negative</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="o">-</span><span class="n">memo</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">LL2NUM</span><span class="p">(</span><span class="n">memo</span><span class="p">);</span>
<span class="p">}</span></code></pre></figure>

<p>We start by checking if the number is negative, then convert ASCII characters into the corresponding integer one by one.</p>

<p>The limitation, however, is that this can only work for an integer that fits in a native integer type, as such we only enter
this fast path if the number of digits is low enough:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="cp">#define MAX_FAST_INTEGER_SIZE 18
</span>
<span class="kt">long</span> <span class="n">len</span> <span class="o">=</span> <span class="n">p</span> <span class="o">-</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">memo</span><span class="p">;</span>
<span class="k">if</span> <span class="p">(</span><span class="n">RB_LIKELY</span><span class="p">(</span><span class="n">len</span> <span class="o">&lt;</span> <span class="n">MAX_FAST_INTEGER_SIZE</span><span class="p">))</span> <span class="p">{</span>
    <span class="o">*</span><span class="n">result</span> <span class="o">=</span> <span class="n">fast_parse_integer</span><span class="p">(</span><span class="n">json</span><span class="o">-&gt;</span><span class="n">memo</span><span class="p">,</span> <span class="n">p</span><span class="p">);</span>
<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="n">fbuffer_clear</span><span class="p">(</span><span class="o">&amp;</span><span class="n">json</span><span class="o">-&gt;</span><span class="n">fbuffer</span><span class="p">);</span>
    <span class="n">fbuffer_append</span><span class="p">(</span><span class="o">&amp;</span><span class="n">json</span><span class="o">-&gt;</span><span class="n">fbuffer</span><span class="p">,</span> <span class="n">json</span><span class="o">-&gt;</span><span class="n">memo</span><span class="p">,</span> <span class="n">len</span><span class="p">);</span>
    <span class="n">fbuffer_append_char</span><span class="p">(</span><span class="o">&amp;</span><span class="n">json</span><span class="o">-&gt;</span><span class="n">fbuffer</span><span class="p">,</span> <span class="sc">'\0'</span><span class="p">);</span>
    <span class="o">*</span><span class="n">result</span> <span class="o">=</span> <span class="n">rb_cstr2inum</span><span class="p">(</span><span class="n">FBUFFER_PTR</span><span class="p">(</span><span class="o">&amp;</span><span class="n">json</span><span class="o">-&gt;</span><span class="n">fbuffer</span><span class="p">),</span> <span class="mi">10</span><span class="p">);</span>
<span class="p">}</span></code></pre></figure>

<p>Why 18? Because regardless of the CPU architecture, <a href="https://en.wikibooks.org/wiki/C_Programming/limits.h#Member_constants">in C a <code class="language-plaintext highlighter-rouge">long long</code> must support a maximum value of <code class="language-plaintext highlighter-rouge">9,223,372,036,854,775,807</code>
and a minimum value of <code class="language-plaintext highlighter-rouge">−9,223,372,036,854,775,808</code></a>, in other words, it’s always a 64-bit integer.
That’s 19 digits, but there are some 19-digit numbers that don’t fit in a <code class="language-plaintext highlighter-rouge">long long</code>, so 18.</p>

<p>It would be possible to handle slightly bigger numbers by using an <code class="language-plaintext highlighter-rouge">unsigned long long</code>, but I didn’t think it was worth it.</p>

<p>As for the impact on the micro-benchmark, it was pretty good:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>== Parsing small nested array (121 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after   124.666k i/100ms
Calculating -------------------------------------
               after      1.258M (± 2.3%) i/s  (794.63 ns/i) -      6.358M in   5.055135s

Comparison:
              before:   816626.3 i/s
               after:  1258454.3 i/s - 1.54x  faster
</code></pre></div></div>

<p>But that’s a micro-benchmark of course, here’s the effect on more realistic ones:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>== Parsing twitter.json (567916 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after    92.000 i/100ms
Calculating -------------------------------------
               after    939.320 (± 1.0%) i/s    (1.06 ms/i) -      4.784k in   5.093485s

Comparison:
              before:      875.5 i/s
               after:      939.3 i/s - 1.07x  faster


== Parsing citm_catalog.json (1727030 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after    43.000 i/100ms
Calculating -------------------------------------
               after    430.366 (± 0.9%) i/s    (2.32 ms/i) -      2.193k in   5.096015s

Comparison:
              before:      388.8 i/s
               after:      430.4 i/s - 1.11x  faster
</code></pre></div></div>

<h2 id="avoid-duplicated-work">Avoid Duplicated Work</h2>

<p>The final parser optimization that shipped with <code class="language-plaintext highlighter-rouge">json 2.9.0</code>, was submitted by <a href="https://github.com/tenderlove">Aaron Patterson</a>.
I’m not too sure how he got to work on it, perhaps he was attracted by the smell of blood when he saw me cursing
against Ragel in our company Slack, who knows?</p>

<p>The key element of Aaron’s patch is that he changed this:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="n">np</span> <span class="o">=</span> <span class="n">JSON_parse_float</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">fpc</span><span class="p">,</span> <span class="n">pe</span><span class="p">,</span> <span class="n">result</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">np</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">fexec</span> <span class="n">np</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">np</span> <span class="o">=</span> <span class="n">JSON_parse_integer</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">fpc</span><span class="p">,</span> <span class="n">pe</span><span class="p">,</span> <span class="n">result</span><span class="p">);</span></code></pre></figure>

<p>Into this:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="n">np</span> <span class="o">=</span> <span class="n">JSON_parse_number</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">fpc</span><span class="p">,</span> <span class="n">pe</span><span class="p">,</span> <span class="n">result</span><span class="p">);</span></code></pre></figure>

<p>If it’s not yet obvious, the previous version of the parser would first try to parse a float, and if it failed to do so,
would try to parse an integer. This is quite wasteful, because all floats start with an integer, so whenever the next value to
parse was an integer, it would first be fully scanned by <code class="language-plaintext highlighter-rouge">JSON_parse_float</code> to figure out it’s not an integer, and then the parser
would backtrack and scan the same bytes again in <code class="language-plaintext highlighter-rouge">JSON_parse_integer</code>.</p>

<p>You can look at <a href="https://github.com/ruby/json/pull/698/">the full patch</a>, which also contains some changes to the grammar and
state machine to make the above change possible, but that really is the core of it.</p>

<p>And you might think it’s indeed more efficient, but probably not that big of a deal in the grand scheme of things, but actually
it did speedup <code class="language-plaintext highlighter-rouge">twitter.json</code> and <code class="language-plaintext highlighter-rouge">citm_catalog.json</code> by a nice <code class="language-plaintext highlighter-rouge">5%</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>== Parsing twitter.json (567916 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after    95.000 i/100ms
Calculating -------------------------------------
               after    947.551 (± 0.9%) i/s    (1.06 ms/i) -      4.750k in   5.013354s

Comparison:
              before:      904.4 i/s
               after:      947.6 i/s - 1.05x  faster


== Parsing citm_catalog.json (1727030 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after    45.000 i/100ms
Calculating -------------------------------------
               after    458.244 (± 0.2%) i/s    (2.18 ms/i) -      2.295k in   5.008296s

Comparison:
              before:      432.7 i/s
               after:      458.2 i/s - 1.06x  faster
</code></pre></div></div>

<h2 id="fin">Fin?</h2>

<p>And that’s it, that was the final optimization performed before I released <code class="language-plaintext highlighter-rouge">json 2.9.0</code>, so I will conclude this series.</p>

<p>If you wonder how fast it now is, here’s a final <code class="language-plaintext highlighter-rouge">twitter.json</code> benchmark against the competition:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>== Parsing twitter.json (567916 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
                json    93.000 i/100ms
                  oj    66.000 i/100ms
          Oj::Parser    80.000 i/100ms
           rapidjson    58.000 i/100ms
Calculating -------------------------------------
                json    928.472 (± 0.3%) i/s    (1.08 ms/i) -      4.650k in   5.008282s
                  oj    666.198 (± 0.8%) i/s    (1.50 ms/i) -      3.366k in   5.052899s
          Oj::Parser    803.031 (± 0.2%) i/s    (1.25 ms/i) -      4.080k in   5.080788s
           rapidjson    584.869 (± 0.2%) i/s    (1.71 ms/i) -      2.958k in   5.057565s

Comparison:
                json:      928.5 i/s
          Oj::Parser:      803.0 i/s - 1.16x  slower
                  oj:      666.2 i/s - 1.39x  slower
           rapidjson:      584.9 i/s - 1.59x  slower
</code></pre></div></div>

<p>That isn’t to say I’m done optimizing, I have quite a few ideas for the future, but I wanted to stabilize the gem prior to the
release of Ruby 3.4.0, and I feel it is now fast enough that there’s no urgency.</p>

<p>But to give you an idea of what may happen in the future, I’d like to drop Ragel and replace it with a simpler recursive descent
parser. The existing one could certainly be improved, but I find it much harder to work with generated parsers than to write them
manually.</p>

<p>I’m also currently pairing with <a href="https://github.com/etiennebarrie">Étienne Barrié</a> on <a href="https://github.com/ruby/json/pull/718">a better API for both the parser and the encoder</a>
which would allow to reduce the setup cost even further, stop relying as much on global state, and would generally be more ergonomic.</p>

<p>I hope you enjoyed this blog series, I’ll try to continue writing, next, I’d like to share some thoughts on <a href="https://github.com/Shopify/pitchfork">Pitchfork</a>,
but I may need to set the stage for it with other posts to explain some key concepts.</p>]]></content><author><name></name></author><category term="ruby" /><category term="json" /><summary type="html"><![CDATA[In the previous post, we started covering some parser optimizations. There’s just a handful more to cover until we reached what’s the state of the currently released version of ruby/json.]]></summary></entry></feed>